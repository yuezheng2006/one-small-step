[{"id":0,"title":"如何避免 KVCache 失效","content":"学到一个新的小技巧, 跟 AI 对话的时候, 尤其是多个context反复对话, 一直保持一个session 的情况, 如果你的算力非常紧张.\n那么能不带动态内容, 尽量不要带动态内容, 比如当前时间戳, 会导致 KVCache 失效.\n\n因为这样会导致每次生成的 token 序列不同, 迫使模型重新计算整个序列的 KVCache, 无法复用之前的缓存.\n\n一些复杂的 UI, 比如 OpenWebUI, 会给当前聊天用 LLM 取名的这个操作, 虽然会更方便, 但如果发生在会话中间, 也会导致 KVCache\n失效.\n\n那么如何判断 KVCache 失效呢？实际应用中, 像 Llama.cpp 这类推理引擎确实对 KVCache 管理非常敏感.\n一个简单的测试方法是观察推理时的内存波动 - 如果内存频大起大落, 说明 KVCache 未能有效复用.","routePath":"/guide/ai/How-to-avoid-KVCache-invalidation","lang":"zh","toc":[],"domain":"","frontmatter":{"title":"如何避免 KVCache 失效","description":"如何避免 KVCache 失效","date":20250513,"plainLanguage":"**避免 KVCache 失效说白了就是：** 别让 AI \"重新算一遍\"——保持输入稳定，AI 能复用之前的计算。\n\n就像你背课文，背到一半被打断，重新开始就得从头背。KVCache 就是 AI 的\"记忆断点\"——如果输入变了，AI 就得从头算，浪费时间和资源。\n\n**用大白话说：**\n想象老板在炒菜，已经炒到一半了（KVCache）。如果你突然改菜单，他就得把锅洗了重新炒（KVCache 失效）。如果你不改，他接着炒就行（KVCache 复用）。\n\n**什么会导致 KVCache 失效：**\n1. **动态内容**：每次输入都不一样（比如\"当前时间：2024-01-20 10:15:23\"）\n2. **中途改名**：聊天中途用 LLM 给对话取名，导致输入序列变化\n3. **格式变化**：输入格式不统一，每次都不一样\n\n**怎么避免：**\n- **去掉时间戳**：别每次都带\"当前时间\"，除非真的需要\n- **固定 System Prompt**：系统提示词要稳定，别频繁改\n- **批量命名**：要取名就在对话结束后取，别在中间取\n\n**怎么判断 KVCache 是否失效：**\n- 看显存/内存波动：如果一会儿高一会儿低，说明频繁重算\n- 看响应速度：如果突然变慢，可能是 KVCache 失效了\n\n**实际影响：**\n- KVCache 有效：生成速度快，显存占用稳定\n- KVCache 失效：每次都重新算，慢而且费资源\n\n说白了，避免 KVCache 失效就是\"别没事找事\"——输入能稳定就稳定，别动不动就变，让 AI 能\"接着之前的继续\"，而不是\"从头再来\"。\n"},"version":""},{"id":1,"title":"大模型微调最佳实践指南 (超简洁版)","content":"(图片来自 unstructured.io)\n\n\n1. 决策阶段（ROI最高）#\n\n✅ 建议：\n\n * 优先使用prompt工程解决简单模式匹配需求 (例如修改文本格式, 大小写转换等)\n * 评估是否需要知识更新 → 优先考虑RAG方案\n * 确认数据量是否足够（数学推理任务可尝试小一些的数据, 但大量数据带来的优势仍然是显著的）\n\n❌ 不建议：\n\n * 用微调解决简单响应模式问题（大炮打蚊子）\n * 试图通过微调注入新知识 (请使用RAG)\n * 在数据量<500条时强行微调\n\n\n2. 数据准备（占80%时间投入）#\n\n✅ 建议：\n\n * 构建多样化样本（覆盖边缘案例）\n * 确保输入输出格式明确统一\n * 使用LLM生成增强数据\n * 按任务类型结构化数据（指令/对话/开放式）\n\n❌ 不建议：\n\n * 堆砌同质化数据\n * 保留未清洗的脏数据\n * 忽略数据去重\n\n\n3. 模型选择（中高ROI）#\n\n✅ 建议：\n\n * 优先选择1-13B实用模型（如Llama-3.x-8B）\n * 严格检查商业使用许可\n * 根据任务复杂度选择尺寸\n\n❌ 不建议：\n\n * 盲目使用100B+大模型\n * 忽略许可证限制\n * 为简单任务过度配置\n\n\n4. 评估体系（常被忽视的高ROI）#\n\n✅ 建议：\n\n * 建立量化评估基准\n * 对比不同超参数效果\n * 定期验证模型退化\n\n❌ 不建议：\n\n * 仅凭主观感受评估\n * 忽略baseline对比\n * 训练后不做回归测试\n\n\n5. 微调策略（中ROI）#\n\n✅ 建议：\n\n * 优先尝试LoRA及其变种（DoRA/AdaLoRA）\n * 领域适配时结合剪枝+微调\n * 使用SGD+学习率调度器\n\n❌ 不建议：\n\n * 默认使用QLoRA（仅在显存不足时使用）\n * 设置过高lora_alpha值（建议从2×rank开始）\n * 盲目相信论文报告的训练时间\n\n\n6. 训练优化（技术细节）#\n\n✅ 建议：\n\n * 混合精度+量化节省显存\n * 梯度累积模拟大批量\n * 定期保存检查点\n\n❌ 不建议：\n\n * 在单卡上强推大batch_size\n * 忽略模型并行化选项\n * 训练中断后从头开始\n\n\n7. 资源分配建议#\n\n * 80%时间：数据工程\n * 15%时间：评估体系构建\n * 5%时间：实际训练配置\n\n\n版本注意#\n\n * 保持对LoRA生态的跟踪（每月新变种出现）\n * 关注Sophia优化器等新进展\n * 定期验证不同量化方案效果","routePath":"/guide/ai/LLM-fine-tuning-summary","lang":"zh","toc":[{"text":"1. 决策阶段（ROI最高）","id":"1-决策阶段roi最高","depth":2,"charIndex":24},{"text":"2. 数据准备（占80%时间投入）","id":"2-数据准备占80时间投入","depth":2,"charIndex":244},{"text":"3. 模型选择（中高ROI）","id":"3-模型选择中高roi","depth":2,"charIndex":391},{"text":"4. 评估体系（常被忽视的高ROI）","id":"4-评估体系常被忽视的高roi","depth":2,"charIndex":527},{"text":"5. 微调策略（中ROI）","id":"5-微调策略中roi","depth":2,"charIndex":644},{"text":"6. 训练优化（技术细节）","id":"6-训练优化技术细节","depth":2,"charIndex":813},{"text":"7. 资源分配建议","id":"7-资源分配建议","depth":2,"charIndex":933},{"text":"版本注意","id":"版本注意","depth":2,"charIndex":992}],"domain":"","frontmatter":{"title":"大模型微调最佳实践指南 (超简洁版)","description":"大模型微调最佳实践指南 (超简洁版)","date":20250210,"plainLanguage":"**大模型微调最佳实践说白了就是：** 少踩坑的\"避雷指南\"——80%时间准备数据，5%时间训练。\n\n就像炒菜，80%时间在洗菜切菜准备（数据），15%时间调味试味（评估），5%时间真正炒（训练）。很多人上来就炒，结果菜没洗、调料没准备，炒出来能好吃吗？\n\n**用大白话说：**\n想开烧烤摊，关键不是买多贵的炉子（模型大小），而是：1) 食材新鲜多样（数据质量），2) 调料配方对（评估体系），3) 火候掌握好（训练参数）。很多人花大钱买设备，结果食材不行，照样难吃。\n\n**核心原则（按重要性排序）：**\n\n**1. 决策阶段（最重要！）**\n- **能用 prompt 解决，就别微调**：就像能用微波炉热饭，别非得开火做\n- **需要新知识？用 RAG，别微调**：微调教不会新知识，只能教新技能\n- **数据少于 500 条？别微调**：样本太少，微调出来的模型会\"偏科\"\n\n**2. 数据准备（80%时间）**\n- **多样化**：别全是\"你好\"、\"谢谢\"这种，要有各种场景\n- **干净**：重复的删掉，错误的改掉，脏数据能毁了整个模型\n- **可以用 AI 生成数据**：让 GPT-4 帮你生成更多训练样本\n\n**3. 模型选择（别贪大）**\n- **推荐 1-13B 的模型**：比如 Llama-3-8B，够用又省钱\n- **别盲目用 100B+ 的大模型**：就像买车，法拉利是好，但你只是买菜，买个普通车就行\n\n**4. 评估体系（常被忽视）**\n- **量化评估**：别凭感觉，要有数据（准确率、召回率等）\n- **对比 baseline**：微调前后要对比，别微调完反而变差了\n\n**5. 训练策略**\n- **优先用 LoRA**：省显存、训练快、效果好\n- **别用 QLoRA**（除非显存真的不够）：QLoRA 是妥协方案，不是最佳方案\n\n说白了，微调最佳实践就是\"磨刀不误砍柴工\"——80%精力在准备，20%精力在执行，别本末倒置。\n"},"version":""},{"id":2,"title":"","content":"大模型精度格式一览#\n\n精度格式              全称/别称                               位数              格式构成 (符号/指数/尾数)                            代表性模型                                          主要特点与应用\nFP128             四倍精度浮点 (Quadruple Precision)        128位            1 / 15 / 112                               科学计算模拟、高精度金融模型                                 提供约34位十进制有效数字的极高精度，主要用于科学计算、金融建模和需要极高数值精度的研究领域。在AI领域应用较少，但对精\n                                                                                                                                                                度要求极高的场景会使用。\nFP64              双精度浮点 (Double Precision)            64位             1 / 11 / 52                                AlphaFold（部分计算）、科学计算应用                         提供约15位十进制有效数字的高精度，是科学计算的标准。在AI训练中较少使用（由于计算成本高），但在需要高精度梯度累积或数\n                                                                                                                                                                值稳定性的场景中会使用。\nFP32              单精度浮点 (Single Precision)            32位             1 / 8 / 23                                 GPT-2、ResNet、早期BERT等传统模型                       动态范围和精度都很高，是传统深度学习训练的基准，兼容性最广。\nTF32              TensorFloat-32                      32位 (内部19位计算)   1 / 8 / 10                                 A100上训练的Llama、GPT-3等大模型                        NVIDIA Ampere架构及更新GPU中Tensor\n                                                                                                                                                                Core的内部格式。拥有与FP32相同的动态范围和FP16的精度，通过截断FP32的尾数位实现加速，无需修改代码即可提升\n                                                                                                                                                                FP32训练速度。\nFP16              半精度浮点 (Half Precision)              16位             1 / 5 / 10                                 GPT-3（混合精度训练）、Stable Diffusion                 相比FP32内存和计算减半。动态范围较小（约6e-5 到\n                                                                                                                                                                65504），训练时可能出现上溢或下溢，通常需要结合动态损失缩放（Dynamic Loss Scaling）使用。\nBF16              BFloat16 / 脑浮点数                     16位             1 / 8 / 7                                  Llama 2/3、Qwen、Gemini                          动态范围与FP32相同，解决了FP16的溢出问题，但尾数精度较低。🔥\n                                                                                                                                                                目前Instruction大模型训练最常用的精度格式，非常适合大规模模型的训练，是云端AI芯片的主流格式。\nINT16             16位整型 (16-bit Integer)              16位             value = (int_value - zero_point) * scale   音频处理模型、部分混合精度训练场景                              提供比INT8更高的精度和更大的动态范围。内存占用为FP32的1/2，适用于需要比INT8更高精度的量化场景。在某些混合\n                                                                                                                                                                精度训练中用于梯度和权重的中间表示。\nFP8 (E4M3)        8位浮点                                8位              1 / 4 / 3                                  H100上训练的大模型、Transformer Engine                 NVIDIA\n                                                                                                                                                                Hopper架构及更新GPU支持。动态范围较小，但精度相对E5M2更高。适用于模型权重和激活值的表示，在Transfor\n                                                                                                                                                                mer模型的前向传播中表现出色。\nFP8 (E5M2)        8位浮点                                8位              1 / 5 / 2                                  DeepSeek-R1 (671B)、H100上训练的大模型（梯度计算）           NVIDIA\n                                                                                                                                                                Hopper架构及更新GPU支持。动态范围比E4M3更广，但精度较低。适用于反向传播中的梯度计算，能更好地处理数值较大的\n                                                                                                                                                                梯度值。\nMXFP8 (E5M2)      Microscaling FP8                    8位              1 / 5 / 2 + 每32值共享的E8M0缩放                  OCP微缩放规范研究、实验性推理                               微缩放8位浮点格式，每32个E5M2值共享一个E8M0缩放因子（8位指数+0位尾数，表示2的幂次）。支持硬件加速，提供比\n                                                                                                                                                                标准FP8更大的动态范围（2^-127到2^128）。\nINT8              8位整型 (8-bit Integer)                8位              value = (int_value - zero_point) * scale   MobileNet、MobileBERT、各类Llama量化推理版本             内存占用为FP32的1/4，计算速度极快，尤其在有INT8加速单元的硬件上。广泛用于模型推理量化，通过缩放因子和零点将浮\n                                                                                                                                                                点数映射到[-128, 127]或的整数范围。\nFP4 (E2M1)        4位浮点标准格式                            4位              1 / 2 / 1 + 软件缩放因子                         实验性量化研究                                        基础的4位浮点格式，需要软件层面的缩放因子。相比FP16内存最多减少4倍，但与FP8相比存在明显的准确性下降风险。无硬件\n                                                                                                                                                                加速缩放支持。\nMXFP4             Microscaling FP4                    4位              1 / 2 / 1 + 每32值共享的E8M0缩放                  GPT-OSS (120B/20B)、OCP微缩放规范研究                  微缩放浮点格式，每32个E2M1值共享一个E8M0缩放因子（8位指数+0位尾数，表示2的幂次）。支持硬件加速缩放，相比F\n                                                                                                                                                                P16内存最多减少4倍。与FP8相比存在明显的准确性下降风险，但硬件效率更高。\nNVFP4             NVIDIA FP4                          4位              1 / 2 / 1 + 每16值共享的FP8缩放                   Blackwell架构上的超大规模LLM推理                         NVIDIA\n                                                                                                                                                                Blackwell架构引入。每16个值共享一个FP8缩放因子，采用二级微块缩放策略。支持硬件加速缩放，相比FP16内存最\n                                                                                                                                                                多减少4倍。特别适用于大型LLM推理，准确性下降风险相对较低。\nINT4              4位整型 (4-bit Integer)                4位              value = (int_value - zero_point) * scale   Kimi k2 thinking、Llama-GPTQ/AWQ、ChatGLM-INT4   极高的压缩率（FP32的1/8）。通常用于推理，对模型性能有一定挑战，需要配合先进的量化算法（如GPTQ,\n                                                                                                                                                                AWQ）来降低精度损失。将浮点数映射到[-8, 7]或的整数范围。\nINT2              2位整型 (2-bit Integer)                2位              value = (int_value - zero_point) * scale   极端量化实验、研究原型                                    极端压缩率（FP32的1/16）。将浮点数映射到4个离散值（如[-2, -1, 1,\n                                                                                                                                                                2]）。主要用于极端量化研究，需要特殊的量化技术来维持可用的模型性能。目前仍处于实验阶段。\n1-bit (Binary)    1位二值化 (e.g., BinaryNet, XNOR-Net)   1位              参数被量化为 {-1, +1}                            BinaryNet、XNOR-Net、边缘设备轻量级模型                   纯二值化网络，权重只有两个值。内存占用极小（FP32的1/32），使用XNOR和popcount操作替代乘法和加法，能效\n                                                                                                                                                                极高。主要用于边缘设备推理。准确性损失较大，需要特殊训练技术。\n1-bit (Ternary)   1位/三元量化 (e.g., BitNet b1.58)        ~1.58位          参数被量化为 {-1, 0, 1}                          BitNet b1.58、Microsoft 1-bit LLM研究             极致的压缩和能效。通过将权重约束到三个值，可以用log2(3) ≈\n                                                                                                                                                                1.58位来存储。这使得乘法运算可以被替换为更高效的加法/减法，极大地降低了计算成本。目前属于前沿研究领域。\n\n\nNVIDIA 专属精度格式#\n\nNVIDIA在其GPU架构中引入了多个专属的精度格式，以优化AI训练和推理性能：\n\n 1. TF32 (TensorFloat-32) - Ampere架构（2020年）引入，无需代码修改即可加速FP32训练\n 2. FP8 (E4M3 & E5M2) - Hopper架构（2022年，H100）引入，首个硬件支持的8位浮点格式\n 3. NVFP4 - Blackwell架构（2024年，B100/B200）引入，支持极致压缩的大模型推理\n\n这些格式均在NVIDIA的Tensor Core中获得硬件加速支持，代表了业界在低精度AI计算方面的领先技术。\n\n\n微缩放格式与E8M0缩放因子#\n\nE8M0（8位指数+0位尾数）是一种特殊的8位格式，专门用作微缩放（Microscaling, MX）格式的缩放因子，而非独立的数据格式：\n\n * 结构：8位指数，无尾数，无符号位\n * 表示范围：只能表示2的整数次幂，范围从2^-127到2^128\n * 用途：在MXFP8、MXFP4等微缩放格式中作为共享缩放因子，大幅扩展动态范围\n * 优势：通过块级共享缩放因子，在极低位宽下仍能保持足够的数值表示范围\n\n这种设计使得微缩放格式能够在保持硬件效率的同时，提供比传统低精度格式更好的数值稳定性。","routePath":"/guide/ai/Parameter-Precision-Formats-for-LLMs","lang":"zh","toc":[{"text":"大模型精度格式一览","id":"大模型精度格式一览","depth":2,"charIndex":-1},{"text":"NVIDIA 专属精度格式","id":"nvidia-专属精度格式","depth":2,"charIndex":7918},{"text":"微缩放格式与E8M0缩放因子","id":"微缩放格式与e8m0缩放因子","depth":2,"charIndex":8215}],"domain":"","frontmatter":{"title":null,"description":null,"date":20251110,"plainLanguage":"**大模型精度格式说白了就是：** 用多少位数字存一个参数——位数越少越省空间，但精度会下降。\n\n就像照片质量，4K（FP32）最清晰但文件大，1080P（FP16）清晰度还行文件小一半，720P（INT8）更小但有点糊，480P（INT4）很糊但超小。精度格式就是在\"清晰度\"和\"文件大小\"之间选择。\n\n**用大白话说：**\n想象称肉，电子秤精确到 0.01 克（FP32），弹簧秤精确到 10 克（INT8）。电子秤准但贵又慢，弹簧秤快又便宜但不够准。大模型精度就是选\"用什么秤\"。\n\n**常见精度（从高到低）：**\n\n**训练常用：**\n- **FP32（32位）**：最准，但占空间大、慢（基准）\n- **BF16（16位）**：🔥 **目前最流行**的训练精度，省一半空间，效果还行\n- **FP8（8位）**：DeepSeek-V3 用的，超省资源，H100 专属\n\n**推理常用：**\n- **Q8（8位整数）**：质量接近原版，文件小 4 倍\n- **Q5（5位）**：🔥 **推荐**，质量和大小平衡得最好\n- **Q4（4位）**：🔥 **最常用**，文件小 8 倍，质量还能接受\n- **Q2（2位）**：超大模型（200B+）测试用，质量会明显下降\n\n**极限压缩：**\n- **1-bit（二值化）**：只有 -1 和 +1 两个值，超省但质量很差\n\n**核心权衡：**\n- 精度高 = 效果好但占空间大、慢\n- 精度低 = 省空间、快但效果差\n\n**选择建议：**\n- **训练**：BF16（主流）或 FP8（有 H100 的话）\n- **推理**：Q4_K_M 或 Q5_K_M（普通人首选）\n- **测试**：Q2_K_M（超大模型，想快速体验）\n\n说白了，精度格式就是\"精度vs空间\"的权衡——就像买车，豪车（高精度）舒服但贵，经济型车（低精度）便宜但体验稍差，看你的需求和预算。\n"},"version":""},{"id":3,"title":"什么是 Dual Chunk Attention","content":"(图片来自论文 \"Training-Free Long-Context Scaling of Large Language Models\")\n\nDCA（Dual Chunk Attention, 双块注意力机制）：由香港大学等机构于 2024 年提出，是一种无需训练即可扩展大型语言模型上下文窗口的技术。\n\n它通过将长序列的注意力计算分解为基于块（chunk）的模块，使得 Llama2 70B (原生 4k 上下文) 能够支持超过 100k tokens\n的上下文窗口，而无需任何持续训练。\n\n\nDCA 的工作原理#\n\nDCA 基于这样一个核心思想：通过重新设计相对位置矩阵的构建方式，使其能够准确反映两个 token 之间的相对位置，同时保持预训练模型的原始位置索引和嵌入。\n\n\n三个核心组件#\n\n 1. 块内注意力（Intra-Chunk Attention）：\n    \n    * 处理同一块内的 tokens\n    * 维持原始的相对位置编码\n    * 每个块的大小小于预训练窗口大小\n\n 2. 块间注意力（Inter-Chunk Attention）：\n    \n    * 处理不同块之间的 tokens\n    * 通过特殊的位置索引映射保持长程依赖\n    * 避免位置索引超出预训练范围\n\n 3. 连续块注意力（Successive-Chunk Attention）：\n    \n    * 专门处理相邻块之间的 tokens\n    * 确保块边界处的连续性\n    * 维护局部性特征\n\n\n数学表示#\n\n对于长度为 $L$ 的序列，DCA 将其分割为 $C = \\lceil L/w \\rceil$ 个块，其中 $w$ 是块大小（通常设为预训练窗口大小）：\n\n块内注意力： $$A_{intra} = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right)\nV_i$$\n\n块间注意力： $$A_{inter} = \\text{softmax}\\left(\\frac{Q_i K_j^T}{\\sqrt{d_k}} \\cdot\nM_{ij}\\right) V_j$$\n\n其中 $M_{ij}$ 是位置掩码矩阵，确保相对位置不超出预训练范围。\n\n参数效率分析：\n\n * 原始注意力计算复杂度：$O(L^2)$\n * DCA 计算复杂度：$O(L \\cdot w)$，其中 $w \\ll L$\n * 内存消耗降低：从 $L^2$ 降至约 $L \\cdot w$\n * 与 Flash Attention 2 无缝集成，进一步优化效率\n\n\nDCA 的主要特点和优势#\n\n * 无需训练： DCA 是完全训练无关的方法，可以直接应用于现有的预训练模型，无需额外的微调或训练成本。\n * 显著扩展上下文： 可以将 4k 上下文窗口的模型扩展到 100k+ tokens，扩展倍数超过 25 倍。\n * 保持原始性能： 在扩展上下文的同时，困惑度（PPL）增长微乎其微，远优于传统的位置插值方法。\n * 计算效率高： 通过块分割大幅降低计算复杂度，与 Flash Attention 结合可在有限硬件上处理超长序列。\n * 正交性强： 可以与现有的位置编码缩放方法（如 PI、NTK）结合使用，进一步提升性能。\n * 实用性强： 在长文档问答、文档总结等实际任务中表现可媲美甚至超越专门微调的模型。\n\n\nDCA 的应用场景#\n\n * 长文档分析： 处理大型 PDF 文档、学术论文、法律合同等超长文本的理解和问答。\n * 扩展对话历史： 维护更长的对话上下文，提升聊天机器人的连贯性和记忆能力。\n * 代码理解： 分析大型代码库，理解跨文件的复杂依赖关系。\n * 文档摘要： 对超长文档进行准确的摘要生成，保持全局信息的完整性。\n * 信息检索： 在大型文档集合中进行精确的信息定位和提取。\n\n需要注意的是，DCA 主要解决的是上下文长度限制问题，而不是模型的核心能力提升。对于需要增强模型特定领域知识的场景，仍建议结合 RAG（检索增强生成）技术。\n\n\nDCA 的局限性#\n\n虽然 DCA 表现优异，但也存在一些限制：\n\n * 硬件要求： 处理超长序列仍需要较大的 GPU 内存，特别是对于 70B 等大规模模型。\n * 块大小敏感： 块大小的选择会影响性能，需要根据具体任务和模型进行调优。\n * 某些任务局限： 对于需要频繁跨块信息交互的任务，性能可能不如专门训练的长上下文模型。\n\n总结，DCA 是一项突破性的技术，通过巧妙的注意力机制重设计，在无需任何训练成本的前提下大幅扩展了语言模型的上下文处理能力，为长文档理解和处理开辟了新的可能性。\n\n随着对长上下文需求的不断增长，DCA 及其衍生技术将在 AI 应用中发挥越来越重要的作用。\n\n\n支持 DCA 的实现#\n\n * ChunkLlama (论文官方实现，支持多种 Llama 模型变体)\n * 可集成到现有的 Transformer 库中，如 Hugging Face Transformers\n\n\nReference#\n\n * Training-Free Long-Context Scaling of Large Language Models\n * ChunkLlama: Official Implementation\n * Rotary Position Embedding (RoPE)\n * Flash Attention: Fast and Memory-Efficient Exact Attention","routePath":"/guide/ai/What-is-Dual-Chunk-Attention","lang":"zh","toc":[{"text":"DCA 的工作原理","id":"dca-的工作原理","depth":2,"charIndex":248},{"text":"三个核心组件","id":"三个核心组件","depth":3,"charIndex":341},{"text":"数学表示","id":"数学表示","depth":3,"charIndex":659},{"text":"DCA 的主要特点和优势","id":"dca-的主要特点和优势","depth":2,"charIndex":1107},{"text":"DCA 的应用场景","id":"dca-的应用场景","depth":2,"charIndex":1436},{"text":"DCA 的局限性","id":"dca-的局限性","depth":2,"charIndex":1712},{"text":"支持 DCA 的实现","id":"支持-dca-的实现","depth":2,"charIndex":2009},{"text":"Reference","id":"reference","depth":2,"charIndex":2117}],"domain":"","frontmatter":{"title":"什么是 Dual Chunk Attention","description":"什么是 Dual Chunk Attention","date":20250809,"plainLanguage":"**双块注意力（DCA）说白了就是：** 把长文章切成小段处理，但又不失去整体联系——既省资源又能理解全文。\n\n就像读长篇小说，一次性读完太累（内存爆），DCA 就是\"分章节读，但记住章节之间的关系\"。既不用一次性把整本书装进脑子，又能理解全书。\n\n**用大白话说：**\n想象你要吃一大盘串（100 串）。传统方法是把所有串都放嘴里嚼（显存爆炸），DCA 就是\"一次吃 10 串，但记住之前吃过的味道\"。既不会撑死，又能品出整盘的味道。\n\n**核心机制（三个注意力）：**\n1. **块内注意力**：处理同一段内的内容（比如同一章的内容）\n2. **块间注意力**：处理不同段之间的关系（比如第 1 章和第 5 章的联系）\n3. **相邻块注意力**：处理相邻段的连接（比如第 3 章和第 4 章的衔接）\n\n**核心优势：**\n- **无需训练**：直接用在现有模型上，不用重新训练\n- **超长上下文**：4k 上下文→100k+ 上下文（扩展 25 倍+）\n- **效果不差**：困惑度几乎不降，甚至某些任务还更好\n- **省资源**：计算量从 O(n²) 降到 O(n·w)，省显存省时间\n\n**应用场景：**\n- 读整本书、处理长篇文档（论文、合同、报告）\n- 分析大型代码库（跨文件理解）\n- 超长对话（记住几千轮对话历史）\n\n**局限：**\n- 还是需要较大显存（虽然比原来小很多）\n- 块大小需要调优\n- 某些需要频繁跨块交互的任务效果稍差\n\n说白了，DCA 就是\"分段吃大象\"——把长文章切成小块，但保持块与块之间的联系，既能处理超长文本，又不会\"失忆\"。\n"},"version":""},{"id":4,"title":"什么是召回（Recall）","content":"召回（Recall）是机器学习分类任务中最重要的评估指标之一，用于衡量模型能否找到所有目标类别的对象。与准确率（Accuracy）关注整体正确性不同，召回率专注\n于回答一个关键问题：在所有真正的正样本中，我们的模型成功识别出了多少？\n\n这个指标在 RAG, 医疗诊断、欺诈检测、故障预警等关键应用中尤为重要，因为漏检的代价往往远高于误报。\n\n\n召回的工作原理#\n\n召回基于混淆矩阵（Confusion Matrix）的概念，通过分析模型在目标类别上的表现来评估其\"查全率\"。\n\n\n核心概念#\n\n我们用让大模型回答《孔乙己》这篇文章中出现了几次\"孔乙己\"这个名字作为例子. 召回率的计算涉及两个关键概念：\n\n 1. 真阳性（True Positive, TP）：\n    \n    * 实际为正类，模型也预测为正类\n    * 例如：\"孔乙己是站着喝酒而穿长衫的唯一的人\", 模型命中了这里, 内容也真的包含孔乙己, 即为TP\n\n 2. 假阴性（False Negative, FN）：\n    \n    * 实际为正类，但模型预测为负类\n    * 例如：\"掌柜取下粉板说，“孔乙己还欠十九个钱呢！”\", 模型没有命中这里, 但其实内容包含孔乙己, 即为FN\n\n\n数学表示#\n\n召回率的计算公式为：\n\n$$\\text{Recall} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP)} +\n\\text{False Negative (FN)}}$$\n\n召回率的取值范围是 0 到 1（或 0% 到 100%）：\n\n * Recall = 1.0：完美召回，找到了所有正样本, \"这篇文章总计出现了33次孔乙己, 分别在...\"\n\n * Recall = 0.0：最差情况，没有找到任何正样本. \"你扯淡, 这篇文章明明说的是猹, 没有什么孔乙己...\"\n\n\n召回能力对大模型的影响#\n\n在大型语言模型（LLM）的实际应用中，召回能力直接影响着模型在多个关键维度的表现：\n\n * 生成代码的时候变量的生命周期管理, 比如变量的调用出现在了变量声明前面, 这通常是召回出问题导致的.\n\n * RAG 系统中的信息检索召回,\n   在检索增强生成系统中，召回率决定了检索阶段能否找到所有相关文档，直接影响最终答案的完整性和准确性。\"我只搜到了一篇关于2024年财务报告的数据\"\n   实际上数据库里躺着10篇.\n\n * 长文本理解中的关键信息捕获, 处理长文档时，召回能力决定了模型能否捕获所有重要信息点，影响文档理解和摘要的完整性。比如我们举的孔乙己这个例子。\n\n * 多轮对话中的上下文召回,\n   在长对话场景中，模型需要准确召回历史信息以维持对话连贯性和个性化体验。聊了几次忘了用户叫啥，通常这个会用向量数据库来解决模型长期个性化记忆问题。\n\n * 指令理解的完整性,\n   复杂指令通常包含多个要求，召回能力不足会导致只执行部分需求，忽略重要约束条件。\"帮我打开电风扇关闭吹风机空调调到26度微波炉定时3分钟...结果大模型只记\n   得微波炉定时3分钟\"\n\n * 知识一致性与事实准确性, 召回能力影响模型知识表达的完整性，部分召回可能导致答案不够全面或缺乏跨域知识整合。 \"1,2,3,4,5,... 总计有3个!\"\n\n总的来说，在大模型应用中，召回能力的不足往往比准确率的下降更容易导致用户体验的显著下降。下期我们会讲另一个概念, 准确率.\n\n\n有没有大模型召回性能的评测#\n\n有的, Fiction.LiveBench 就是. 也是目前社区引用数量最多的. 但是 Fiction.LiveBench 不公布他们的测试过程和数据,\n所以最终结论很容易受到质疑. 但说实话也没有其它更好的可以参考了.\n\n\nReference#\n\n * Accuracy vs. precision vs. recall in machine learning\n * The Precision-Recall Trade-off","routePath":"/guide/ai/What-is-Recall","lang":"zh","toc":[{"text":"召回的工作原理","id":"召回的工作原理","depth":2,"charIndex":171},{"text":"核心概念","id":"核心概念","depth":3,"charIndex":239},{"text":"数学表示","id":"数学表示","depth":3,"charIndex":531},{"text":"召回能力对大模型的影响","id":"召回能力对大模型的影响","depth":2,"charIndex":813},{"text":"有没有大模型召回性能的评测","id":"有没有大模型召回性能的评测","depth":2,"charIndex":1462},{"text":"Reference","id":"reference","depth":2,"charIndex":1591}],"domain":"","frontmatter":{"title":"什么是召回（Recall）","description":"什么是召回（Recall）","date":20250812,"plainLanguage":"**召回（Recall）说白了就是：** 衡量 AI \"找全了没有\"的指标——漏了多少个。\n\n就像找东西，你要找家里所有的钥匙（总共 10 把）。你找到了 8 把，那召回率就是 80%（找到 8 把 / 总共 10 把）。漏掉的那 2 把就是\"假阴性\"——明明是钥匙，你没找到。\n\n**用大白话说：**\n想象你要数一篇文章里\"孔乙己\"出现了几次（实际 33 次）。如果你只数出 20 次，召回率 = 20/33 = 60%。如果你全数出来了，召回率 = 100%。召回率就是\"找全了没有\"。\n\n**核心公式：**\n召回率 = 找到的 / 实际有的 = TP / (TP + FN)\n- **TP（真阳性）**：该找的，找到了\n- **FN（假阴性）**：该找的，漏了\n\n**为啥重要：**\n- **医疗诊断**：漏诊（低召回）可能致命，宁可误诊也不能漏诊\n- **RAG 搜索**：漏掉关键文档，AI 就回答不全\n- **代码生成**：漏掉变量声明，代码就报错\n- **多轮对话**：忘了用户说过的话，对话就断了\n\n**对大模型的影响：**\n- 召回低 = 经常\"丢三落四\"、\"选择性失忆\"\n- 比如你问\"总结前面所有要点\"，它只总结了一半\n- 或者你让它\"关电风扇打开空调微波炉定时3分钟\"，它只记得最后一个\n\n说白了，召回率就是\"AI 的记忆力\"——能不能把该记住的都记住，该找到的都找到，不遗漏。\n"},"version":""},{"id":5,"title":"什么时候应该微调, 什么时候不应该微调?","content":"核心：微调主要教会 LLM \"模式\", 而非\"知识\".\n\n理解这一点至关重要.微调调整的是模型的顶层参数, 使其能够识别和复制特定的文本模式, 而不是向模型灌输新的事实性知识.\n\n\n什么时候应该考虑微调？#\n\n1. 输出特定格式或风格\n\n * 需求场景: 需要模型严格按照某种格式输出, 例如生成特定结构的 JSON、XML, 或者模仿某种独特的写作风格（如莎士比亚戏剧腔调）,\n   或者控制输出文本的长度火模式（例如, 总是输出单一句子的品牌口号, 输出固定词牌名格式的诗词等等）.\n\n2. 映射特定的输入到特定的输出\n\n * 需求场景: 您希望模型能够稳定地将某一特定类型的输入信息, 转化为另一种特定类型的输出信息.比如格式转换器.\n\n3. 专门化模型以执行狭窄、定义明确的任务\n\n * 需求场景: 您需要一个在某个特定、垂直领域或任务上表现卓越的模型, 而不是一个\"万金油\"式的通用模型.比如医学模型.\n\n4. 当提示工程\n\n * 需求场景: 您已经投入了大量精力进行提示词设计和优化, 但模型输出的稳定性、格式一致性或风格符合度仍旧无法达到预期.\n\n\n什么时候不应该（或者优先不考虑）微调？#\n\n1. 试图教授模型新的知识或事实\n\n * 原因: 微调主要影响模型的表层行为模式, 并不能有效地让模型\"记住\"或\"理解\"新的事实性信息.模型的知识主要来源于其大规模的预训练阶段.\n * 替代方案: 对于需要模型掌握并运用最新知识的场景, 应优先考虑**检索增强生成 ** 等技术.RAG 允许模型在生成文本时,\n   动态地从外部知识库（如数据库、文档集）中检索相关信息并融入到回答中.\n * 例子:\n   * 试图通过微调让模型记住您公司最新的产品规格参数.\n   * 期望模型通过微调学习并回答关于最近发生的全球新闻事件的问题.\n\n2. 期望模型获得复杂的推理能力\n\n * 原因: 微调通常不会显著提升模型的基础逻辑推理、数学计算或复杂问题解决能力.这些核心能力更多地是在预训练阶段形成的.\n * 例子: 期望通过微调使模型能够解决复杂的数学定理证明或进行多步骤的逻辑演绎.\n\n3. 训练数据集质量低下或缺乏多样性\n\n * 原因: 微调的效果高度依赖于训练数据的质量和多样性.如果您的数据集规模过小、包含大量错误、或者样本间的差异性不足（即所谓的\"数据聚类\",\n   所有样本都高度相似）, 那么微调很可能无法达到预期效果, 甚至可能损害模型原有的性能.结果就是 \"垃圾进, 垃圾出\" .\n * 例子: 使用少量高度同质化的邮件样本去微调一个通用邮件回复模型, 结果可能导致模型只会生成非常单一和刻板的回复.\n\n4. 任务目标可以通过提示工程轻松实现\n\n * 原因: 如果通过精心设计和优化的提示词, 模型已经能够很好地完成您的任务, 那么进行微调可能是一种不必要的资源投入（包括时间、计算资源和数据标注成本）.\n * 例子: 只需要模型进行简单的文本改写、总结或回答基于上下文的简单问题, 这些通常通过提示工程就能解决.\n\n\n微调的最佳实践总结#\n\n成功的微调遵循以下关键原则：\n\n 1. 明确目标：微调重在模式, 而非知识 清楚微调的强项是学习和复制文本的结构、风格和格式, 而不是记忆事实.\n\n 2. 定义清晰的输入-输出映射 您的训练数据应该清晰地展示您期望模型如何从给定的输入转换到期望的输出.\n\n 3. 拥抱数据多样性 训练样本应尽可能覆盖各种文体、风格、格式、长度和主题, 这能促使模型学习到更具泛化能力的模式.\n\n 4. 包含对抗性/边缘案例 引入一些格式错误、内容异常或可能触发非预期行为的样本, 可以增强模型在真实场景中的鲁棒性.\n\n 5. 警惕数据聚类 确保您的训练数据不是仅仅集中在某一非常狭窄的子集上,\n    除非您的目标就是创建一个针对该子集的超专用工具.广泛分布的数据能带来更灵活的生成能力.\n\n 6. 专注专门化任务 一次微调解决一个定义明确的问题.微调旨在打造\"专科医生\", 而非\"全科医生\".\n\n 7. 知识获取靠整合, 而非灌输 对于需要外部知识的任务, 应采用 RAG 等方法, 而不是试图通过微调将知识\"塞进\"模型.\n\n\n参考#\n\n * When to finetune LLMs and when NOT to (a practical guide from a pro who has\n   finetuned hundreds of models) by David Shapiro\n * Improve LLM performance for financial services use cases\n * The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs","routePath":"/guide/ai/When-to-Use-Fine-Tuning-and-When-Not-To","lang":"zh","toc":[{"text":"什么时候应该考虑微调？","id":"什么时候应该考虑微调","depth":2,"charIndex":90},{"text":"什么时候不应该（或者优先不考虑）微调？","id":"什么时候不应该或者优先不考虑微调","depth":2,"charIndex":481},{"text":"微调的最佳实践总结","id":"微调的最佳实践总结","depth":2,"charIndex":1269},{"text":"参考","id":"参考","depth":2,"charIndex":1738}],"domain":"","frontmatter":{"title":"什么时候应该微调, 什么时候不应该微调?","description":"什么时候应该微调, 什么时候不应该微调?","date":20250530,"plainLanguage":"**什么时候微调说白了就是：** 微调教\"技能\"（格式、风格），RAG 教\"知识\"（事实、数据）——别搞反了。\n\n就像学英语，微调是教\"语法、口音、写作风格\"（技能），RAG 是查\"词典、百科\"（知识）。你不会为了记住一个单词就重学一遍语法，对吧？\n\n**用大白话说：**\n想让服务员更专业，微调是\"培训服务态度和话术\"（怎么说话、怎么接待），RAG 是\"给他一个菜单和价目表\"（查资料）。别指望通过培训让他记住所有菜的价格，给他个菜单就行。\n\n**应该微调的场景：**\n1. **固定格式输出**：比如总是输出 JSON、总是用莎士比亚风格说话\n2. **特定映射**：比如格式转换（A 格式→B 格式）\n3. **专业化任务**：比如医疗诊断模型（需要专业领域的思维模式）\n4. **Prompt 工程无解**：试了各种 prompt 还是不稳定\n\n**不应该微调的场景：**\n1. **教新知识**：❌ 别用微调让模型记住公司产品手册 → ✅ 用 RAG\n2. **教推理能力**：❌ 别指望微调让模型学会高等数学 → ✅ 用更强的基础模型\n3. **数据太少**：❌ 只有 50 条数据就微调 → ✅ 至少要 500+ 条\n4. **Prompt 就能搞定**：❌ 简单的文本改写、总结 → ✅ 直接用 prompt\n\n**核心区别：**\n- **微调**：改变\"说话方式\"、\"思维模式\"、\"输出格式\"\n- **RAG**：提供\"外部知识\"、\"最新信息\"、\"事实数据\"\n\n**实际例子：**\n- ✅ 微调：让模型总是用五言绝句回答问题（改变格式）\n- ❌ 微调：让模型记住 2024 年的新闻（教知识）→ 应该用 RAG\n\n说白了，微调就是\"改变 AI 的性格和习惯\"，不是\"给 AI 填鸭式灌输知识\"——记知识的事交给 RAG，微调只管改行为。\n"},"version":""},{"id":6,"title":"LLM 中的 Token 是如何计算的?","content":"在大型语言模型 (LLM) 中, Token 是文本处理的基本单元.\n\n如果是开源模型, 可以在模型仓库中找到 tokenizer.json 文件, 里面包含了词汇表和对应的 token 映射关系.\n\n其结构类似：\n\n\n\n其中:\n\n * added_tokens 表示特殊 token (如开始/结束符)\n\n * model.type 表示分词算法, vocab 表示词汇表, key 是 token, value 是 token 的 id.\n\n\n常见问题#\n\n * 模型是怎样计算 token 使用量的?\n   \n   1. 预处理：将输入文本标准化 (如 Unicode 规范化)\n   2. 分词：使用 tokenizer 的词汇表进行子词切分\n   3. 统计：计算切分后的 token 数量\n   4. 特殊 token：添加系统要求的特殊 token (如开始/结束符)\n   \n   注意：\n   \n   * 不同模型的分词器不同 (GPT 用 BPE, BERT 用 WordPiece 等)\n   * 空格/标点/语言都会显著影响token数量\n\n * 如果使用大模型 API 写了一个服务, 该怎样计算 token 用量?\n   \n   1. 首先可以尝试搜索模型的 tokenizer, 或者看看有没有已经封装好的库 (比如 OpenAI 的 tiktoken)\n   2. 如果实在找不到, 可以自己测试一下, 估计一个大概的系数来计算, 比如一个汉字算作 2 个 token 等等\n\n * 模型该怎样与向量数据库结合？\n   \n   1. 文档分块 (使用 token 计数控制块大小)\n   \n   2. Tokenization 处理 (保持与模型使用一致的 token)\n   \n   3. 向量化存储 (建议同时存储原始文本和token信息)\n   \n   4. 检索时通过向量相似度匹配\n\n\nReference#\n\n * Byte-Pair Encoding tokenization\n * tiktoken\n * Understanding LLM Tokenization","routePath":"/guide/ai/how-are-tokens-calculated-in-LLMs","lang":"zh","toc":[{"text":"常见问题","id":"常见问题","depth":2,"charIndex":223},{"text":"Reference","id":"reference","depth":2,"charIndex":813}],"domain":"","frontmatter":{"title":"LLM 中的 Token 是如何计算的?","description":"LLM 中的 Token 是如何计算的?","date":20250218,"plainLanguage":"**Token 计算说白了就是：** AI 把文字切成\"单词碎片\"，然后按碎片收费。\n\n就像你发短信，以前按字收费（1毛/字），现在 AI 按 Token 收费。但 Token 不是\"字\"，而是\"词的碎片\"——比如\"苹果\"可能是 1 个 Token，\"iPhone\"可能是 2 个 Token（i + Phone）。\n\n**用大白话说：**\n想象你买烤串，老板不按\"串\"收费，而是按\"肉块\"收费。一串羊肉可能有 5 块肉，一串鸡翅可能有 3 块肉。Token 就是这些\"肉块\"——AI 把你的文字切成小块，按小块数收费。\n\n**切分规则：**\n- **英文**：通常一个单词 = 1-2 个 Token（\"apple\" = 1 个，\"application\" 可能 = 2 个）\n- **中文**：通常一个字 = 1-2 个 Token（\"苹\" 和 \"果\" 可能各算 1 个）\n- **标点/空格**：也算 Token\n\n**怎么算费用：**\n1. AI 先把你的话切成 Token（用分词器 tokenizer）\n2. 数一下总共多少个 Token\n3. 按 Token 数收费（比如 GPT-4：输入 $0.03/1K tokens，输出 $0.06/1K tokens）\n\n**实用技巧：**\n- 想省钱？用短句、少用复杂词汇\n- 中文比英文贵（因为中文 Token 多）\n- 可以用在线工具（如 tiktoken）提前算一下会用多少 Token\n\n**与向量数据库配合：**\n- 文档分块时，按 Token 数控制每块大小（比如每块 512 Token）\n- 这样既不会太长（超模型上下文），也不会太短（浪费空间）\n\n说白了，Token 就是 AI 的\"计费单位\"——就像你打车按公里算钱，用 AI 按 Token 算钱。\n"},"version":""},{"id":7,"title":"Transformer 的优化方案都有哪些?","content":"目前使用 Transformer 架构的模型, 都使用了一些优化方案来达到更好的效果或更高的性能, 所以我整理了常见的优化方案 (包括训练和推理),\n后续会详细讲解每个优化方案的技术细节.\n\n\n注意力机制优化#\n\n * Flash Attention\n   \n   * 减少内存访问和计算复杂度, 显著提升训练和推理速度\n   * 被 Llama2, Qwen, PaLM2, Mistral, DeepSeek 等采用\n   * 部分闭源模型可能采用类似技术或自研方案\n\n * Multi-Query Attention (MQA)\n   \n   * 减少 Key 和 Value 的头数, 降低内存使用和计算量\n   * 被 PaLM, Falcon, BLOOM 等采用\n\n * Grouped-Query Attention (GQA)\n   \n   * MQA 的改进版本, 通过分组共享 Key/Value 矩阵（而非完全独立）实现性能和效率的平衡\n   * 被 Llama2, PaLM2, Gemini, Mistral, DeepSeek 等采用\n\n\n位置编码优化#\n\n * RoPE (Rotary Position Embedding)\n   \n   * 通过旋转矩阵实现相对位置编码, 支持更好的长度外推性\n   * 被 Llama, DeepSeek, Qwen, Mistral, Falcon, PaLM 等广泛采用\n   * 支持 NTK-aware 插值和 Dynamic NTK 等长度扩展方法\n\n * ALiBi (Attention with Linear Biases)\n   \n   * 线性注意力偏置, 有助于外推到更长序列\n   * 被 Bloom, Stable LM 等采用\n\n\n架构优化#\n\n * 并行计算优化\n   \n   * 通过解耦注意力层和前馈层的计算路径，实现：\n     1. 减少层间计算依赖，提升计算并行度\n     2. 优化内存访问模式，降低显存占用\n     3. 提高计算资源利用率（特别是 Tensor Core）\n     4. 支持更大 batch size 的训练\n   * 典型实现方案：\n     * PaLM 并行结构：同时计算注意力层和前馈层，结果合并后做残差连接\n     * GPT-3 模型并行：通过张量/流水线并行实现超大规模模型训练\n   * 被 PaLM, GPT-3, T5, Megatron-LM 等模型采用\n\n * 激活函数优化\n   \n   * 使用 SwiGLU 替代标准 FFN 中的激活函数, 提供更好的性能\n   * 被 PaLM, Llama2, Gemini, Qwen 等采用\n\n * 稀疏专家模型 (MoE)\n   \n   * 通过多个专家网络实现大规模参数扩展\n   * 被 Mixture-of-Experts, Switch Transformer 等采用\n\n\n上下文长度扩展#\n\n * 滑动窗口注意力\n   \n   * 局部注意力机制, 减少内存使用, 支持更长序列处理\n   * 被 Longformer, BigBird 等采用\n\n * 稀疏注意力\n   \n   * 只关注重要的 token, 降低计算复杂度, 提高处理长序列的能力\n   * 被 Sparse Transformer, Reformer, Longformer 等采用\n\n\n内存优化#\n\n * 参数共享\n   * 跨层参数复用, 减少模型参数量, 降低内存需求\n   * 被 ALBERT, T5 等采用\n\n\n训练优化#\n\n * 混合精度训练\n   \n   * FP16/BF16 混合精度训练广泛应用于大模型训练\n   * FP8 目前主要用于推理阶段（如 NVIDIA H100）, 但 DeepSeek-V3 使用了 FP8 训练, 带来了巨大的成本优势,\n     甚至最新的论文还尝试了 FP4 训练\n   * 大多数现代大模型使用 BF16 训练\n\n * 梯度检查点\n   \n   * 训练时动态重计算, 节省显存, 略微增加计算时间\n   * 被大模型训练普遍采用\n\n\n推理优化#\n\n * KV Cache\n   \n   * 通过缓存历史 token 的 Key/Value 矩阵实现：\n     1. 避免重复计算历史 token 的注意力结果\n     2. 减少解码时的计算量（复杂度从 O(n²) 降为 O(n)）\n     3. 降低内存带宽需求，提升推理速度\n     4. 支持更长的上下文处理\n   * 内存管理策略：\n     * 预分配固定长度内存\n     * 动态扩展机制（如 vLLM 的 PagedAttention）\n   * 被 Llama 系列、GPT 系列、PaLM、Gemini、Qwen 等主流模型采用\n\n * 量化技术\n   \n   * INT8/INT4 量化, 减少模型大小和内存占用, 加快推理速度\n   * 主流量化方法包括 GPTQ, AWQ 等\n   * 支持 per-tensor 和 per-channel 量化粒度\n   * 被 LLaMA-2, ChatGLM, Qwen, Mistral, Yi 等采用\n\n * 推理加速技术\n   \n   * 推测解码 (Speculative Decoding)\n   * 连续批处理 (Continuous Batching)\n   * 动态批处理, 提高 GPU 利用率\n\n\n特定硬件优化#\n\n * GPU 特化\n   * CUDA 核心优化\n   * Tensor Core 利用\n   * 显存访问优化\n   * 算子融合\n   * 内存布局优化\n\n这些优化方案通常会组合使用，不同的模型会根据自己的具体需求选择合适的优化方案。比如：\n\n * Llama2: 采用 GQA + RoPE + Flash Attention\n * PaLM2: 使用 GQA + 并行计算优化\n * Qwen: 采用 Flash Attention + RoPE\n\n接下来几期我会详细介绍这些优化方案的技术细节, 以及它们在不同模型中的应用.","routePath":"/guide/ai/how-to-optimize-transformer","lang":"zh","toc":[{"text":"**注意力机制优化**","id":"注意力机制优化","depth":3,"charIndex":-1},{"text":"**位置编码优化**","id":"位置编码优化","depth":3,"charIndex":-1},{"text":"**架构优化**","id":"架构优化","depth":3,"charIndex":-1},{"text":"**上下文长度扩展**","id":"上下文长度扩展","depth":3,"charIndex":-1},{"text":"**内存优化**","id":"内存优化","depth":3,"charIndex":-1},{"text":"**训练优化**","id":"训练优化","depth":3,"charIndex":-1},{"text":"**推理优化**","id":"推理优化","depth":3,"charIndex":-1},{"text":"**特定硬件优化**","id":"特定硬件优化","depth":3,"charIndex":-1}],"domain":"","frontmatter":{"title":"Transformer 的优化方案都有哪些?","description":"Transformer 的优化方案都有哪些?","date":20250127,"plainLanguage":"**Transformer 优化方案说白了就是：** 让 AI \"跑得更快、吃得更少\"的各种技巧。\n\n就像汽车改装，有的加涡轮增压（Flash Attention），有的减重（量化），有的换更省油的引擎（MoE）。目标都是：更快、更省、效果还不能差。\n\n**用大白话说：**\n想象你开餐馆，生意太好忙不过来。你可以：1) 雇更多人（扩大模型），2) 让厨师炒菜更快（优化算法），3) 简化菜单（减少参数），4) 用更好的炉灶（更好的硬件）。Transformer 优化就是这些\"餐馆提速\"的方法。\n\n**核心优化方向：**\n\n**1. 注意力机制优化**（最关键）\n- **Flash Attention**：减少内存读写，速度快 3-4 倍\n- **MQA/GQA**：共享部分参数，省显存又不太影响效果\n\n**2. 位置编码优化**\n- **RoPE**：让模型能处理更长的文本\n- **ALiBi**：另一种长文本方案\n\n**3. 架构优化**\n- **并行计算**：多个模块同时工作，不用排队\n- **MoE（专家模型）**：100 个专家，每次只用 8 个\n\n**4. 内存优化**\n- **KV Cache**：记住之前算过的东西，不用重复计算\n- **量化**：从 32 位降到 4 位，模型小了 8 倍\n\n**5. 训练优化**\n- **混合精度**：用 FP16/BF16 训练，又快又省显存\n- **梯度检查点**：边算边删中间结果，省显存\n\n**6. 推理优化**\n- **推测解码**：先猜几个词，猜对了就直接用\n- **连续批处理**：多个请求一起处理，提高效率\n\n说白了，Transformer 优化就是\"各显神通让模型跑得又快又好\"——就像 F1 赛车，每个细节都要优化到极致。\n"},"version":""},{"id":8,"title":"如何本地运行 GGUF 格式的 LLM 模型?","content":"本篇内容教大家如何本地运行 GGUF 格式的 LLM 模型. 这里以最新的 DeepSeek-R1-Distill-Qwen-32B-GGUF 模型为例.\n\n\n模型下载#\n\n我比较喜欢 unsloth 团队的量化版本, 所以这里下载的是 unsloth 团队的量化版本.\n\n下载地址 https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/tree/main\n\n\n\n注意这个是个合集包, 里面有 Q2-Q8 的量化版本, 选中你喜欢的量化版本, 点击这个下载按钮即可. 不需要全部下载.\n\n\n编译 llama.cpp#\n\n由于这个模型比较新 (2025-01-21发布的), 所以需要编译最新的 llama.cpp 才能支持这个模型.\n\n首先下载 llama.cpp 的源码, 然后编译.\n\n\n\n如果你的电脑没有 cmake, 请自己搜索如何安装 cmake. 问问 LLM 是好方法.\n\n\n模型运行#\n\n直接运行编译好的 llama.cpp 即可.\n\n\n\n\nok 啦~!#\n\n在浏览器访问你指定的 IP 和端口, 例如 http://192.168.1.2:9990 即可.\n\n","routePath":"/guide/ai/how-to-run-gguf-LLM-model","lang":"zh","toc":[{"text":"模型下载","id":"模型下载","depth":2,"charIndex":79},{"text":"编译 llama.cpp","id":"编译-llamacpp","depth":2,"charIndex":282},{"text":"模型运行","id":"模型运行","depth":2,"charIndex":432},{"text":"ok 啦~!","id":"ok-啦","depth":2,"charIndex":466}],"domain":"","frontmatter":{"title":"如何本地运行 GGUF 格式的 LLM 模型?","description":"如何本地运行 GGUF 格式的 LLM 模型?","date":20250122,"plainLanguage":"**本地运行 GGUF 模型说白了就是：** 三步搞定——下载模型、编译工具、运行。\n\n就像你要在家里看电影，需要：1) 下载电影文件，2) 安装播放器，3) 双击播放。运行 GGUF 模型也是这个思路。\n\n**用大白话说：**\n想在家做川菜，你需要：1) 买食材（下载模型），2) 准备锅碗瓢盆（编译 llama.cpp），3) 开火做菜（运行模型）。就这么简单。\n\n**三步流程：**\n\n**第一步：下模型**\n- 去 Hugging Face 找你想要的模型（比如 DeepSeek-R1）\n- 选一个量化版本（推荐 Q4 或 Q5）\n- 下载，注意文件可能几十 GB\n\n**第二步：编译 llama.cpp**\n- 下载 llama.cpp 源码\n- 运行几行命令编译（就像安装软件）\n- 如果模型很新，确保用最新版 llama.cpp\n\n**第三步：运行**\n- 一行命令启动：`llama-server -m 模型文件路径`\n- 浏览器打开 `localhost:端口号`\n- 开始聊天！\n\n**注意事项：**\n- 确保电脑有足够显存/内存（Q4 的 32B 模型大概需要 20GB）\n- 新模型可能需要新版 llama.cpp 才能支持\n- 可以用 `--host 0.0.0.0` 让局域网其他设备也能访问\n\n说白了，运行 GGUF 模型就是\"下载-编译-运行\"三步曲，比想象中简单多了。\n"},"version":""},{"id":9,"title":"什么是 AI Agent","content":"(image from generativeai.pub)\n\nAI Agent（人工智能代理, 可不是AI特工哦）目前的定义已经很混乱了。但就实际使用来讲 AI Agent\n是旨在增强大语言模型，最终达到可以自动完成任务的系统。\n\nAI Agent 一般由大语言模型 (充当大脑), 调度/编排系统 (充当触发器和任务决策), 工具调用 (充当手脚), 记忆与学习 (充当经验), 多模态感知\n(充当眼睛和耳朵) 等组成整。\n\nAI Agent 可以是极其简单的 prompt + LLM + 触发器组成, 比如一个中转英的翻译 Agent:\n\n\n\n（text 是用户输入的文本, 会由触发器自动拼进去）\n\n这样一个简单的 prompt 就是一个AI Agent, 用户只要点击这个翻译 Agent 的图标,\n进去后在输入框输入文本后点击翻译即可，整个过程不需要任何人工干预。\n\nAI Agent 也可以很复杂, 下图就是 ComfyUI 中将给定人物照片生成另一张人物照片的姿势的 AI Agent:\n\n\n\n(图片来自 www.runcomfy.com)\n\n下面则是 refly.ai 中一个给定命题, 自动搜索并生成文章的 AI Agent 的示例:\n\n\n\n(图片来自 refly.ai)","routePath":"/guide/ai/what-is-AI-Agent","lang":"zh","toc":[],"domain":"","frontmatter":{"title":"什么是 AI Agent","description":"什么是 AI Agent","date":20250220,"plainLanguage":"**AI Agent 说白了就是：** 给 AI 配了\"手脚\"，让它不仅能说，还能干。\n\n就像你雇了个助理，他不仅能听懂你的话（大语言模型），还能帮你查资料（工具调用）、记住之前的事（记忆）、看图片听声音（多模态）。AI Agent 就是这样一个\"全能助理\"。\n\n**用大白话说：**\n最简单的 AI Agent 就像你手机里的\"翻译助手\"——你点一下，输入中文，它自动翻译成英文。复杂一点的，就像你让助理\"帮我写篇文章\"，他会自己去查资料、整理思路、写出来，全程不用你管。\n\n**核心组成：**\n1. **大脑（LLM）**：理解你的意图，做决策\n2. **手脚（工具调用）**：能操作其他系统，比如搜索、写代码、发邮件\n3. **记忆**：记住之前的对话和结果\n4. **感知**：能看图片、听声音、读文档\n\n**简单 vs 复杂：**\n- **简单版**：一个 prompt + 触发器，比如\"翻译这段文字\"\n- **复杂版**：能自动规划任务、调用多个工具、处理多步骤流程\n\n说白了，AI Agent 就是让 AI 从\"聊天机器人\"变成\"能干活的小助手\"。它不仅能回答你的问题，还能帮你完成任务。\n"},"version":""},{"id":10,"title":"","content":"(图片来源：team-gpt.com)\n\nAI 幻觉 (AI Hallucination) 是指人工智能模型生成的看似合理但实际上不准确、虚构或与事实不符的内容。\n\n这种现象在生成式 AI 中普遍存在，模型会以高度自信的姿态输出错误信息，以我的经验, 务必对AI生成的任何数值内容保持足够的警惕。\n\n\n幻觉产生机制#\n\nAI 幻觉的核心产生机制包含三个关键环节：\n\n * 模式匹配驱动：基于统计规律而非事实理解生成文本\n * 知识边界模糊：无法区分训练数据中的事实与虚构内容\n * 置信度错位：流畅的表达形式与内容准确性脱钩\n\n这种机制导致模型可能将不同领域的知识片段进行不合理组合。\n\n\n主要表现形式#\n\n * 事实扭曲：篡改真实事件的时间、地点等关键要素\n * 学术造假：虚构不存在的论文、实验数据和学术概念\n * 逻辑断裂：在连续对话中出现自相矛盾的陈述\n * 过度泛化：将特定场景的规律错误推广到其他领域\n\n\n技术局限性#\n\n * 数据依赖困境：输出质量受限于训练数据的时效性和完整性\n * 概率生成本质：优先选择统计上合理而非事实正确的表达\n * 验证机制缺失：缺乏实时的事实核查和逻辑自检能力\n * 上下文敏感度：长对话中错误信息可能被反复强化\n\n\n缓解方案#\n\n * 混合架构：结合检索增强（RAG）与生成模型\n * 置信度校准：输出时附带准确性概率评估\n * 溯源机制：标注生成内容的参考来源(如:Google Gemini能标注生成内容来源和对生成内容进行来源审查)\n * 动态修正：建立实时反馈的错误修正回路\n\n\nRefs#\n\n * Understanding the AI Hallucination Phenomenon","routePath":"/guide/ai/what-is-AI-Hallucination","lang":"zh","toc":[{"text":"幻觉产生机制","id":"幻觉产生机制","depth":2,"charIndex":150},{"text":"主要表现形式","id":"主要表现形式","depth":2,"charIndex":294},{"text":"技术局限性","id":"技术局限性","depth":2,"charIndex":408},{"text":"缓解方案","id":"缓解方案","depth":2,"charIndex":530},{"text":"Refs","id":"refs","depth":2,"charIndex":666}],"domain":"","frontmatter":{"title":null,"description":null,"date":20250309,"plainLanguage":"**AI 幻觉说白了就是：** AI \"一本正经地胡说八道\"。\n\n就像你问朋友\"拿破仑哪年出生的？\"他不知道，但不好意思说不知道，就编了个日期，还说得特别自信。AI 幻觉就是这样——不知道就瞎编，而且编得很像真的。\n\n**用大白话说：**\n想象你问老板\"今年公司业绩咋样？\"他其实不知道具体数字，但为了显得专业，随口说\"增长了 73.6%\"。你听了觉得挺专业，实际上他是瞎编的。AI 幻觉就是这样——用自信掩盖无知。\n\n**典型表现：**\n1. **编数据**：随口说个数字，比如\"研究表明 92.7% 的人...\"（根本没这研究）\n2. **编论文**：引用一篇不存在的论文，还编出作者、期刊、发表时间\n3. **篡改事实**：把历史事件的时间、地点搞错\n4. **自相矛盾**：前面说 A，后面说非 A，自己打脸\n\n**为什么会这样？**\n- AI 本质是\"概率模型\"，它不\"理解\"真假，只知道\"哪个词接下来概率最大\"\n- 训练数据里有真有假，AI 分不清\n- 没有\"我不知道\"这个选项，只能硬着头皮编\n\n**怎么应对：**\n- **核实关键信息**：特别是数字、日期、专有名词，务必自己查证\n- **要求提供来源**：让 AI 标注信息来源（虽然来源也可能是编的）\n- **用 RAG**：让 AI 先查资料再回答，减少瞎编的机会\n- **保持怀疑**：对 AI 说的话保持批判性思维\n\n说白了，AI 幻觉就是\"AI 也会吹牛\"——而且吹得特别自信。记住：AI 是工具，不是权威，别全信。\n"},"version":""},{"id":11,"title":"什么是 LLM 蒸馏技术?","content":"LLM 蒸馏 (Distillation) 是一种技术，用于将大型语言模型 (LLM)\n的知识转移到较小的模型中。其主要目的是在保持模型性能的同时，减少模型的大小和计算资源需求。通过蒸馏技术，较小的模型可以在推理时更高效地运行，适用于资源受限的环境\n。\n\n\n蒸馏过程#\n\n蒸馏过程通常包括以下几个步骤：\n\n * 训练教师模型：首先训练一个大型且性能优越的教师模型。\n * 生成软标签：使用教师模型对训练数据进行预测，生成软目标 (soft targets) ，这些目标包含了教师模型的概率分布信息。\n * 训练学生模型：使用软目标 (soft targets) 和原始训练数据 (hard targets) 来训练较小的学生模型，使其能够模仿教师模型的行为。\n   这种方法不仅可以提高模型的效率，还可以在某些情况下提高模型的泛化能力。\n\n\n蒸馏的优点#\n\n * 减少模型大小和计算资源需求\n * 增加推理速度\n * 易于访问和部署\n\n(其实就是小模型相对于大模型的优点)\n\n\n蒸馏可能存在的问题#\n\n * 信息丢失：由于学生模型比教师模型小，可能无法完全捕捉教师模型的所有知识和细节，导致信息丢失。\n * 依赖教师模型：学生模型的性能高度依赖于教师模型的质量，如果教师模型本身存在偏差或错误，学生模型可能会继承这些问题。\n * 适用性限制：蒸馏技术可能不适用于所有类型的模型或任务，尤其是那些需要高精度和复杂推理的任务。\n\n\n典型例子#\n\n * GPT-4o (教师模型) 中提炼出 GPT-4o-mini (学生模型)\n * DeepSeek-R1 (教师模型) 中提炼出 DeepSeek-R1-Distill-Qwen-32B (学生模型) (这个不是传统意义上的蒸馏了,\n   是蒸馏+数据增强+微调)\n\n\n其他蒸馏技术#\n\n * 数据增强: 使用教师模型生成额外的训练数据。通过创建更大、更具包容性的数据集，学生可以接触到更广泛的场景和示例，从而提高其泛化性能。\n\n * 中间层蒸馏: 将知识从教师模型的中间层转移到学生。通过学习这些中间表示，学生可以捕获更详细和结构化的信息，从而获得更好的整体表现。\n\n * 多教师蒸馏: 通过汇总不同教师模型的知识，学生模型可以实现更全面的理解并提高稳健性，因为它整合了不同的观点和见解。\n\n\n题外话#\n\n强烈推荐读一下下面引用中的第一个和第二个论文, 里面有详细的蒸馏技术介绍. 第二个论文是 Geoffrey Hinton 写的, 他正式在这篇论文里面首次引入了\nsoft targets 和 hard targets 的概念.\n\n\nRefs#\n\n * Knowledge Distillation: A Survey\n * Distilling the Knowledge in a Neural Network\n * LLM Distillation Explained: Applications, Implementation & More","routePath":"/guide/ai/what-is-LLM-distill","lang":"zh","toc":[{"text":"蒸馏过程","id":"蒸馏过程","depth":2,"charIndex":128},{"text":"蒸馏的优点","id":"蒸馏的优点","depth":2,"charIndex":370},{"text":"蒸馏可能存在的问题","id":"蒸馏可能存在的问题","depth":2,"charIndex":438},{"text":"典型例子","id":"典型例子","depth":2,"charIndex":613},{"text":"其他蒸馏技术","id":"其他蒸馏技术","depth":2,"charIndex":758},{"text":"题外话","id":"题外话","depth":2,"charIndex":971},{"text":"Refs","id":"refs","depth":2,"charIndex":1093}],"domain":"","frontmatter":{"title":"什么是 LLM 蒸馏技术?","description":"什么是 LLM 蒸馏技术?","date":20250123,"plainLanguage":"**LLM 蒸馏说白了就是：** 让\"大师傅\"教\"小徒弟\"，让小徒弟学会大师傅的本事。\n\n就像武侠小说里，高手把自己的内力传给徒弟。蒸馏就是让大模型把\"知识\"传给小模型，让小模型也能干得差不多。\n\n**用大白话说：**\n想象一个米其林三星厨师（大模型），他做菜很厉害但很贵很慢。现在让他教个学徒（小模型），学徒学会后，虽然做得没师傅那么好，但也有 90 分了，而且便宜快速。\n\n**蒸馏流程：**\n1. **训练大师傅**：先训练一个超强的大模型（教师模型）\n2. **大师傅示范**：让大模型预测一些问题，记录下它的\"思考过程\"（概率分布）\n3. **小徒弟学习**：让小模型学习大模型的\"思考过程\"，而不只是学标准答案\n\n**关键概念：**\n- **硬标签（hard targets）**：标准答案，比如\"这是只猫\"\n- **软标签（soft targets）**：大模型的概率，比如\"90% 是猫，8% 是狗，2% 是狐狸\"\n- 小模型学的是软标签，包含了更多信息\n\n**好处：**\n- 小模型体积小、速度快、省资源\n- 效果接近大模型（通常 90%-95%）\n- 可以部署到手机、边缘设备\n\n**缺点：**\n- 总会损失一些能力\n- 依赖大模型的质量（大模型错了，小模型也错）\n\n说白了，蒸馏就是\"师傅带徒弟\"的 AI 版本——让小模型站在巨人的肩膀上，用更少的资源做差不多的事。\n"},"version":""},{"id":12,"title":"什么是 LLM 微调技术?","content":"LLM 微调 (Fine-tuning)\n是一种通过特定领域数据对预训练语言模型进行二次训练的技术。目的是在保持模型通用语言理解能力的基础上，使其适应特定任务或领域。\n\n通过微调技术，基础模型可以显著提升在目标领域（如医疗、法律、金融等）的表现。\n\n\n微调过程#\n\n典型微调流程包括以下步骤：\n\n * 选择基础模型：使用预训练好的通用语言模型（如 Qwen、LLaMA 等）作为起点\n * 准备领域数据：收集与目标领域相关的标注数据集\n * 调整超参数(Hyperparameters)：设置合适的学习率、训练轮次等参数（通常比预训练时更小, 这些都是超参数）\n * 领域适应训练：在保持原有参数的基础上，用领域数据继续训练模型\n * 评估验证：通过领域特定的评估指标检验微调效果\n * 迭代上述过程来获得更好的结果直到满意\n\n\n微调的优点#\n\n * 显著提升特定任务/领域表现\n * 数据效率高（相比从头训练）\n * 计算资源需求相对较少\n * 灵活性高（可针对不同场景定制）\n\n\n微调可能存在的问题#\n\n * 过拟合风险：过度适配训练数据可能导致泛化能力下降\n\n * 领域依赖：在非目标领域表现可能下降\n\n * 需要领域专业知识：数据准备和评估指标设计需要领域知识\n\n * 计算资源需求：虽然比预训练少，但仍需要可观资源\n\n\n题外话#\n\n推荐阅读 ULMFiT 论文（首次提出 NLP 三阶段微调框架）和 BERT 论文的微调章节，这两个工作奠定了现代语言模型微调的基础方法论。\n\n\nRefs#\n\n * Universal Language Model Fine-tuning for Text Classification\n * The Ultimate Guide to LLM Fine Tuning: Best Practices & Tools\n * Fine-tuning vs Prompt Engineering","routePath":"/guide/ai/what-is-LLM-fine-tuning","lang":"zh","toc":[{"text":"微调过程","id":"微调过程","depth":2,"charIndex":125},{"text":"微调的优点","id":"微调的优点","depth":2,"charIndex":364},{"text":"微调可能存在的问题","id":"微调可能存在的问题","depth":2,"charIndex":441},{"text":"题外话","id":"题外话","depth":2,"charIndex":564},{"text":"Refs","id":"refs","depth":2,"charIndex":643}],"domain":"","frontmatter":{"title":"什么是 LLM 微调技术?","description":"什么是 LLM 微调技术?","date":20250208,"plainLanguage":"**LLM 微调说白了就是：** 给通用 AI 模型\"上培训班\"，让它在某个专业领域更牛。\n\n就像你从大学毕业，学了很多基础知识，但要进医院当医生，还得接受专业培训。微调就是给 AI 也上这样的\"专业培训班\"。\n\n**用大白话说：**\n想象你会做基本的家常菜（预训练模型），但想开川菜馆，就得专门学川菜的做法（微调）。你不用从切菜、炒菜这些基础学起，只需要学川菜的特色做法（麻辣、花椒等），很快就能上手。\n\n**微调流程：**\n1. **选基础模型**：找个已经训练好的通用模型（比如 GPT、Qwen）\n2. **准备专业数据**：收集你想让它擅长的领域的资料（医疗、法律等）\n3. **小步训练**：用这些专业资料继续训练模型，但步子要小（学习率低）\n4. **测试调整**：看效果，不好就再调整\n\n**好处：**\n- 省时省钱：比从头训练快多了\n- 效果好：在专业领域表现更好\n- 灵活：可以针对不同任务定制\n\n**缺点：**\n- **过拟合**：太专注某个领域，可能在其他领域变傻\n- **需要专业知识**：得知道怎么准备数据、怎么评估效果\n\n说白了，微调就是\"让通才变专才\"的过程——不是从头培养，而是在已有基础上加强某个方向。\n"},"version":""},{"id":13,"title":"什么是 LoRA","content":"(图片来自 Aporia)\n\nLoRA（Low-Rank Adaptation, 低秩自适应）：由微软于 2021 年提出，是一种高效微调大型语言模型（LLM）的技术。\n\n注意区分无线通信领域的 LoRa (Long Range).\n\n它通过在冻结的预训练模型权重旁添加小型可训练的\"低秩适应\"层，显著降低了微调过程中的计算和内存需求，无需重新训练整个模型。\n\n\nLoRA 的工作原理#\n\n(如果不知道什么是低秩矩阵, 可以看上一篇内容 什么是矩阵的秩？什么是低秩矩阵？)\n\nLoRA 基于这样一个假设：模型微调时的权重更新矩阵 ΔW 具有内在的低秩性质。具体推导过程：\n\n 1. 传统微调：假设原始预训练权重矩阵为 $W \\in \\mathbb{R}^{d \\times k}$，其中：\n    \n    * d：输入维度（例如 transformer 的 hidden_size）\n    * k：输出维度 全参数微调需要更新整个矩阵，参数量为 $d \\times k$\n\n 2. 低秩分解：引入两个更小的矩阵：\n    \n    * $W_A \\in \\mathbb{R}^{d \\times r}$（降维矩阵）\n    * $W_B \\in \\mathbb{R}^{r \\times k}$（升维矩阵） 其中秩 r ≪ min(d, k)\n\n 3. 更新机制：将权重更新表示为：\n    \n    $W_{updated} = W + \\Delta W = W + W_A \\cdot W_B$ 这相当于用低秩矩阵的乘积来近似全秩更新\n\n 4. 参数冻结：原始权重矩阵 W 保持冻结（不更新），只训练 $W_A$ 和 $W_B$\n\n参数效率分析：\n\n * 原始可训练参数：$d \\times k$（例如 d=1024, k=1024 → 1,048,576 参数）\n * LoRA 参数：$r \\times (d + k)$（取 r=8 → 8×(1024+1024)=16,384 参数, 参数减少约 64 倍）\n * 实际应用中需要权衡：r 值过小可能导致性能下降，r 值过大会降低效率（通常根据模型规模选择 4-128 之间的值）\n\n这种低秩适应有效的深层原因是：模型在不同任务间的知识迁移主要发生在低维子空间，通过调整这些关键方向就能实现高效适应。\n\n\nLoRA 的主要特点和优势#\n\n * 参数高效： LoRA 仅训练少量额外参数，而不是完整的模型权重。典型场景下可训练参数减少 10-100 倍（具体取决于原模型尺寸和r值）\n\n * 训练速度快： 由于需要更新的参数大幅减少，LoRA 微调比全参数微调速度更快，训练时间更短。\n\n * 存储空间小： LoRA 适配器通常只有几十到几百 MB，而完整的模型可能需要几十 GB。这使得存储和分发多个专门的微调版本变得更加实际。\n\n * 推理性能优化： LoRA 权重可以在推理前合并回原始模型，不会增加推理时的计算开销。\n\n * 可组合性： 多个 LoRA 适配器可以被合并或按不同比例混合，创建具有混合能力的模型。\n\n * 质量保持： 在许多任务中，LoRA 微调的性能与全参数微调相当甚至更好，同时减少了过拟合风险。\n\n\nLoRA 的应用#\n\n * 个性化模型： 为特定领域或任务快速创建专门的模型变体。\n * 资源受限环境： 在计算资源有限的环境中实现大型模型的微调。\n * 多任务学习： 为不同任务维护单独的 LoRA 适配器，而不是完整的模型副本。\n * 参数高效迁移学习： 将通用模型快速适配到专业领域（如医疗、法律、金融等）。\n * 在线学习： 持续更新模型以适应新数据，而不需要重新训练完整模型。\n\n但是要注意, 微调更应针对模型能力上的更新, 而不是知识上的更新. 知识上的更新应该交给 RAG 来完成.\n\n总结，LoRA 是一种突破性的微调技术，通过显著降低计算需求和存储成本，使得大型语言模型的个性化和专业化变得更加实用和广泛可行。\n\n随着模型规模不断增长，LoRA 及其变体在 AI 模型部署和适配中的重要性将继续提升。\n\n\n支持 LoRA 的框架#\n\n * PEFT (由 Hugging Face 开发的参数高效微调库)\n * unsloth (高效微调框架，显存需求可低至5GB, DeepSeek 几个量化模型的流行版本全是 unsloth 团队的)\n * LLaMA-Factory (用于LLaMA系列模型的流行微调框架)\n\n\nReference#\n\n * LoRA: Low-Rank Adaptation of Large Language Models\n * Low-Rank Adaptation: A Closer Look at LoRA\n * This conceptual guide gives a brief overview of LoRA\n * Efficient Fine-Tuning of LMs with Low-Rank Adaptation","routePath":"/guide/ai/what-is-LoRA","lang":"zh","toc":[{"text":"LoRA 的工作原理","id":"lora-的工作原理","depth":2,"charIndex":181},{"text":"LoRA 的主要特点和优势","id":"lora-的主要特点和优势","depth":2,"charIndex":984},{"text":"LoRA 的应用","id":"lora-的应用","depth":2,"charIndex":1345},{"text":"支持 LoRA 的框架","id":"支持-lora-的框架","depth":2,"charIndex":1707},{"text":"Reference","id":"reference","depth":2,"charIndex":1864}],"domain":"","frontmatter":{"title":"什么是 LoRA","description":"什么是 LoRA","date":20250228,"plainLanguage":"**LoRA 说白了就是：** 给大模型\"打补丁\"，而不是\"重装系统\"。\n\n就像你买了个 iPhone，想让它更适合你的使用习惯。传统方法是刷机（全参数微调），风险大、耗时长。LoRA 就像装个 App，只改你需要的那部分功能，其他不动。\n\n**用菜市场的话说：**\n想象你有个万能调料盒，里面有 1000 种调料。你想做川菜，传统方法是把所有调料都重新配一遍。LoRA 就是只加几样新调料（比如花椒、辣椒），其他调料不动，这样既省事又不会把原来的味道搞乱。\n\n**核心原理：**\n1. **不动原模型**：预训练好的大模型\"冻结\"，就像把原调料盒锁起来\n2. **加小补丁**：只训练几个很小的\"适配层\"（可能只有原模型的 1% 大小）\n3. **效果一样好**：虽然只改了 1%，但效果跟全改差不多\n\n**好处：**\n- **省显存**：以前需要 80GB 显存，现在可能只要 20GB\n- **训练快**：参数少，训练时间短\n- **存储小**：补丁文件可能只有几百 MB，而不是几十 GB\n- **可组合**：可以同时用多个补丁，就像同时装多个 App\n\n**注意：**\nLoRA 适合\"教模型新技能\"（比如让它说话更礼貌），不适合\"教模型新知识\"（比如让它知道 2024 年的新闻）。新知识应该用 RAG。\n\n说白了，LoRA 就是\"微调大模型的正确姿势\"——用最小的代价，获得最大的效果。\n"},"version":""},{"id":14,"title":"什么是 MoE 模型","content":"MoE (Mixture of Experts, 混合专家模型) 是一种通过组合多个专业子模型 (专家) 来提升模型性能的神经网络架构.\n\n它通过动态路由机制选择性地激活部分专家, 在保持模型容量的同时显著降低计算成本, 已成为大规模语言模型的重要技术方案.\n\n典型的 MoE 模型如 DeepSeek-R1, DeepSeek-V3, Mistral 8x7b.\n\n一些闭源的商业模型可能也是 MoE 架构的 (比如 GPT-4, 相关信息可以参考 gpt-4-architecture-infrastructure).\n\n\nMoE 模型的架构#\n\n * 门控网络或路由: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家.\n * 稀疏 MoE 层: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层. MoE 层包含若干\"专家\",\n   每个专家本身是一个独立的神经网络.\n\n\nMoE 模型的主要特点和优势#\n\n * 专家系统架构： 模型由多个独立专家网络 (Expert) 和门控网络 (Gating Network) 组成, 每个专家专注于处理特定类型的输入模式.\n * 稀疏激活机制： 通过动态路由算法 (如Top-K Gating) , 每个输入样本仅激活少量专家\n   (比如DeepSeek-R1每个token仅激活8个专家) , 来达到减少计算量的目的.\n * 高效扩展性： 模型容量可随专家数量线性增长, 而计算成本仅与激活的专家数量相关, 适合构建超大规模模型 (100B+) .\n * 多任务优化： 不同专家可自发学习不同特征表示, 天然适配多任务学习场景, 在机器翻译、多模态任务中表现突出.\n * 训练稳定性： 通过负载均衡策略 (如专家容量限制、辅助损失函数) 解决专家利用率不均衡问题, 确保训练过程稳定.\n\n\nMoE 模型面临的挑战#\n\n * 需要大量显存： 需要为每个专家分配显存, 导致需要大量显存. 当然也有一些优化方法, 比如 KTransformers\n * 在微调方面存在诸多挑战：\n   * 微调新任务时可能破坏预训练阶段形成的专家专业化分工\n   * 微调数据分布变化可能导致部分专家过拟合, 出现\"专家坍缩\"现象 (1-2个专家处理90%+的输入)\n * 动态路由优化： 微调阶段输入分布变化可能导致预训练阶段学习到的路由策略失效\n\n\nReference#\n\n * 混合专家模型 (MoE) 详解\n * Adaptive Mixture of Local Experts","routePath":"/guide/ai/what-is-MoE","lang":"zh","toc":[{"text":"MoE 模型的架构","id":"moe-模型的架构","depth":2,"charIndex":263},{"text":"MoE 模型的主要特点和优势","id":"moe-模型的主要特点和优势","depth":2,"charIndex":408},{"text":"MoE 模型面临的挑战","id":"moe-模型面临的挑战","depth":2,"charIndex":789},{"text":"Reference","id":"reference","depth":2,"charIndex":1011}],"domain":"","frontmatter":{"title":"什么是 MoE 模型","description":"什么是 MoE 模型","date":20250217,"plainLanguage":"**MoE 模型说白了就是：** 一个\"专家团队\"，每次只叫最合适的几个人干活。\n\n想象一个公司，有 100 个专家，但每次接项目只让最相关的 8 个人来干。这样既能保证专业，又不用让所有人都加班。\n\n**用菜市场的话说：**\n就像你去菜市场，有卖菜的、卖肉的、卖鱼的、卖调料的... 你买鱼的时候，只需要找卖鱼的，不用把所有摊位都逛一遍。MoE 就是让 AI 也这样\"按需找人\"。\n\n**核心机制：**\n1. **路由系统**：就像前台，看你问什么问题，决定找哪个专家\n2. **专家团队**：每个专家擅长不同领域（比如一个擅长数学，一个擅长文学）\n3. **只激活部分**：每次只让几个专家工作，其他休息，省电省时间\n\n**好处：**\n- 模型可以做得很大（1000 亿参数），但实际运行时只用一小部分\n- 就像你有一个超大的工具箱，但每次只拿需要的几样，不会把所有工具都搬出来\n\n**缺点：**\n需要更多显存（因为所有专家都要\"待命\"），而且微调时容易\"专家罢工\"（某些专家不干活了）。\n\n说白了，MoE 就是\"人多力量大，但只让该干活的人干活\"的 AI 版本。\n"},"version":""},{"id":15,"title":"什么是 RAG 技术?","content":"(图片来自 qdrant.tech/articles/what-is-rag-in-ai)\n\nRAG (Retrieval-Augmented Generation, 检索增强生成) 是一种结合检索与生成的混合式语言模型技术。\n\n其核心思想是通过外部知识库增强语言模型的生成能力，使模型在回答时能够动态检索相关信息，从而提高生成内容的准确性和事实性。\n\n\nRAG 工作流程#\n\n典型 RAG 系统包含三个关键阶段：\n\n * 检索阶段：根据输入问题从外部知识库（如维基百科、专业文档等）检索相关文档片段\n\n * 增强阶段：将检索到的相关文本与原始问题拼接，形成增强后的输入(即 prompt)\n\n * 生成阶段：语言模型基于增强后的上下文生成最终回答 这种机制使模型能够突破训练数据的限制，实时获取最新知识。\n\n\nRAG 的优点#\n\n * 提升事实准确性：通过检索真实数据减少模型\"幻觉\"\n\n * 动态知识更新：无需重新训练即可更新知识库\n\n * 领域适应性强：通过替换知识库快速适配不同专业领域\n\n * 可解释性增强：可追溯答案的参考来源\n\n\nRAG 可能存在的问题#\n\n * 检索质量依赖：检索结果的质量直接影响最终生成效果\n\n * 延迟增加：检索步骤会引入额外的计算和IO开销\n\n * 知识更新成本：需要维护高质量且及时更新的知识库\n\n * 上下文长度限制：检索内容可能超出模型上下文窗口\n\n\nRefs#\n\nWhat is RAG (Retrieval-Augmented Generation)?","routePath":"/guide/ai/what-is-RAG","lang":"zh","toc":[{"text":"RAG 工作流程","id":"rag-工作流程","depth":2,"charIndex":177},{"text":"RAG 的优点","id":"rag-的优点","depth":2,"charIndex":355},{"text":"RAG 可能存在的问题","id":"rag-可能存在的问题","depth":2,"charIndex":471},{"text":"Refs","id":"refs","depth":2,"charIndex":598}],"domain":"","frontmatter":{"title":"什么是 RAG 技术?","description":"什么是 RAG 技术?","date":20250209,"plainLanguage":"**RAG 说白了就是：** 让 AI \"查资料\"再回答，而不是只靠\"记忆\"。\n\n就像你考试时，可以带参考书。遇到不会的题，先翻书找答案，然后结合书上的内容和题目，写出答案。RAG 就是让 AI 也这样干。\n\n**用大白话说：**\n想象你跟朋友聊历史，他说\"秦始皇哪年统一六国的？\"你记不清了，就掏出手机查一下，然后说\"公元前 221 年\"。RAG 就是给 AI 配了个\"手机\"，让它遇到不知道的问题时，先去\"查资料\"，再回答。\n\n**工作流程：**\n1. **检索**：AI 收到问题后，先去知识库（比如维基百科、公司文档）里找相关内容\n2. **增强**：把找到的资料和问题拼在一起，形成\"增强版问题\"\n3. **生成**：基于这个\"增强版问题\"生成答案\n\n**好处：**\n- **减少胡说八道**：有资料支撑，不容易\"编造\"答案\n- **知识更新快**：更新知识库就行，不用重新训练模型\n- **可追溯**：能告诉你答案来自哪里，就像论文的参考文献\n\n**缺点：**\n- 慢一点：因为要先\"查资料\"\n- 依赖资料质量：如果资料是错的，答案也可能错\n\n说白了，RAG 就是\"让 AI 更靠谱\"的方法——不是让它变得更聪明，而是让它\"知道自己的局限\"，不知道就查，而不是瞎编。\n"},"version":""},{"id":16,"title":"什么是 Flash Attention?","content":"Flash Attention 是一种优化的注意力机制, 旨在提高深度学习模型中注意力计算的效率. 它通优化访存机制来加速训练和推理过程.\n\n目前的GPU架构中, HBM 容量大但处理速度慢, SRAM 虽然容量小但操作速度快.\n\n标准的注意力机制使用 HBM 来存储、读取和写入注意力分数矩阵（attention score matrix, 矩阵存储 Q/K/V). 具体步骤为将这些从\nHBM 加载到 GPU 的片上 SRAM, 然后执行注意力机制的单个步骤, 然后写回 HBM, 并重复此过程.\n\n而 Flash Attention 则是采用分块计算（Tiling）技术，将大型注意力矩阵划分为多个块（tile），在 SRAM 中逐块执行计算。通过：\n\n * 分块策略：将 Q/K/V 矩阵分块后流水线处理，避免存储完整的中间矩阵\n * 重计算（Recomputation）：在反向传播时动态重新计算前向结果，而非存储中间值\n * IO优化：通过精确的内存访问控制，使数据在 HBM 和 SRAM 间的移动最小化\n\n\n优点#\n\n * 计算效率高：通过分块并行计算和半精度（FP16/BF16）优化，充分利用 GPU Tensor Cores\n * 内存使用减少：重计算技术减少 4-20 倍内存占用，支持更长序列训练\n * 训练加速：反向传播通过延迟重计算优化，实现端到端 2-4 倍加速\n * 精度保持：采用平铺分块策略时仍保持数值稳定性，支持混合精度训练\n\n\n缺点#\n\n * 实现复杂：由于需要对底层计算进行优化, Flash Attention 的实现可能比传统注意力机制更复杂.\n * 硬件依赖：在某些情况下, 可能需要特定的硬件支持才能充分发挥其性能优势.\n * 调试困难：优化后的计算过程可能导致调试和故障排查变得更加困难.\n\n总的来说, Flash Attention 是一种强大的工具, 能够在不牺牲性能的情况下提高模型的效率, 但在实现和使用时需要考虑其复杂性和硬件要求.\n\n\n性能#\n\n\n\n当使用 H100 显卡且序列长度是512时（数据来自论文测试），PyTorch 的标准处理速度是 62 Tflops，而 Flash Attention\n则可以达到 157 Tflops，Flash Attention 2 则可以达到215 Tflops。在 FP16/BF16 精度下，实际加速比可达标准实现的\n3-4 倍。\n\n\nRefs#\n\n * FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n * flash-attention on github","routePath":"/guide/ai/what-is-flash-attention","lang":"zh","toc":[{"text":"优点","id":"优点","depth":3,"charIndex":461},{"text":"缺点","id":"缺点","depth":3,"charIndex":634},{"text":"性能","id":"性能","depth":2,"charIndex":850},{"text":"Refs","id":"refs","depth":2,"charIndex":1023}],"domain":"","frontmatter":{"title":"什么是 Flash Attention?","description":"什么是 Flash Attention?","date":20250201,"plainLanguage":"**Flash Attention 说白了就是：** 让 AI \"思考\"更快更省内存的优化技巧。\n\n想象你的大脑有两个\"记忆区\"：一个是慢速但容量大的\"仓库\"（HBM），一个是快速但容量小的\"工作台\"（SRAM）。传统方法就像把所有东西都搬到工作台上，结果工作台放不下，只能来回搬，累死。\n\n**用菜市场的话说：**\n就像你去买菜，传统方法是把所有菜都搬到收银台，结果台子放不下，只能搬一部分、算一部分、再搬一部分... Flash Attention 就是\"分批拿菜\"，每次只拿需要的几样，算完再拿下一批，不用把所有菜都堆在收银台。\n\n**核心技巧：**\n1. **分块处理**：把大任务拆成小块，一块一块处理\n2. **不存中间结果**：算完就扔，需要时再算一遍（虽然多算一次，但省内存）\n3. **优化搬运**：减少在\"仓库\"和\"工作台\"之间搬东西的次数\n\n**效果：**\n- 速度快 3-4 倍：就像从\"走路\"变成\"坐高铁\"\n- 内存省 4-20 倍：以前需要 100GB 显存，现在可能只要 20GB\n- 能处理更长的文本：以前最多 2048 个词，现在能到 8192 甚至更长\n\n说白了，Flash Attention 就是\"用聪明的方法偷懒\"——不是真的偷懒，而是让 GPU 更高效地工作，就像你整理房间时，不是把所有东西都摊开，而是分类分批处理。\n"},"version":""},{"id":17,"title":"什么是 GGUF","content":"GGUF（GGML Universal\nFile）是一种专为大型语言模型（LLM）设计的文件格式。它旨在解决大型模型在实际应用中遇到的存储效率、加载速度、兼容性和扩展性等问题，从而简化 LLM\n的使用和部署。\n\n\nGGUF 的主要特点和优势#\n\n * 高效存储： GGUF\n   格式优化了数据的存储方式，减少了存储空间的占用，这对于包含大量参数的大型模型尤为重要。它采用紧凑的二进制编码格式和优化的数据结构来高效地保存模型参数（权重\n   和偏差）。\n * 单文件部署： 它们可以轻松分发和加载，加载模型所需的所有信息都包含在模型文件中，不需要任何外部文件来获取附加信息。\n * 快速加载： GGUF 格式支持快速加载模型数据（使用 mmap），这对于需要即时响应的应用场景非常有用，例如在线聊天机器人或实时翻译系统。\n * 跨平台兼容性： GGUF 兼容多种编程语言，例如 Python 和\n   R，非常方便在不同平台和环境中使用。大部分语言都可以使用少量代码轻松加载和保存模型，无需外部库。\n * 支持微调： GGUF 支持微调，允许用户根据特定的应用场景调整 LLM，并存储跨应用部署模型的提示模板。\n * 取代 GGML： GGUF 是 GGML 的替代者。GGML 由于在灵活性和扩展性方面存在一些限制，已被弃用，由 GGUF 取代。\n\n\nGGUF 的应用#\n\nGGUF 格式的模型文件可以用于各种应用场景，例如：\n\n * 本地部署 LLM： GGUF 格式使得在消费级计算机硬件（包括 CPU 和 GPU）上运行 LLM 成为可能。\n * 移动设备上的 LLM 推理： 由于其高效的存储和加载特性，GGUF 也适用于在移动设备上进行 LLM 推理。\n * 快速原型开发： GGUF 使得开发者可以更快速地加载和测试不同的 LLM 模型。\n\n总而言之，GGUF 是一种重要的 LLM 文件格式，它通过提高存储效率、加载速度和兼容性，简化了 LLM 的使用和部署，并有望成为未来大模型文件标准格式之一。\n\n\n那些框架支持 GGUF#\n\n * ggml\n * llama.cpp\n * InfniLM\n * crabml\n\n\nReference#\n\n * https://github.com/ggerganov/ggml/blob/master/docs/gguf.md","routePath":"/guide/ai/what-is-gguf","lang":"zh","toc":[{"text":"GGUF 的主要特点和优势","id":"gguf-的主要特点和优势","depth":2,"charIndex":106},{"text":"GGUF 的应用","id":"gguf-的应用","depth":2,"charIndex":576},{"text":"那些框架支持 GGUF","id":"那些框架支持-gguf","depth":2,"charIndex":859},{"text":"Reference","id":"reference","depth":2,"charIndex":917}],"domain":"","frontmatter":{"title":"什么是 GGUF","description":"什么是 GGUF","date":20250113,"plainLanguage":"**GGUF 说白了就是：** 大模型的\"压缩包\"格式，但比 ZIP 高级多了。\n\n想象一下，你要把一个几百 GB 的游戏传给朋友。如果用普通压缩，可能压到 50GB，但解压还得等半天。GGUF 就像是一个\"智能压缩包\"——不仅体积小，还能直接\"边解压边玩\"，不用等全部解压完。\n\n**用菜市场大妈的话说：**\n以前存大模型，就像把一仓库的货分成几千个小箱子，每次用都要把所有箱子搬出来。GGUF 就是把所有货打包成一个超级大箱子，但神奇的是，你只需要打开一个小口，就能拿到想要的任何东西，还不用把整个箱子都打开。\n\n**核心就三点：**\n1. **省地方**：同样的模型，GGUF 格式能省不少硬盘空间\n2. **开得快**：就像手机秒开 App，不用等加载\n3. **一个文件搞定**：不用配环境、不用装依赖，一个文件就能跑\n\n说白了，GGUF 就是让\"普通人也能在自家电脑上跑大模型\"这件事变得可能。以前你得是技术大牛才能搞定，现在就像双击打开 Word 文档一样简单。\n"},"version":""},{"id":18,"title":"","content":"Grouped-Query Attention（分组查询注意力）是 Transformer\n架构的改进型注意力机制，在多头注意力（MHA）和多查询注意力（MQA）之间取得平衡。通过分组共享键值投影，在保持模型容量的同时显著降低计算资源消耗。\n\n\n工作原理#\n\n给定输入向量 $Q$（查询）、$K$（键）和 $V$（值），GQA 将查询头分组处理：\n\n$$ \\text{GroupedQuery}(Q, K, V) = \\text{Concat}(\\text{group}_1, \\ldots,\n\\text{group}_g)W^O $$\n\n每个组内共享键值投影：\n\n$$ \\text{group}i = \\text{Attention}(QW_i^Q, KW{\\lfloor i/m \\rfloor}^K,\nVW_{\\lfloor i/m \\rfloor}^V) $$\n\n其中：\n\n * $g$ 为分组数（通常 $g \\ll h$）\n * $m = h/g$ 每组包含的头数\n * $W_i^Q \\in \\mathbb{R}^{d_{model}\\times d_k}$ 为每组独立的查询投影\n * $W_j^K, W_j^V \\in \\mathbb{R}^{d_{model}\\times d_k}$ 为组间共享的键值投影\n\n\n核心机制#\n\n * 动态分组策略：\n   \n   * 将 $h$ 个查询头划分为 $g$ 个组\n   * 每组包含 $m=h/g$ 个查询头共享同一组键值投影\n   * 通过线性投影实现特征空间的分组耦合\n\n * 参数效率： 总参数量为： $$ \\underbrace{hd_kd_{model}}{\\text{查询投影}} +\n   \\underbrace{2gd_kd{model}}{\\text{键值投影}} = (h + 2g)d_kd{model} $$ 相比 MHA 减少\n   $3hd_kd_{model} - (h+2g)d_kd_{model}$ 参数\n\n\n优点#\n\n * 显存优化：键值缓存显存占用降至 MHA 的 $g/h$，例如 8 头分组为 2 组时显存减少 75%\n * 质量保留：PaLM 2 实验显示，GQA（g=8）与 MHA 相比在质量指标上差异小于 0.5%\n * 灵活扩展：通过调整分组数 $g$ 实现质量与效率的连续调节：\n   * $g=h$ 时退化为标准 MHA\n   * $g=1$ 时等价于 MQA\n\n\n缺点#\n\n * 分组调优成本：需要实验确定最佳分组数，不同任务/架构可能有不同最优配置\n * 投影偏差风险：共享键值投影可能限制不同组的特征多样性\n * 实现复杂度：需要管理分组投影的矩阵运算，可能引入额外的张量变换开销\n\n\n性能对比#\n\nGQA 论文中使用的T5 Large 和 XXL 模型在多头注意力、5% 训练的 T5-XXL 模型在多查询和分组查询注意力下，在摘要数据集 CNN/Daily\nMail、arXiv、PubMed、MediaSum 和 MultiNews，翻译数据集 WMT，以及问答数据集 TriviaQA\n上的推理时间和平均开发集性能比较。\n\n模型          推理时间 (S)   平均     CNN    ARXIV   PUBMED   MEDIASUM   MULTINEWS   WMT    TRIVIAQA\nMHA-Large   0.37       46.0   42.9   44.6    46.2     35.5       46.6        27.7   78.2\nMHA-XXL     1.51       47.2   43.8   45.6    47.5     36.4       46.9        28.4   81.9\nMQA-XXL     0.24       46.6   43.0   45.0    46.9     36.1       46.5        28.5   81.3\nGQA-8-XXL   0.28       47.1   43.5   45.4    47.7     36.3       47.2        28.4   81.6\n\n\nRefs#\n\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head\nCheckpoints","routePath":"/guide/ai/what-is-gropued-query-attention","lang":"zh","toc":[{"text":"工作原理","id":"工作原理","depth":3,"charIndex":122},{"text":"核心机制","id":"核心机制","depth":3,"charIndex":564},{"text":"优点","id":"优点","depth":3,"charIndex":853},{"text":"缺点","id":"缺点","depth":3,"charIndex":1042},{"text":"性能对比","id":"性能对比","depth":3,"charIndex":1155},{"text":"Refs","id":"refs","depth":2,"charIndex":1779}],"domain":"","frontmatter":{"title":null,"description":null,"date":20250205,"plainLanguage":"**Grouped-Query Attention 说白了就是：** MHA 和 MQA 的\"折中方案\"——既要效果，也要省资源。\n\n就像公司里，既不想每个部门都配HR（太贵），也不想所有部门共享一个HR（忙不过来）。那就分组——每 4 个部门共享一个 HR，既省钱又不会太忙。\n\n**用大白话说：**\n想象 8 个人去撸串，不想每个人都点一份菜单（浪费），也不想大家共用一份菜单（抢着看）。那就 2 个人共用一份菜单，这样既节省又不拥挤。\n\n**三种方案对比：**\n- **MHA（多头）**：8 个人，8 份菜单，最灵活但最浪费\n- **MQA（多查询）**：8 个人，1 份菜单，最省但可能不够用\n- **GQA（分组查询）**：8 个人，2 份菜单（分 4 组），平衡方案\n\n**好处：**\n- **灵活调节**：可以根据需求调整分组数（2 组、4 组、8 组）\n- **效果好**：比 MQA 效果好，比 MHA 省资源\n- **实用性强**：适合生产环境部署\n\n**缺点：**\n- **调参麻烦**：需要实验找最佳分组数\n- **实现复杂**：代码比 MHA/MQA 稍微复杂一点\n\n说白了，GQA 就是\"中庸之道\"的 AI 版本——不走极端，在效果和效率之间找个平衡点，是目前最实用的方案。\n"},"version":""},{"id":19,"title":"什么是 LLM 的困惑度 (Perplexity)","content":"困惑度（Perplexity）是评估语言模型（LLM）性能的关键指标。它衡量模型在预测文本序列时的不确定性程度，用于量化模型对未见过的数据的预测能力。\n\n\n困惑度的定义与计算#\n\n * 数学定义： 困惑度是模型对测试集的对数概率的负平均值。数学上表示为： $$\\text{Perplexity} =\n   \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log p(x_i)\\right)$$ 其中 N\n   是测试集中的标记数量，p(x_i) 是模型对标记 x_i 的预测概率。\n\n * 直观理解： 困惑度可以理解为模型在每个位置上平均需要考虑的可能选项数量。困惑度越低，说明模型的预测准确性越高。\n\n * 熵的关系： 困惑度是序列交叉熵的指数，直接反映了模型预测的不确定性。\n\n\n困惑度的重要意义#\n\n * 模型评估： 困惑度提供了一种客观的方式来比较不同语言模型的性能，是训练过程中的重要监控指标。通常微调或者量化过程中也会使用。\n\n * 过拟合检测： 通过比较训练集和验证集上的困惑度，可以发现模型是否过拟合。\n\n * 预测质量： 较低的困惑度通常意味着模型能够生成更流畅、更连贯的文本。\n\n * 领域适应性： 困惑度可以反映模型在特定领域文本上的表现，帮助评估模型的领域迁移能力。\n\n\n困惑度的使用场景#\n\n困惑度广泛应用于各种 LLM 相关的场景：\n\n * 模型训练： 作为训练过程中的优化目标和停止条件。\n\n * 模型选择： 帮助研究人员在不同架构和参数设置之间做出选择。\n\n * 数据质量评估： 高质量的训练数据通常会导致模型在相似数据上具有较低的困惑度。\n\n * 生成文本评估： 虽然不是唯一指标，但困惑度可以作为生成文本质量的参考。\n\n\n困惑度的局限性#\n\n尽管困惑度广泛使用，但它也存在一些局限性：\n\n * 不完全反映语义理解： 低困惑度不一定意味着模型真正理解了文本的语义。\n\n * 对于开放式生成任务不够全面： 在创意写作等任务中，困惑度可能无法完全捕捉输出的质量。\n\n * 与人类评价不完全一致： 困惑度是一个统计指标，可能与人类对文本质量的主观评价存在差异。\n\n * 模型大小的影响： 大型模型往往能够达到较低的困惑度，但这并不总是转化为更好的实际应用效果。\n\n总之，困惑度是评估 LLM 性能的重要指标，提供了一种客观测量模型预测能力的方法。然而，它应该与其他评估方法结合使用，以全面了解模型的实际性能。\n\n\n支持困惑度评估的工具#\n\n * HuggingFace Evaluate\n\n\nReference#\n\n * https://en.wikipedia.org/wiki/Perplexity\n * https://huggingface.co/docs/transformers/perplexity","routePath":"/guide/ai/what-is-llm-perplexity","lang":"zh","toc":[{"text":"困惑度的定义与计算","id":"困惑度的定义与计算","depth":2,"charIndex":77},{"text":"困惑度的重要意义","id":"困惑度的重要意义","depth":2,"charIndex":354},{"text":"困惑度的使用场景","id":"困惑度的使用场景","depth":2,"charIndex":560},{"text":"困惑度的局限性","id":"困惑度的局限性","depth":2,"charIndex":741},{"text":"支持困惑度评估的工具","id":"支持困惑度评估的工具","depth":2,"charIndex":1033},{"text":"Reference","id":"reference","depth":2,"charIndex":1072}],"domain":"","frontmatter":{"title":"什么是 LLM 的困惑度 (Perplexity)","description":"什么是 LLM 的困惑度 (Perplexity)","date":20250403,"plainLanguage":"**困惑度 (Perplexity) 说白了就是：** 给 AI 模型打分的\"考试成绩\"——分数越低越好。\n\n就像考试，老师问\"明天是星期几？\"你说\"星期三\"，老师说\"对，满分\"。如果你说\"不知道\"，得 0 分。困惑度就是衡量 AI \"猜得准不准\"的分数。\n\n**用大白话说：**\n想象盲品测试，让你蒙着眼睛尝 10 道菜，每次猜是什么。如果你能准确猜出 9 道，说明你很厉害（困惑度低）。如果只猜对 2 道，说明你不行（困惑度高）。困惑度就是衡量 AI \"蒙题\"能力的指标。\n\n**直观理解：**\n- **困惑度 = 1**：完美预测，每次都猜对\n- **困惑度 = 10**：就像在 10 个选项里瞎蒙，平均要猜 10 次才对一次\n- **困惑度 = 100**：非常迷惑，基本靠运气\n\n**核心公式（不用背）：**\n困惑度 = 模型预测的\"惊讶程度\"。AI 预测越准确，越不惊讶，困惑度越低。\n\n**应用场景：**\n- **训练监控**：看困惑度降没降，判断模型有没有在学习\n- **模型对比**：两个模型哪个好？看困惑度，低的赢\n- **量化评估**：量化后效果咋样？看困惑度涨了多少\n\n**局限性：**\n- 困惑度低不代表模型真懂，可能只是死记硬背\n- 对创意写作等任务，困惑度不够全面\n- 跟人类感受可能不一致（AI 觉得好，人可能觉得不好）\n\n说白了，困惑度就是\"AI 的考试成绩单\"——虽然不能说明一切，但至少能告诉你模型学得怎么样。\n"},"version":""},{"id":20,"title":"什么是模态编码？","content":"(图片来自 researchgate.net)\n\n模态编码（Modal Encoding）是处理多模态数据时，将原始数据转换为特定模态特征 (可以理解为一种统一格式)\n表示的过程。其核心目标是保留数据模态特性，同时提取机器可理解的语义特征。\n\n\n核心原理#\n\n模态编码典型处理流程包含：\n\n * 信号预处理：将原始数据转换为标准格式\n   \n   * 图像：归一化/尺寸调整\n   * 文本：分词/词干提取\n   * 音频：分帧/频谱转换\n\n * 特征提取：使用模态专用编码器\n   \n   * 视觉：CNN/ViT提取空间特征\n   * 文本：BERT/GPT提取语义特征\n   * 音频：HuBERT/Mel-Frequency特征提取\n   * 3D点云: ULIP-2提取特征\n\n * 表示优化：通过池化/注意力机制获得紧凑表示\n\n\n技术优势（系统实现视角）#\n\n * 特征解耦：允许不同模态独立优化编码器\n\n * 并行处理：各模态编码可分布式执行\n\n * 硬件适配：为特定模态选择最优计算单元（如GPU加速图像编码）\n\n * 缓存复用：编码结果可离线预计算存储\n\n * 渐进增强：支持编码器单独升级替换\n\n\n实现挑战（工程化角度）#\n\n * 计算异构：不同模态编码器的资源需求差异\n * 时序同步：流式场景下的多模态对齐难题\n * 模态偏差：编码器过拟合特定数据分布\n * 特征膨胀：高维特征带来的存储压力\n * 版本控制：编码器更新导致的特征空间漂移\n\n\n与向量嵌入的区别#\n\n维度     模态编码             向量嵌入\n处理阶段   多模态处理前端          跨模态对齐后端\n输入数据   原始信号（像素/声波/字符）   模态编码输出的特征表示\n输出特性   保留模态特性的特征图/序列    扁平化的跨模态可比向量\n优化目标   最大化模态内信息保留       最小化跨模态语义距离\n可解释性   高（对应具体感知特征）      低（抽象语义表示）\n典型操作   卷积/池化/词干提取       线性投影/注意力融合\n\n协同工作示例（图像-文本场景）：\n\n 1. 图像通过CNN编码 → 输出14×14×512特征图（空间感知特征）\n 2. 文本通过BERT编码 → 输出768维词向量序列（语法语义特征）\n 3. 两者分别通过向量嵌入层 → 统一为1024维语义向量\n 4. 在共享空间计算余弦相似度实现图文匹配 (这里使用余弦相似度的优点包括不受向量绝对大小影响只关注方向, 以及输出范围固定在[-1,\n    1]易于最后概率化处理)\n\n这种分层设计既保留了模态特异性（编码阶段），又实现了跨模态交互（嵌入阶段），是当前多模态系统的典型架构范式。\n\n\nRefs#\n\n * modality-encoder-in-multimodal-large-language-models","routePath":"/guide/ai/what-is-modal-encoding","lang":"zh","toc":[{"text":"核心原理","id":"核心原理","depth":2,"charIndex":122},{"text":"技术优势（系统实现视角）","id":"技术优势系统实现视角","depth":2,"charIndex":370},{"text":"实现挑战（工程化角度）","id":"实现挑战工程化角度","depth":2,"charIndex":508},{"text":"与向量嵌入的区别","id":"与向量嵌入的区别","depth":2,"charIndex":633},{"text":"Refs","id":"refs","depth":2,"charIndex":1151}],"domain":"","frontmatter":{"title":"什么是模态编码？","description":"什么是模态编码？","date":20250315,"plainLanguage":"**模态编码说白了就是：** 把不同类型的数据（图片、文字、声音）翻译成 AI 能理解的\"通用语言\"。\n\n就像联合国开会，各国代表说不同语言，需要翻译成英语才能交流。模态编码就是把图片的\"视觉语言\"、文字的\"文本语言\"、声音的\"听觉语言\"都翻译成 AI 的\"数字语言\"。\n\n**用大白话说：**\n想象你去国际美食节，有中餐、日料、意大利菜。每种菜的\"语言\"不一样（食材、做法、口味）。模态编码就像把所有菜都用\"卡路里、营养成分、口感\"这套统一标准来描述，这样 AI 就能比较和理解它们了。\n\n**处理流程：**\n1. **预处理**：把原始数据变成标准格式（图片调整大小、文字分词、音频分帧）\n2. **特征提取**：用专门的编码器提取特征（CNN 看图、BERT 读文、HuBERT 听音）\n3. **优化表示**：压缩成紧凑的表示，去掉冗余信息\n\n**核心价值：**\n- **保留特性**：图片的编码还保留\"空间信息\"，文字的编码还保留\"语法信息\"\n- **可处理**：把\"看不懂\"的原始数据变成\"能算\"的数字\n- **可组合**：编码后的数据能一起处理、比较、融合\n\n**与向量嵌入的区别：**\n- **模态编码**：第一步，把原始数据变成\"半成品\"（还保留模态特性）\n- **向量嵌入**：第二步，把\"半成品\"变成\"成品\"（统一格式，可以直接比较）\n\n说白了，模态编码就是\"翻译第一步\"——把各种语言翻译成\"中间语言\"，为后续的跨模态处理打基础。\n"},"version":""},{"id":21,"title":"什么是 Multi-Head Attention?","content":"多头注意力（Multi-Head Attention）是 Transformer 架构中的一个核心组件，它通过并行运行多个注意力机制来增强模型的性能。\n\n在多头注意力机制中，\"头\"是指一个独立的注意力机制。每个头有自己的一组权重，用于计算输入的自注意力。通过使用多个头，模型可以从不同的角度和特征空间中提取信息。\n\n\n工作原理#\n\n首先我们来看注意力公式, 给定输入向量 $Q$（查询）、$K$（键）和 $V$（值），注意力机制的计算公式为：\n\n$$ \\text{Attention}(Q, K, V) =\n\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n\n其中，$d_k$ 是$K$ (键) 向量的维度。\n\n多头注意力则将上面的公式拆分, 通过多个独立的注意力头来增强模型的能力。每个头有自己的查询、键和值的线性变换。公式如下：\n\n$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots,\n\\text{head}_h)W^O $$\n\n其中每个 $\\text{head}_i$ 计算为：\n\n$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n\n$W_i^Q, W_i^K, W_i^V, W^O$ 是学习到的参数矩阵。\n\n多头注意力机制将输入分成多个\"头\"，每个头独立地执行自注意力计算，然后将所有头的输出合并起来。每个头可以关注输入序列的不同方面，从而捕获更丰富的特征信息。\n\n\n核心机制#\n\n * 维度拆分：将输入向量维度 $d_{model}$ 通过线性投影拆分为$h$个$d_k$维度（$d_k$ =\n   $d_{model}/h$），每个头关注不同的特征子空间\n\n\n优点#\n\n * 并行计算优化：虽然总计算量（FLOPs）与单头注意力相同，但拆分后的多个小矩阵乘法（尺寸$h×d_k$）更适配GPU并行计算特性 (当然实际 FLOPs\n   消耗会略高于单头，因为增加了投影矩阵计算)\n\n\n缺点#\n\n * 内存/计算开销：每个头需要独立的 Q/K/V 投影矩阵，参数数量随头数线性增长：\n   \n   $$ \\underbrace{3hd_kd_{model}}{\\text{输入投影}} +\n   \\underbrace{hd_kd{model}}{\\text{输出投影}} = 4hd_kd{model} $$\n   \n   其中：\n   \n   * 输入投影：每个头包含 $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{model}\\times d_k}$\n     三个矩阵，共 $3hd_kd_{model}$ 参数\n   * 输出投影：合并矩阵 $W^O \\in \\mathbb{R}^{hd_k\\times d_{model}}$，贡献 $hd_kd_{model}$ 参数\n   * 当采用标准配置 $d_k = d_{model}/h$ 时，总参数量简化为 $4d_{model}^2$（与头数无关）\n\n * 键值缓存瓶颈：自回归解码时每个头需要独立缓存 K/V 矩阵，显存占用为 $2bd_{model}L$（b=batch_size, L=seq_len）\n\n * 信息冗余：实验表明不同头可能学习到相似的注意力模式（尤其在后层），造成计算资源浪费\n\n * 工程复杂度：多头并行计算需要精细的内存布局管理，在长序列场景下容易导致内存带宽瓶颈\n\n\n与 MQA/GQA 的对比#\n\n特性       MULTI-HEAD (MHA)   MULTI-QUERY (MQA)                 GROUPED-QUERY (GQA)\n键值投影共享   无                  所有头共享同一 K/V 投影                    分组内共享 K/V 投影\n参数量      $4hd_kd_{model}$   $hd_kd_{model} + 2d_kd_{model}$   $(h + 2g)d_kd_{model}$\n解码显存占用   高                  极低（1/h）                           中等（g/h）\n模型容量     最高                 最低                                可调节（通过分组数 g）\n典型应用场景   编码器                低内存推理场景                           质量与效率的平衡点\n\n后续我们会逐一介绍多头注意力的优化版本 MQA/GQA 的原理和实现.\n\n\nRefs#\n\n * Attention Is All You Need\n * Fast Transformer Decoding: One Write-Head is All You Need","routePath":"/guide/ai/what-is-multi-head-attention","lang":"zh","toc":[{"text":"工作原理","id":"工作原理","depth":3,"charIndex":158},{"text":"核心机制","id":"核心机制","depth":3,"charIndex":697},{"text":"优点","id":"优点","depth":3,"charIndex":795},{"text":"缺点","id":"缺点","depth":3,"charIndex":907},{"text":"与 MQA/GQA 的对比","id":"与-mqagqa-的对比","depth":3,"charIndex":1512},{"text":"Refs","id":"refs","depth":2,"charIndex":2026}],"domain":"","frontmatter":{"title":"什么是 Multi-Head Attention?","description":"什么是 Multi-Head Attention?","date":20250202,"plainLanguage":"**Multi-Head Attention 说白了就是：** 让 AI \"多角度看问题\"，而不是只从一个角度看。\n\n就像你看一部电影，有人关注剧情，有人关注演技，有人关注摄影。多头注意力就是让 AI 也这样\"多角度分析\"一句话——每个\"头\"关注不同的方面。\n\n**用大白话说：**\n想象你去相亲，一个朋友帮你看对方的外貌，一个帮你看性格，一个帮你看经济条件。最后综合大家的意见，你做决策。多头注意力就是让 AI 也这样\"团队作战\"。\n\n**工作原理：**\n1. **拆分**：把输入拆成多份，每个\"头\"负责一份\n2. **各自分析**：每个头独立地分析它负责的部分\n3. **合并结果**：把所有头的分析结果拼起来，得出最终结论\n\n**好处：**\n- **更全面**：多角度看问题，不容易漏掉重要信息\n- **并行计算**：多个头同时工作，速度快\n- **能力强**：能捕捉更复杂的语言模式\n\n**缺点：**\n- **耗资源**：每个头都要独立计算，显存占用大\n- **可能冗余**：有些头可能学到相似的东西，浪费资源\n\n说白了，多头注意力就是\"三个臭皮匠，顶个诸葛亮\"的 AI 版本——虽然每个头单独看可能不够聪明，但合起来就很强大。\n"},"version":""},{"id":22,"title":"什么是多模态模型？","content":"(图片来自 medium.com/@tenyks_blogger)\n\n多模态模型（Multimodal\nLLM）是能够同时处理和关联多种数据模态（如文本、图像、音频、视频等）的大语言模型。这类模型通过统一表示空间，实现跨模态的语义理解和内容生成。\n\n\n核心工作原理#\n\n多模态模型的工作机制包含三个关键阶段：\n\n * 模态编码：使用专用编码器（CNN/ViT处理图像，BERT处理文本等）提取各模态特征\n\n * 特征对齐：通过交叉注意力机制（cross-attention）建立细粒度跨模态关联（如图像区域与文本描述的对应关系）\n\n * 联合推理：在共享表示空间中进行跨模态信息融合与语义推理\n\n\n技术优势（系统实现视角）#\n\n * 统一接口：支持自然语言作为跨模态交互的统一接口\n\n * 知识迁移：视觉-语言等跨模态知识的相互增强\n\n * 上下文扩展：能同时利用多模态上下文信息（如文本描述+示意图）\n\n * 数据效率：通过多任务学习提升小样本场景表现\n\n * 灵活部署：架构灵活性：支持级联式（冻结编码器+可训练适配器）或端到端联合训练架构 (不同模态流程整合到单一神经网络中的架构)\n\n\n实现挑战（工程化角度）#\n\n * 计算复杂度：多模态并行处理带来的显存/算力压力\n\n * 对齐噪声：跨模态数据标注的噪声会影响注意力机制\n\n * 模态鸿沟：不同模态特征分布的差异导致融合困难\n\n * 延迟累积：级联架构中各组件（如图像编码器+LLM）的推理延迟叠加问题\n\n * 评估困境：现有基准（如MMLU、MMBENCH）难以全面评估跨模态推理能力\n\n\nRefs#\n\n * A Survey on Multimodal Large Language Models\n * LLaVA: Large Language and Vision Assistant","routePath":"/guide/ai/what-is-multi-model-llm","lang":"zh","toc":[{"text":"核心工作原理","id":"核心工作原理","depth":2,"charIndex":125},{"text":"技术优势（系统实现视角）","id":"技术优势系统实现视角","depth":2,"charIndex":298},{"text":"实现挑战（工程化角度）","id":"实现挑战工程化角度","depth":2,"charIndex":496},{"text":"Refs","id":"refs","depth":2,"charIndex":675}],"domain":"","frontmatter":{"title":"什么是多模态模型？","description":"什么是多模态模型？","date":20250317,"plainLanguage":"**多模态模型说白了就是：** 能同时\"看图\"、\"读文\"、\"听声\"的全能 AI——就像人一样用多种感官理解世界。\n\n就像你跟朋友聊天，他给你看一张照片，说\"这是我昨天去的地方\"，你能理解图片和文字的关系。多模态模型就是让 AI 也能这样\"看图说话\"、\"听音识图\"。\n\n**用大白话说：**\n传统 AI 就像只会看菜单的服务员（只懂文字），多模态模型就像既能看菜单、又能看菜品图片、还能闻味道的全能服务员。你指着图片说\"来一份这个\"，他能理解；你说\"来点辣的\"，他也能推荐。\n\n**工作原理：**\n1. **模态编码**：用不同的\"翻译器\"处理图片、文字、声音\n2. **特征对齐**：把不同模态的信息\"对齐\"（比如图片的\"苹果区域\"对应文字的\"apple\"）\n3. **联合推理**：在统一空间里综合处理所有信息，做决策\n\n**核心能力：**\n- **看图说话**：给一张图，AI 能描述内容\n- **听音识图**：听到声音，能联想到相关图片\n- **跨模态检索**：输入文字，找相关图片；或反过来\n- **多模态理解**：同时理解文字+图片的组合含义\n\n**典型应用：**\n- **GPT-4V**：能看图回答问题\n- **Gemini**：能同时处理文字、图片、视频\n- **LLaVA**：开源的视觉-语言助手\n\n**挑战：**\n- 计算量大（要同时处理多种数据）\n- 不同模态数据分布差异大，难对齐\n- 推理延迟叠加（图片编码 + 文字生成）\n\n说白了，多模态模型就是\"五感俱全的 AI\"——不再只能\"读书\"，还能\"看图\"、\"听音\"，更接近人类的感知方式。\n"},"version":""},{"id":23,"title":"什么是 Multi-Query Attention?","content":"(image from medium.com/towards-data-science)\n\n多查询注意力（Multi-Query Attention）是 Transformer\n解码器的优化版本，通过共享键/值投影来显著降低内存消耗，特别适合自回归生成任务。\n\n\n工作原理#\n\n在标准多头注意力基础上进行关键修改：所有注意力头共享同一组键（K）和值（V）的投影矩阵，仅保留查询（Q）的独立投影。公式如下：\n\n$$ \\text{MultiQuery}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots,\n\\text{head}_h)W^O $$\n\n其中每个 $\\text{head}_i$ 计算为：\n\n$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW^K, VW^V) $$\n\n$W_i^Q \\in \\mathbb{R}^{d_{model}\\times d_k}$ 保持独立，而 $W^K, W^V \\in\n\\mathbb{R}^{d_{model}\\times d_k}$ 被所有头共享。\n\n\n核心机制#\n\n * 键值共享：所有注意力头共享同一组 K/V 投影矩阵，仅保留 Q 的独立投影\n * 内存优化：自回归解码时只需缓存单组 K/V 矩阵，显存占用降低为原始 MHA 的 $1/h$\n\n\n优点#\n\n * 参数效率：投影矩阵参数量从 $4hd_kd_{model}$ 降为 $hd_kd_{model} + 2d_kd_{model}$（减少约 75%）\n * 解码加速：KV 缓存量减少 h 倍，在长序列生成（如 2048 tokens）时显著降低内存带宽压力\n * 硬件友好：共享的 K/V 投影产生更规整的内存访问模式，提升 GPU/TPU 利用率\n\n\n缺点#\n\n * 容量限制：共享 K/V 投影削弱了模型对不同表示子空间的捕捉能力，可能影响生成质量\n * 训练挑战：需要更谨慎的参数初始化来补偿表示能力的损失\n * 工程复杂度：共享投影引入跨头依赖，增加分布式计算的同步开销\n\n\n与 MHA/GQA 的对比#\n\n特性       MULTI-HEAD (MHA)   MULTI-QUERY (MQA)       GROUPED-QUERY (GQA)\n键值投影共享   无                  所有头共享同一 K/V 投影          分组内共享 K/V 投影\n参数量      $4hd_kd_{model}$   $(h + 2)d_kd_{model}$   $(h + 2g)d_kd_{model}$\n解码显存占用   $2bd_{model}L$     $2bd_kL$                $2bgd_kL$\n模型质量     最优                 基线模型 90%-95%            接近 MHA (98%-99%)\n典型应用场景   预训练                低内存推理场景                 生产环境部署\n\n\nRefs#\n\nDemystifying GQA — Grouped Query Attention for Efficient LLM Pre-training","routePath":"/guide/ai/what-is-multi-query-attention","lang":"zh","toc":[{"text":"工作原理","id":"工作原理","depth":3,"charIndex":131},{"text":"核心机制","id":"核心机制","depth":3,"charIndex":491},{"text":"优点","id":"优点","depth":3,"charIndex":591},{"text":"缺点","id":"缺点","depth":3,"charIndex":776},{"text":"与 MHA/GQA 的对比","id":"与-mhagqa-的对比","depth":3,"charIndex":891},{"text":"Refs","id":"refs","depth":2,"charIndex":1311}],"domain":"","frontmatter":{"title":"什么是 Multi-Query Attention?","description":"什么是 Multi-Query Attention?","date":20250204,"plainLanguage":"**Multi-Query Attention 说白了就是：** 多头注意力的\"省钱版\"——共享资源，但效果差不多。\n\n就像公司里，以前每个部门都配一个HR、一个财务。现在所有部门共享一个HR、一个财务，省钱又高效。MQA 就是让 AI 也这样\"共享资源\"。\n\n**用大白话说：**\n想象你和朋友们去撸串，以前每个人都点一份菜单、一份调料。现在大家共用一份菜单、一份调料，只是每个人点的菜不一样。这样省事又省钱，吃得还一样香。\n\n**核心改进：**\n- **Multi-Head Attention**：每个头都有自己的\"查询\"、\"键\"、\"值\"\n- **Multi-Query Attention**：每个头有自己的\"查询\"，但共享同一套\"键\"和\"值\"\n\n**好处：**\n- **省显存**：显存占用降低 75%（假设 8 个头）\n- **速度快**：生成文本时更快，因为要缓存的东西少了\n- **效果还行**：虽然共享了资源，但效果只差了 5%-10%\n\n**缺点：**\n- **能力稍弱**：共享资源意味着每个头的\"个性\"少了\n- **训练难度**：需要更小心地调参数\n\n说白了，MQA 就是\"勤俭持家\"的 AI 版本——用更少的资源，做差不多的事，适合在手机、边缘设备这些资源紧张的地方跑模型。\n"},"version":""},{"id":24,"title":"什么是 ONNX","content":"(图片来自 ultralytics.com)\n\nONNX（Open Neural Network Exchange）是一种开放的神经网络交换格式。\n\n它由微软和Facebook于2017年共同推出，现由Linux基金会的LF\nAI托管，旨在解决不同深度学习框架之间的互操作性问题，实现模型在不同平台和工具链之间的无缝迁移。\n\n\nONNX 的主要特点和优势#\n\n * 跨框架兼容性：\n   支持主流深度学习框架（PyTorch/TensorFlow/MXNet等）的模型转换，打破框架生态壁垒。开发者可以用PyTorch训练模型，导出为ONNX\n   格式后通过ONNX-TensorFlow等适配器在TensorFlow中部署。\n * 高效推理性能： 通过运行时优化（如ONNX Runtime）实现低延迟推理，支持CPU/GPU/FPGA等多种硬件加速，在服务端和移动端均可获得优异性能。\n * 可扩展性设计： 采用protobuf格式存储计算图和权重参数，通过Operator Sets机制支持自定义算子扩展，持续跟进AI技术演进。\n * 标准化模型格式： 定义统一的模型表示规范，包含网络结构、层参数、输入输出格式等完整信息，支持可视化工具（如Netron）直接解析。\n\n\nONNX 的应用场景#\n\n * 跨框架模型转换： 将PyTorch训练的视觉模型转换为ONNX格式后，可进一步转换为TensorFlow Lite格式部署到移动端\n * 生产环境部署： 通过ONNX Runtime实现高性能推理，支持动态batching和硬件加速\n * 边缘计算优化： 与TensorRT/OpenVINO等推理引擎配合，实现模型在IoT设备的量化部署\n * 算法研究验证： 快速在不同框架间迁移实现，方便论文复现和效果对比\n\n\nONNX 生态系统支持#\n\n * 训练框架： PyTorch, TensorFlow（通过tf2onnx工具）, MXNet, PaddlePaddle\n * 推理引擎： ONNX Runtime, TensorRT, OpenVINO, NCNN\n * 云服务平台： Azure ML, AWS SageMaker, NVIDIA Triton\n * 辅助工具： Netron（模型可视化）, ONNX-SIM（模型优化）\n\n\nReference#\n\n * https://onnx.ai/\n * https://github.com/onnx/onnx\n * https://onnxruntime.ai/","routePath":"/guide/ai/what-is-onnx","lang":"zh","toc":[{"text":"ONNX 的主要特点和优势","id":"onnx-的主要特点和优势","depth":2,"charIndex":163},{"text":"ONNX 的应用场景","id":"onnx-的应用场景","depth":2,"charIndex":539},{"text":"ONNX 生态系统支持","id":"onnx-生态系统支持","depth":2,"charIndex":763},{"text":"Reference","id":"reference","depth":2,"charIndex":978}],"domain":"","frontmatter":{"title":"什么是 ONNX","description":"什么是 ONNX","date":20250211,"plainLanguage":"**ONNX 说白了就是：** AI 模型的\"通用转接头\"，让不同框架的模型互相兼容。\n\n就像你的手机充电器，以前苹果用 Lightning、安卓用 Type-C，不能通用。ONNX 就是\"统一标准\"，让所有模型都能在不同框架间无缝切换。\n\n**用大白话说：**\n想象你在 PyTorch 做的菜（训练的模型），本来只能用 PyTorch 的锅（框架）来炒。有了 ONNX，就像把菜装进万能保鲜盒，TensorFlow 的锅、Caffe 的锅都能拿来用。\n\n**核心价值：**\n1. **跨框架**：PyTorch 训练，TensorFlow 部署，随意切换\n2. **高效推理**：用 ONNX Runtime 跑模型，速度更快\n3. **跨硬件**：CPU、GPU、FPGA、手机都能跑\n4. **标准化**：统一的模型格式，方便分享和部署\n\n**典型场景：**\n- 你用 PyTorch 训练了个模型，想部署到手机上（TensorFlow Lite）\n- 先转成 ONNX 格式，再转成 TensorFlow Lite\n- 就像\"中转站\"，ONNX 帮你打通不同框架\n\n**生态支持：**\n- 训练：PyTorch、TensorFlow、MXNet\n- 推理：ONNX Runtime、TensorRT、OpenVINO\n- 云平台：Azure、AWS、阿里云都支持\n\n说白了，ONNX 就是 AI 模型的\"世界语\"——让不同框架之间能互相\"听懂\"，不用再为框架绑定发愁。\n"},"version":""},{"id":25,"title":"什么是 Pythonic 函数调用","content":"Pythonic function call 是一种新的工具调用方式，它允许 LLM 通过生成 Python 代码来调用函数，而不是传统的 JSON\nschema 方式。\n\n\n主要优势：#\n\n 1. 更接近自然语言和人类思维方式\n 2. 能够在单次对话中处理复杂的多步骤任务\n 3. 支持更灵活的逻辑控制（如条件判断、循环等）\n 4. 充分利用了 LLM 在预训练中获得的程序性知识\n\n\n如何使用#\n\n以下是一个典型的使用示例：\n\n首先定义可用的函数：\n\n\n\n当用户提出请求时，LLM 会生成完整的 Python 代码来解决问题：\n\n\n\n\n执行过程#\n\n代码执行通过 exec-python 包来完成，它会：\n\n 1. 解析模型生成的代码\n 2. 在安全的环境中执行代码\n 3. 跟踪函数调用和变量状态\n 4. 返回结构化的执行结果\n\n根据基准测试结果（DPAB-α），使用 Pythonic function call 的模型在复杂任务处理上通常比使用 JSON-based function\ncall 的模型表现更好。例如，Claude 3.5 Sonnet 在 Pythonic 方式下得分为 87，而在 JSON 方式下仅为 45。\n\nMODEL NAME                    PYTHONIC   JSON\nClosed Models                            \nClaude 3.5 Sonnet             87         45\no1-preview-2024-09-12         55         39\no1-mini-2024-09-12            59         35\ngpt-4o-2024-11-20             60         30\nOpen Models                              \n> 100B Parameters                        \nDeepSeek V3 (685B)            63         33\nMiniMax-01                    62         40\nLlama-3.1-405B-Instruct       60         38\n> 30B Parameters                         \nQwen-2.5-Coder-32b-Instruct   68         32\nQwen-2.5-72b-instruct         65         39\nLlama-3.3-70b-Instruct        59         40\nQwQ-32b-Preview               47         21\n< 20B Parameters                         \nDria-Agent-a-7B               70         38\nQwen2.5-Coder-7B-Instruct     44         39\nDria-Agent-a-3B               72         31\nQwen2.5-Coder-3B-Instruct     26         37\nQwen-2.5-7B-Instruct          47         34\nPhi-4 (14B)                   55         35\n\n\n参考资料#\n\n * Python Is All You Need? Introducing Dria-Agent-α\n * Dria Pythonic Agent Benchmark (DPAB)","routePath":"/guide/ai/what-is-pythonic-function-call","lang":"zh","toc":[{"text":"主要优势：","id":"主要优势","depth":2,"charIndex":87},{"text":"如何使用","id":"如何使用","depth":2,"charIndex":193},{"text":"执行过程","id":"执行过程","depth":2,"charIndex":269},{"text":"参考资料","id":"参考资料","depth":2,"charIndex":1527}],"domain":"","frontmatter":{"title":"什么是 Pythonic 函数调用","description":"什么是 Pythonic 函数调用","date":20250117,"plainLanguage":"**Pythonic 函数调用说白了就是：** 让 AI 写 Python 代码来调用工具，而不是写 JSON。\n\n就像你跟助理说\"帮我预约明天 10 点的会议\"，以前他得填一个复杂的表格（JSON），现在他直接写几行代码就搞定。更自然，更灵活。\n\n**用撸串时的话说：**\n以前让 AI 做事，就像让它填表格——\"姓名填这里，时间填那里\"，很死板。Pythonic 方式就像让它写个便条——\"明天10点开会，记得提醒我\"，更灵活，能处理复杂情况（比如\"如果时间冲突就改到下午\"）。\n\n**两种方式对比：**\n- **传统 JSON 方式**：就像填表格，一格一格填，写一个函数调用参数的 JSON 对象，比如 {\"function\": \"make_appointment\", \"params\": {\"day\": \"2024-01-20\", \"time\": \"10:00\"}}\n- **Pythonic 方式**：就像写便条，直接写 Python 代码，可以计算时间、检查冲突、循环处理，比如 \"tomorrow = datetime.now() + timedelta(days=1)\" 然后 \"if check_availability(...): make_appointment(...)\"\n\n**好处：**\n- **更灵活**：能处理复杂逻辑（if、for、while 循环、条件判断）\n- **更自然**：AI 在训练时学过很多 Python 代码，写起来更得心应手\n- **更强大**：一次能干多件事，不用来回对话\n- **更高效**：测试数据显示，用 Pythonic 方式的模型得分是 JSON 方式的 2 倍（比如 Claude 3.5 Sonnet：87 vs 45）\n\n说白了，Pythonic 函数调用就是\"让 AI 用写代码的方式干活\"——比填表格更自然、更强大、更高效。就像从\"手工填Excel\"升级到\"写Python脚本批量处理\"。\n"},"version":""},{"id":26,"title":"什么是大语言模型量化? 每个量化精度都代表什么?","content":"量化 (Quantization) 是一种通过降低模型参数的数值精度来压缩模型大小的技术. 在深度学习中, 模型参数通常以32位浮点数 (FP32) 存储,\n通过量化可以将其转换为更低精度的表示形式, 从而减少模型的内存占用和计算开销.\n\n如图, FP32 的大小是 4 字节 (每个字节8bit, 4字节 * 8bit = 32bit), 而 FP16 的大小是 2 字节 (每个字节8bit,\n2字节 * 8bit = 16bit).\n\n这也是为什么大家喜欢用 Q4 量化模型的原因, 跟 FP16 (16bit) 的模型相比, Q4 (4bit) 的模型只有 1/4 的大小.\n运行起来需要的内存也是1/4.\n\n现在大多数模型训练都采用 FP16 的精度, 最近出圈的 DeepSeek-V3 采用了 FP8 精度训练, 能显著提升训练速度和降低硬件成本.\n\n\nQ4_K_M 到底是什么意思?#\n\n所以我们大概了解了 Q4 实际上指的是 4bit 量化, 那么后缀都是什么意思呢？ 于是我整理了常见的量化精度和后缀供大家参考：\n\n这种命名方式一般是 GGUF & GGML 格式的模型. 他们通常采用 K 量化模型, 格式类似 Q4_K_M, 这里的 Q 后面的数字代表量化精度, K 代表\nK 量化方法, M 代表模型在尺寸和 PPL (Perplexity, 困惑度) 之间的平衡度, 有 0, 1, XS, S, M, L 等.\n\nPPL 是评估语言模型性能的重要指标, 它用来衡量模型对下一个词的预测准确程度\n\n常见 K 量化版本的PPL对比 (这是一个7B模型):\n\nTYPE    PPL INCREASE   PPL 13B TO 7B %   FILE SIZE   NOTE\nq2_k    0.8698         >100%             2.67GB      超大号的模型想要测试, 可以考虑 Q2 版本, 比如 unsloth 团队的 DeepSeek-V3-Q2_K_M\n                                                     量化版本, 我测下来实际感觉是可用的\nq4_0    0.2499         38.3%             3.5GB       \nq4_1    0.1846         28.3%             3.9GB       \nq4_ks   0.1149         17.6%             3.56GB      \nq4_km   0.0535         8.2%              3.80GB      如果没有提供 Q5 量化版本, 那么 Q4 量化版本也可以考虑, 建议至少 Q4_K_M 版本\nq5_0    0.0796         12.2%             4.3GB       \nq5_1    0.0415         6.36%             4.7GB       \nq5_ks   0.0353         5.41%             4.33GB      \nq5_km   0.0142         2.18%             4.45GB      目前我最推荐的量化大小, 实际体验下来各种模型量化中这个版本是最理想的\nq6_k    0.0044         0.67%             5.15GB      \nk8_0    0.0004         0.061%            6.7GB       如果您显存十分富裕, 当然推荐这个\n\n(数据来自: llama.cpp PR 1684)\n\n(具体全部量化选项可以看 llama.cpp 的源代码 ggml-quants.h)\n\n\n那 bf16, 4bit, int4, fp8 这种呢?#\n\n这种常见于其它量化格式的模型. 有了上面的经验我们很容易猜出来 bf16 是 16bit 的精度, 4bit 是 4bit 的精度. 同样也建议至少使用\n4bit 量化的模型, 除非模型特别大 200B+.","routePath":"/guide/ai/what-is-quantization-in-LLM","lang":"zh","toc":[{"text":"Q4_K_M 到底是什么意思?","id":"q4_k_m-到底是什么意思","depth":2,"charIndex":383},{"text":"那 bf16, 4bit, int4, fp8 这种呢?","id":"那-bf16-4bit-int4-fp8-这种呢","depth":2,"charIndex":1644}],"domain":"","frontmatter":{"title":"什么是大语言模型量化? 每个量化精度都代表什么?","description":"什么是大语言模型量化? 每个量化精度都代表什么?","date":20250129,"plainLanguage":"**量化说白了就是：** 把模型的\"精度\"降低，就像把高清照片压缩成标清，但 AI 还能用。\n\n就像你存照片，原图 10MB，压缩后 2MB，虽然画质差了点，但还能看。量化就是把模型参数从\"高清\"（32位）降到\"标清\"（4位、8位），模型变小了，但基本功能还在。\n\n**用菜市场的话说：**\n想象你买水果，以前用电子秤（精确到 0.01 克），现在用弹簧秤（精确到 10 克）。虽然精度低了，但够用，而且秤更便宜、更轻。量化就是给 AI 模型\"换个秤\"。\n\n**精度等级：**\n- **FP32（32位）**：原版，最精确，但占地方大\n- **FP16（16位）**：减半，效果差不多，训练常用\n- **Q4（4位）**：再减半，只有原来的 1/4 大小，普通人跑模型的首选\n- **Q2（2位）**：极限压缩，只有原来的 1/8，效果会差一些\n\n**Q4_K_M 是啥意思？**\n- **Q4**：4位量化\n- **K**：K 量化方法（一种优化算法）\n- **M**：中等质量（在文件大小和效果之间平衡）\n\n**效果：**\n- 模型文件从 100GB 降到 25GB（Q4）\n- 显存需求也降 4 倍\n- 速度可能还更快（因为数据量小了）\n- 但精度会稍微下降（通常不明显）\n\n说白了，量化就是\"用空间换精度\"——牺牲一点点精度，换来巨大的空间和速度提升。对普通人来说，Q4 或 Q5 量化是最佳选择。\n"},"version":""},{"id":27,"title":"什么是表示空间？","content":"(图片来自 huggingface.co/blog/Borise/law-vision-representation-in-mllms)\n\n表示空间 (Representation Space)\n是多模态人工智能中的核心概念，指不同模态数据（文本、图像、音频等）经过编码后映射到的统一数学空间。该空间使机器能够理解跨模态的语义关联，是实现多模态智能的数学基\n础。\n\n\n核心定义#\n\n表示空间本质上是高维向量空间，具有以下关键属性：\n\n * 跨模态可比性：不同模态数据在空间中的距离反映语义相似度\n\n * 线性组合性：支持语义向量运算（如 v_程序员 - v_代码 + v_绘画 ≈ v_画家）\n\n * 层次语义：不同维度对应不同抽象级别的特征（低层纹理→高层语义）\n\n\n技术实现#\n\n\n构建方法#\n\n\n\n\n数学特性#\n\n * 度量学习：定义距离函数 $d(v_i, v_j) = 1 - \\frac{v_i \\cdot v_j}{|v_i||v_j|}$\n\n * 流形假设：同类数据在空间中形成连续流形（如所有\"狗\"图像构成子空间）\n\n * 注意力兼容：支持跨模态注意力计算 $Attention(Q_{text}, K_{image}, V_{image})$\n\n\n应用场景#\n\n * 跨模态检索：text_emb = encode(\"火山喷发\") → 查找 argmin(d(v_img, text_emb))\n\n * 特征融合：[img_emb; audio_emb] → 融合层 → 联合表示\n\n * 零样本学习：通过prompt工程 v_\"医疗报告\" + v_CT影像 → 诊断建议\n\n * 内容生成：文本引导的图像编辑 v_原图 + v_\"添加彩虹\" → 生成新图\n\n\n实现挑战#\n\n * 模态鸿沟：不同模态特征分布差异（如图像特征的L2范数通常大于文本）\n\n * 维度灾难：随着维度增加，数据稀疏性导致相似度度量可靠性下降\n\n * 语义泄漏：图像背景纹理等无关特征被编码到语义维度\n\n * 评估困境：缺乏客观的几何语义评估指标\n\n\nRefs#\n\n * CLIP: Connecting text and images","routePath":"/guide/ai/what-is-representation-space","lang":"zh","toc":[{"text":"核心定义","id":"核心定义","depth":2,"charIndex":183},{"text":"技术实现","id":"技术实现","depth":2,"charIndex":334},{"text":"构建方法","id":"构建方法","depth":3,"charIndex":342},{"text":"数学特性","id":"数学特性","depth":3,"charIndex":352},{"text":"应用场景","id":"应用场景","depth":2,"charIndex":533},{"text":"实现挑战","id":"实现挑战","depth":2,"charIndex":740},{"text":"Refs","id":"refs","depth":2,"charIndex":872}],"domain":"","frontmatter":{"title":"什么是表示空间？","description":"什么是表示空间？","date":20250316,"plainLanguage":"**表示空间说白了就是：** 一个\"魔法空间\"，在这里图片、文字、声音都变成可以比较的\"点\"。\n\n就像地图，把北京、上海、广州都标成坐标点，你就能看出它们的远近关系。表示空间就是把\"苹果\"这个词、一张苹果的照片、\"apple\"这个英文单词都变成空间里的点，意思相近的点就靠得近。\n\n**用大白话说：**\n想象一个巨大的仓库，所有东西都按\"相似度\"摆放。狗的照片和\"汪汪叫\"的声音放在一起，猫的照片和\"喵喵叫\"放在一起。你想找\"可爱的动物\"，直接去那片区域就行。表示空间就是这个\"按语义摆放\"的仓库。\n\n**核心特性：**\n1. **距离=相似度**：空间里两个点越近，代表意思越相似\n2. **可以算术**：`国王 - 男人 + 女人 = 女王`（真的可以这样算！）\n3. **跨模态可比**：图片和文字都在同一个空间，可以直接比较\n\n**怎么构建：**\n- 用不同的编码器把图片、文字变成向量\n- 通过训练让\"意思相近\"的向量靠近\n- 最后形成一个\"语义地图\"\n\n**应用：**\n- **以图搜图**：上传一张图，找相似的图\n- **图文匹配**：判断图片和文字是不是在说同一件事\n- **零样本学习**：从来没见过\"独角兽\"，但知道\"马 + 角 = 独角兽\"\n\n**挑战：**\n- 不同模态的数据分布差异大，不好对齐\n- 维度太高会导致\"维度灾难\"（数据稀疏）\n- 背景噪声可能混进来，影响语义\n\n说白了，表示空间就是\"万物互联的数学空间\"——在这里，一切皆可比较，图片和文字终于能\"对话\"了。\n"},"version":""},{"id":28,"title":"什么是 Safetensors","content":"(图片来自 Hugging Face 官网)\n\nsafetensors 是由 Hugging Face 开发的一种安全高效的张量存储格式. 它专为机器学习模型设计, 旨在解决传统序列化格式（如\npickle）的安全漏洞问题, 同时提供更快的加载速度和跨框架兼容性.\n\n\nSafetensors 的主要特点和优势#\n\n * 安全性优先： 彻底解决了 pickle 格式的远程代码执行（RCE）漏洞风险, 通过限制反序列化操作仅加载张量数据, 从根本上杜绝了恶意代码注入的可能性.\n * 快速加载： 采用零拷贝（zero-copy）技术实现, 在CPU上, 如果文件已缓存, 则可以完全0拷贝（需满足张量连续存储条件）\n * 跨框架支持： 原生支持 PyTorch、TensorFlow、JAX 等主流深度学习框架, 支持多 GPU 设备间的无缝数据共享.\n * 轻量高效： 文件体积与性能经过优化, 支持与 LZ4/Zstandard 等压缩算法配合使用（注意：格式本身不包含压缩功能）\n * 类型安全： 通过严格的元数据校验确保数据类型和形状的一致性, 避免因类型错误导致的运行时崩溃.\n\n\nSafetensors 的应用#\n\nSafetensors 格式已成为现代机器学习生态的重要基础设施：\n\n * Hugging Face 模型分发： Hugging Face Hub 的大部分模型默认采用 safetensors 格式分发\n * 多框架协作： 支持在不同深度学习框架间安全共享模型权重\n * 生产环境部署： 被广泛应用于模型服务化（Model Serving）和边缘计算场景\n\n\n哪些框架支持 Safetensors#\n\n * Hugging Face Transformers\n * Hugging Face Diffusers\n * PyTorch (通过 safetensors 库)\n * TensorFlow (通过 safetensors 库)\n * JAX (通过 safetensors 库)\n\n\nReference#\n\nsafetensors","routePath":"/guide/ai/what-is-safetensors","lang":"zh","toc":[{"text":"Safetensors 的主要特点和优势","id":"safetensors-的主要特点和优势","depth":2,"charIndex":134},{"text":"Safetensors 的应用","id":"safetensors-的应用","depth":2,"charIndex":495},{"text":"哪些框架支持 Safetensors","id":"哪些框架支持-safetensors","depth":2,"charIndex":693},{"text":"Reference","id":"reference","depth":2,"charIndex":860}],"domain":"","frontmatter":{"title":"什么是 Safetensors","description":"什么是 Safetensors","date":20250210,"plainLanguage":"**Safetensors 说白了就是：** 更安全的模型文件格式——不怕中毒，加载还快。\n\n就像你下载文件，以前用的格式（pickle）可能藏病毒，一打开电脑就中招。Safetensors 就是\"防毒保险箱\"，只装数据，不会带病毒。\n\n**用大白话说：**\n想象你买外卖，以前的包装（pickle）可能被人动过手脚，打开可能有问题。Safetensors 就像\"真空包装\"，你能看到里面装了什么，而且肯定没人动过，安全放心。\n\n**核心优势：**\n1. **安全**：不会执行任何代码，杜绝远程攻击\n2. **快速**：加载速度比 pickle 快很多（零拷贝技术）\n3. **跨框架**：PyTorch、TensorFlow、JAX 都能用\n4. **轻量**：文件体积优化，不浪费空间\n\n**为什么要替换 pickle？**\n- **pickle 的问题**：加载时会执行代码，容易被攻击者利用\n- **Safetensors 的方案**：只加载数据，不执行代码，从根本上杜绝风险\n\n**应用：**\n- Hugging Face 上的大部分模型都用 Safetensors 格式\n- 生产环境部署首选\n- 多框架协作必备\n\n说白了，Safetensors 就是\"更安全更快的模型文件格式\"——就像从 HTTP 升级到 HTTPS，安全性和效率都提升了。\n"},"version":""},{"id":29,"title":"什么是 Sliding Window Attention","content":"\n\n图：Transformer中的滑动窗口注意力机制示意图（来源：arXiv 2502.18845v1）\n\nSliding Window Attention（滑动窗口注意力）是一种用于提升大型语言模型长文本处理效率的注意力机制. 它通过限制每个token的注意力范围,\n将Transformer的计算复杂度从平方级（O(n²)）降低到线性级（O(n)）, 同时保持对长距离依赖的捕捉能力.\n\n简单来讲, 在推理引擎中应用 SWA 可以显著降低长上下文的显存消耗.\n\n\nSWA 的核心原理与优势#\n\n * 局部注意力窗口：\n   \n   每个token只关注固定窗口大小（如4096 tokens）内的上下文, 而非整个序列. 如图1所示, 窗口会随着处理位置滑动, 形成连续的上下文覆盖.\n\n * 线性计算复杂度：\n   \n   传统Transformer的注意力计算量随序列长度呈平方增长, SWA通过固定窗口大小实现线性增长, 使处理万级token的文本成为可能.\n\n * 信息接力机制：\n   \n   采用sigmoid激活函数替代softmax（SWAT改进）, 结合平衡的ALiBi位置编码, 使模型能通过滑动窗口逐层传递上下文信息,\n   解决传统SWA的\"注意力下沉\"问题.\n\n * 训练-推理一致性：\n   \n   通过Sliding Window Attention Training（SWAT）框架, 在训练阶段就采用窗口化注意力,\n   消除传统方法中训练全注意力与推理局部注意力的差异.\n\n * 硬件友好性：\n   \n   固定窗口大小更适合GPU的并行计算特性, 配合内存映射（mmap）技术可实现快速加载, 相比传统Transformer推理速度提升3-5倍.\n\n\nSWA 的实际应用#\n\n * 长文档处理：\n   \n   处理整本书籍（如PG-19数据集）、法律文档等超长文本时, 显存占用保持稳定. 实验显示在16k tokens长度下,\n   困惑度（perplexity）仅上升0.15\n\n * 实时对话系统：\n   \n   在持续对话场景中, 采用动态窗口滑动策略, 只保留最近N轮对话的注意力上下文, 避免历史信息累积导致的性能下降\n\n\n支持 SWA 的实践框架#\n\n * llama.cpp 刚刚合并 SWA 支持的 PR\n\n * flash-linear-attention\n   \n   提供CUDA优化的SWA实现, 支持多GPU并行训练\n\n * nanoGPT\n   \n   新增SWA训练模式, 可在消费级GPU上训练10B级长文本模型\n\n\n参考文献#\n\n * 核心论文：https://arxiv.org/abs/2502.18845v1\n * 技术解析：https://klu.ai/glossary/sliding-window-attention","routePath":"/guide/ai/what-is-sliding-window-attention","lang":"zh","toc":[{"text":"SWA 的核心原理与优势","id":"swa-的核心原理与优势","depth":2,"charIndex":233},{"text":"SWA 的实际应用","id":"swa-的实际应用","depth":2,"charIndex":746},{"text":"支持 SWA 的实践框架","id":"支持-swa-的实践框架","depth":2,"charIndex":938},{"text":"参考文献","id":"参考文献","depth":2,"charIndex":1096}],"domain":"","frontmatter":{"title":"什么是 Sliding Window Attention","description":"什么是 Sliding Window Attention","date":20250522,"plainLanguage":"**滑动窗口注意力说白了就是：** 让 AI 只看\"附近的内容\"，而不是\"全文\"，省内存又快。\n\n就像你读书，传统方法要求你每读一个字都回头看一遍全书（累死），滑动窗口就是\"只看前后几页\"（比如前后 100 页），既节省精力又不影响理解。\n\n**用大白话说：**\n想象你在听老师讲课，传统注意力就像每说一句话，你都要把之前说的所有话再回想一遍（脑子要炸）。滑动窗口就是\"只记住最近 10 分钟的内容\"，远一点的就让它飘过去，既不累又能跟上节奏。\n\n**核心机制：**\n1. **固定窗口**：每次只关注前后 N 个词（比如 4096 个词）\n2. **滑动处理**：窗口跟着当前位置移动，就像\"移动的放大镜\"\n3. **信息接力**：通过多层叠加，远距离信息一层层传递过来\n\n**好处：**\n- **省显存**：不用存整个文本的注意力矩阵，只存窗口内的\n- **速度快**：计算量从 O(n²) 降到 O(n)，处理长文本快 3-5 倍\n- **能处理超长文本**：可以处理几万字甚至整本书\n\n**权衡：**\n- 远距离依赖能力稍弱（但通过多层叠加可以缓解）\n- 需要精心设计窗口大小\n\n**应用：**\n- 处理长文档（法律文件、小说、研究报告）\n- 实时对话（只记最近的对话，不记全部历史）\n- 资源受限环境（手机、边缘设备）\n\n说白了，滑动窗口注意力就是\"用局部换全局\"——只看眼前的，远处的让它飘，省资源还不影响效果。\n"},"version":""},{"id":30,"title":"什么是推测性解码","content":"推测性解码是一种用于优化大语言模型推理性能的技术。它的核心思想是：在当前大模型生成当前 token 的同时，使用小的草稿模型对未来的 token 进行预测。\n\n\n工作原理#\n\n双模型结构：\n\n * 主模型（Target Model）：完整的大语言模型，精度高但速度较慢\n * 草稿模型（Draft Model）：较小的模型，速度快但精度较低\n\n多头预测：\n\n * 在模型中添加多个预测头（speculative heads）\n * 每个预测头负责预测未来的一个 token（N+1, N+2, N+3...）\n * 可以在单次前向传播中预测多个 token\n * 使用较小的草稿模型快速预测多个可能的后续 token\n * 草稿模型通常是主模型的蒸馏版本或更小的变体\n\n验证机制：\n\n * 主模型会验证草稿模型预测的 token 序列\n * 如果预测正确，就可以直接使用\n * 如果预测错误，则回退到主模型重新生成\n\n\n性能提升#\n\n根据文章报告的数据：\n\n * 语言模型可获得约 2 倍的速度提升\n * 代码模型可获得约 3 倍的速度提升\n\n\n实现考虑#\n\n模型选择：\n\n * 草稿模型通常是主模型的 1/4 到 1/2 大小\n * 例如：如果主模型是 70B 参数，草稿模型可能是 7B 或 13B 参数\n\n预测头数量：\n\n * 语言模型通常使用 3-4 个预测头\n * 代码模型可以使用 6-8 个预测头\n\n效率平衡：\n\n * 需要在计算量、内存使用和预测准确性之间找到平衡\n * 预测头数量增加会带来额外计算开销\n * 但如果预测准确，可以减少内存访问次数\n\n这项技术的主要优势在于它能显著提升模型的推理速度，同时保证输出质量不受影响。它特别适合需要快速响应的生产环境使用。\n\n\nReference#\n\n * Fast Inference from Transformers via Speculative Decoding\n * Hitchhikers Guide to Speculative Decoding","routePath":"/guide/ai/what-is-speculative-decoding","lang":"zh","toc":[{"text":"工作原理","id":"工作原理","depth":2,"charIndex":80},{"text":"性能提升","id":"性能提升","depth":2,"charIndex":409},{"text":"实现考虑","id":"实现考虑","depth":2,"charIndex":472},{"text":"Reference","id":"reference","depth":2,"charIndex":743}],"domain":"","frontmatter":{"title":"什么是推测性解码","description":"什么是推测性解码","date":20250116,"plainLanguage":"**推测性解码说白了就是：** 让 AI 说话更快的小技巧——先猜后验证。\n\n就像你写作文，不用等老师批改完上一句再写下一句，而是先快速写个草稿，然后让老师检查。如果猜对了，直接用；猜错了，再改。\n\n**用大白话说：**\n想象你在跟朋友聊天，你一边说一边猜他下一句要说什么。如果他真的说了你猜的那句，你就直接接话，不用等他说完。如果他说的不一样，你再重新接话。这样聊天速度就快了。\n\n**具体怎么搞：**\n1. **大模型（主模型）**：就像那个说话很准但有点慢的朋友\n2. **小模型（草稿模型）**：就像那个说话快但可能不准的朋友\n3. **流程**：小模型先快速猜几个词，大模型检查一下，对的就用，错的就重新生成\n\n**效果：**\n速度能快 2-3 倍，但质量不变。就像你找了个\"预判大师\"帮你提前想好要说啥，你只需要点头或摇头就行。\n\n说白了，这就是\"用聪明的方法偷懒\"——不是真的偷懒，而是让 AI 更高效地工作。\n"},"version":""},{"id":31,"title":"什么是 Transformer?","content":"Transformer 是一种用于自然语言处理 (NLP) 的深度学习模型架构, 由 Vaswani 等人在 2017 年提出.\n它主要用于处理序列到序列的任务, 如机器翻译, 文本生成等.\n\n简单来讲, 文本生成的 Transformer 模型的原理是——\"预测下一个词\".\n\n用户给定的文本 (prompt), 模型会预测下一个词最有可能是什么. Transformer\n的核心创新和强大之处在于它使用的自注意力机制（self-attention mechanism）, 这使得它们能够处理整个序列, 并比之前的架构 (RNN)\n更有效地捕捉长距离依赖关系.\n\n另外需要注意的是, GitHub 上的 huggingface/transformers 是 HuggingFace 实现的 Transformer 模型库,\n包括了 Transformer 的实现和大量的预训练模型.\n\n目前的 LLM 基本都基于 Transformer 架构, 并对其进行优化技术和训练方法的改进.\n\n\nTransformer 的结构#\n\n每个文本生成 Transformer 都由以下三个关键组件构成：\n\n\n嵌入层（Embedding）：#\n\n * 文本输入被分割成称为词元（token）的更小单位, 可以是单词或子词\n * 这些词元被转换成称为嵌入（embeddings）的数值向量\n * 这些嵌入向量能够捕捉词语的语义含义\n\n\nTransformer 块：#\n\n这是模型处理和转换输入数据的基本构建单元. 每个块包括：\n\n * 注意力机制（Attention Mechanism）：\n   * Transformer 块的核心组件\n   * 允许词元之间相互通信\n   * 捕捉词语之间的上下文信息和关系\n * 多层感知器（MLP）层：\n   * 一个前馈网络, 独立处理每个词元\n   * 注意力层的目标是在词元之间路由信息\n   * MLP 的目标是优化每个词元的表示\n\n\n输出概率（Output Probabilities）：#\n\n * 最终的线性层和 softmax 层\n\n * 将处理后的嵌入转换为概率\n\n * 使模型能够预测序列中的下一个词元\n\n\nTransformer 的优点：#\n\n * 并行化处理：与 RNN 不同, Transformer 不需要按顺序处理数据, 因此可以更好地利用 GPU 进行并行计算, 提高训练速度.\n * 长距离依赖：自注意力机制使得 Transformer 能够有效捕捉序列中远距离的依赖关系.\n * 灵活性：Transformer 可以很容易地扩展到更大的模型 (如BERT、GPT等) , 并在多种 NLP 任务中表现出色.\n\n\nTransformer 的缺点：#\n\n * 计算复杂度高：自注意力机制的计算复杂度为O(n^2), 当输入序列长度较长时, 计算资源消耗较大.\n\n * 数据需求大：Transformer 通常需要大量的数据进行训练, 以便充分发挥其性能.\n\n * 缺乏内在的序列信息：由于没有内置的序列处理机制 (如 RNN 中的时间步) , 需要额外的机制 (如位置编码) 来引入序列信息.\n\n\nRefs#\n\n关于 Transformer 的内容实在是太多, 很难在简单的一篇文章中介绍清楚, 所以本篇其实算是一个索引, 后续我会针对每个技术细节做详细的介绍.\n\n另外本人虽然是从业者, 但更关注 LLM 下游应用, 对 LLM 训练本身经验并不多, 所以如果有任何错误还请大家指出.\n\n这两个引用中, 第一个是关于 transformer 的最经典论文, Attention Is All You Need. 正是这篇论文首次提出了\nTransformer.\n\n第二个是关于 transformer 的可视化解释, 可以很直观的看到 transformer 的内部结构.\n\n * Attention Is All You Need\n * Transformer Explainer","routePath":"/guide/ai/what-is-transformer","lang":"zh","toc":[{"text":"Transformer 的结构","id":"transformer-的结构","depth":2,"charIndex":444},{"text":"**嵌入层（Embedding）**：","id":"嵌入层embedding","depth":3,"charIndex":-1},{"text":"**Transformer 块**：","id":"transformer-块","depth":3,"charIndex":-1},{"text":"**输出概率（Output Probabilities）**：","id":"输出概率output-probabilities","depth":3,"charIndex":-1},{"text":"Transformer 的优点：","id":"transformer-的优点","depth":2,"charIndex":925},{"text":"Transformer 的缺点：","id":"transformer-的缺点","depth":2,"charIndex":1135},{"text":"Refs","id":"refs","depth":2,"charIndex":1326}],"domain":"","frontmatter":{"title":"什么是 Transformer?","description":"什么是 Transformer?","date":20250126,"plainLanguage":"**Transformer 说白了就是：** AI 的\"阅读理解\"神器，专门用来\"猜下一个词\"。\n\n就像你读小说，看到\"从前有座山，山上有座庙\"，你大概能猜到下一句是\"庙里有个老和尚\"。Transformer 就是干这个的——给它一段话，它猜下一个词是啥。\n\n**用大白话说：**\n想象你跟朋友聊天，他说\"今天天气真...\"，你脑子里立马想到\"好\"或者\"热\"。Transformer 就是那个\"预判大师\"，而且它看得更远——不仅能猜下一个词，还能理解整段话的意思。\n\n**核心就三件事：**\n1. **把文字变数字**：就像把汉字转成拼音，但转成的是 AI 能理解的数字\n2. **注意力机制**：就像你读文章时，有些词会\"高亮\"，Transformer 会自动找到重要的词\n3. **猜词**：最后输出一个概率，告诉你下一个词最可能是啥\n\n**为啥这么牛？**\n- 以前的技术（RNN）像排队，必须一个接一个处理，慢\n- Transformer 可以\"并行处理\"，就像多个人同时看一篇文章，快多了\n- 而且\"记忆力\"更好，能记住很远的上下文\n\n说白了，Transformer 就是让 AI \"真正理解语言\"的开始。现在所有的大模型（GPT、BERT 等）都是基于它改进的。\n"},"version":""},{"id":32,"title":"什么是向量数据库?","content":"(图片来自 www.exxactcorp.com)\n\n向量数据库（Vector Database）是一种专门设计用于存储、管理和搜索 向量嵌入（vector embeddings） 的数据库系统。\n\n其核心价值在于能够高效执行相似性搜索（similarity search），支持AI应用中常见的\"寻找最相似内容\"需求，成为现代人工智能基础设施的重要组成部分。\n\n\n向量数据库的工作原理#\n\n向量数据库的核心功能围绕以下几个关键环节：\n\n * 向量嵌入存储：将文本、图像等内容通过嵌入模型转换为高维数字向量并存储\n * 相似性检索：使用近似最近邻（ANN）算法实现高效搜索，支持余弦相似度、内积、欧式距离等多种度量方式\n * 索引机制：通过HNSW（分层可导航小世界图）实现快速导航，或IVF（反向文件索引）进行聚类加速\n * 元数据过滤：支持在相似性搜索中结合传统数据库的过滤条件（如时间范围、类别标签）\n\n这种设计使向量数据库能在毫秒级别内从数百万甚至数十亿向量中找出最相似项。\n\n\n向量数据库的优势#\n\n * 高效相似性搜索：通过ANN算法实现O(log n)时间复杂度，比传统数据库的精确搜索O(n)快很多\n\n * 混合查询能力：可同时处理\"找到与这张图片相似且价格低于100元的产品\"这类复合查询\n\n * AI应用集成：特别适合实现RAG架构中的长期记忆模块\n\n * 规模可扩展性：支持海量向量数据的存储和检索\n\n * 多模态支持：同时处理文本、图像、音频等不同类型的嵌入向量\n\n * 实时性能：支持高并发、低延迟的查询操作\n\n\n向量数据库可能存在的挑战#\n\n * 精度与速度权衡：更高精度通常意味着更慢的查询速度\n\n * 资源消耗：高维向量索引可能需要大量内存\n\n * 嵌入质量依赖：搜索结果质量很大程度上取决于输入嵌入的质量\n\n * 维度诅咒：随着向量维度增加，搜索效率可能下降\n\n * 复杂调优需求：最佳性能可能需要专业知识进行参数调优\n\n\n常见向量数据库产品#\n\n * Chroma：为AI应用设计的嵌入式向量数据库\n\n * Pinecone：全托管式向量数据库服务\n\n * Weaviate：开源向量搜索引擎\n\n * Milvus：开源分布式向量数据库\n\n * Qdrant：高性能向量相似度搜索引擎\n\n * FAISS (Facebook AI Similarity Search)：需注意这是算法库而非完整数据库（通常需配合其他存储系统使用）\n\n * PGVector：PostgreSQL 的向量扩展，由于 Postgres 性能很强, 因此支持的维度特别高\n\n\nRefs#\n\n * Understanding Vector Databases\n * Vector Databases Explained","routePath":"/guide/ai/what-is-vector-database","lang":"zh","toc":[{"text":"向量数据库的工作原理","id":"向量数据库的工作原理","depth":2,"charIndex":182},{"text":"向量数据库的优势","id":"向量数据库的优势","depth":2,"charIndex":443},{"text":"向量数据库可能存在的挑战","id":"向量数据库可能存在的挑战","depth":2,"charIndex":669},{"text":"常见向量数据库产品","id":"常见向量数据库产品","depth":2,"charIndex":828},{"text":"Refs","id":"refs","depth":2,"charIndex":1094}],"domain":"","frontmatter":{"title":"什么是向量数据库?","description":"什么是向量数据库?","date":20250308,"plainLanguage":"**向量数据库说白了就是：** 专门用来\"找相似东西\"的数据库。\n\n就像你在淘宝\"拍照搜同款\"，上传一张图，它能找到类似的商品。向量数据库就是干这个的——但不仅能搜图片，还能搜文字、声音、视频。\n\n**用大白话说：**\n想象你去菜市场，传统数据库就像\"按名字找摊位\"（精确匹配），你得知道叫啥。向量数据库就像\"按味道找摊位\"（相似性搜索），你闻着味儿就能找到卖臭豆腐的，哪怕摊主换了名字。\n\n**工作原理：**\n1. **把东西变成数字**：图片、文字都转成一串数字（向量）\n2. **建索引**：把这些数字按\"相似度\"组织起来\n3. **快速查找**：给一个新东西，立马找出最相似的 TOP 10\n\n**核心能力：**\n- **相似性搜索**：找\"意思相近\"的东西，不是\"一模一样\"的\n- **混合查询**：既能按相似度搜，又能按价格、时间筛选\n- **毫秒级响应**：从上亿条数据里找出结果，只需几毫秒\n\n**典型应用：**\n- **AI 聊天机器人**：让 AI 记住你之前说的话（RAG 系统）\n- **推荐系统**：你喜欢这个，推荐类似的\n- **图片搜索**：上传图片，找相似图片\n- **去重**：找出重复或相似的内容\n\n**常见产品：**\n- Pinecone（云服务，开箱即用）\n- Milvus（开源，功能强大）\n- Chroma（轻量级，适合小项目）\n- PGVector（PostgreSQL 插件，老司机首选）\n\n说白了，向量数据库就是\"以图搜图、以文搜文\"背后的技术——让 AI 能\"按感觉\"找东西，而不是只能\"按名字\"找。\n"},"version":""},{"id":33,"title":"什么是向量嵌入?","content":"(图片来自 qdrant.tech)\n\n向量嵌入（Vector\nEmbeddings）是将复杂数据（如文本、图像、音频等）转换为密集数值向量的过程和结果。这些向量通常是高维的数字数组，使机器能够\"理解\"数据间的语义关系。\n\n其核心思想是通过数学表示捕捉原始数据的语义信息，将抽象概念映射到多维空间，这样语义空间的相似性，就可以转化为向量空间中的接近性(数学问题)。\n\n\n向量嵌入工作流程#\n\n典型的向量嵌入过程包含三个关键阶段：\n\n * 特征提取：从原始数据（文本、图像等）中识别和提取关键特征\n * 向量化转换：将提取的特征通过神经网络映射到高维向量空间\n * 维度处理：根据需要进行降维或标准化，优化向量表示\n\n这种机制使计算机能够以数学方式处理和\"理解\"复杂的非结构化数据。\n\n\n向量嵌入的优点（针对数据库场景）#\n\n * 稠密表示：相比传统稀疏向量（如TF-IDF）更节省存储空间\n\n * 相似性保持：原始数据相似性在向量空间得以保留（余弦相似度≈语义相似度）\n\n * 跨模态统一：允许文本/图像/视频在同一空间进行联合检索\n\n * 索引友好：适合HNSW、IVF-PQ等近似最近邻算法加速\n\n * 增量更新：支持新数据嵌入无需重建整个向量空间\n\n\n向量嵌入可能存在的问题（数据库视角）#\n\n * 维度膨胀：维度特别多的向量会显著增加存储和内存消耗\n\n * 距离失真：降维处理可能破坏原始空间关系\n\n * 版本漂移：不同模型版本生成的向量不可直接比较\n\n * 冷启动：空数据库阶段难以建立有效索引结构\n\n * 精度衰减：量化压缩（如int8）导致的检索精度损失\n\n\n核心应用场景#\n\n * 混合搜索：结合元数据过滤与向量相似性检索（如语义搜索）\n\n * 内容去重：通过向量距离识别重复/相似内容\n\n * 智能推荐：基于用户行为向量的实时物品匹配（兴趣相似度计算）\n\n * 时序分析：追踪向量漂移模式（用户兴趣/内容热点的演化分析）\n\n * 知识管理：RAG系统中的高效知识检索与上下文关联\n\n * 聚类分析：自动发现数据中的潜在模式和分组结构\n\n * 缓存优化：高频查询结果的向量空间缓存加速\n\n\nRefs#\n\n * What are Embeddings?\n\n * Understanding Vector Embeddings","routePath":"/guide/ai/what-is-vector-embedding","lang":"zh","toc":[{"text":"向量嵌入工作流程","id":"向量嵌入工作流程","depth":2,"charIndex":184},{"text":"向量嵌入的优点（针对数据库场景）","id":"向量嵌入的优点针对数据库场景","depth":2,"charIndex":342},{"text":"向量嵌入可能存在的问题（数据库视角）","id":"向量嵌入可能存在的问题数据库视角","depth":2,"charIndex":528},{"text":"核心应用场景","id":"核心应用场景","depth":2,"charIndex":686},{"text":"Refs","id":"refs","depth":2,"charIndex":902}],"domain":"","frontmatter":{"title":"什么是向量嵌入?","description":"什么是向量嵌入?","date":20250307,"plainLanguage":"**向量嵌入说白了就是：** 把文字、图片这些\"人话\"翻译成 AI 能算的\"数字话\"。\n\n就像你把汉字转成拼音，但向量嵌入是把任何东西（文字、图片、声音）转成一串数字。神奇的是，意思相近的东西，转出来的数字也相近。\n\n**用菜市场的话说：**\n想象菜市场里，每个摊位都有个坐标。卖菜的摊位都在 A 区，卖肉的都在 B 区，卖鱼的都在 C 区。向量嵌入就是给每个词、每张图也标个\"坐标\"，意思相近的标在一起。这样 AI 找\"苹果\"的时候，就能在\"水果区\"找到，而不是跑到\"肉类区\"。\n\n**核心原理：**\n1. **转数字**：把\"苹果\"转成 [0.2, 0.8, 0.1, ...] 这样一串数字\n2. **保相似性**：\"苹果\"和\"梨\"的数字很接近，\"苹果\"和\"汽车\"的数字差很远\n3. **可计算**：AI 可以算两个向量的距离，距离近 = 意思相近\n\n**应用场景：**\n- **搜索**：你搜\"好吃的\"，AI 能找到\"美食\"、\"餐厅\"这些相关词\n- **推荐**：你喜欢看科幻，AI 推荐其他科幻内容\n- **去重**：找出重复或相似的内容\n\n说白了，向量嵌入就是让 AI \"理解\"人类语言和图片的桥梁——不是真的理解，而是用数学方法\"假装理解\"，但效果很好。\n"},"version":""},{"id":34,"title":"什么是vibe coding?","content":"什么是vibe coding?#\n\n\n\nVibe Coding是由Andrej Karpathy （前特斯拉 AI 总监）于 2025 年2月推出的一种革命性的编程范式。其核心在于利用\nAI（尤其是大型语言模型 (LLM)）来辅助编程。开发人员无需编写每一行代码，只需用自然语言描述所需的功能，AI 即可生成可运行的代码。\n\n这种方法正在迅速获得关注，并被誉为改变游戏规则的方法，甚至非程序员也可以“按照氛围编码”。\n\n下面通过一个示例说明如何使用 AI 从构思到落地，涵盖需求描述、代码生成和分享。即使你是初学者，也能亲身体验Vibe Coding的魅力。\n\n\n从头开始开发待办事项列表 Web 应用程序#\n\n\n步骤1：需求分析#\n\n首先，我们用自然语言描述核心功能：\n\n * 该页面应该有一个文本输入字段和一个“添加”按钮，以便用户输入和添加任务。\n * 添加任务后，它应该作为列表项出现在页面上。\n * 每个任务都应该有一个“删除”按钮，以将其从列表中删除。\n\n\n步骤2：创建AI提示词#\n\n我们可以将上述需求转化为清晰的AI提示。例如，使用Claude，我们可以询问：\n\n> 请生成一个基于 HTML、CSS 和 JavaScript 的待办事项列表 Web\n> 应用。该页面应包含一个文本输入字段和一个“添加”按钮。当用户输入任务并点击“添加”时，该任务应显示为下方的列表项。每个任务都应有一个“删除”按钮，用于将其从\n> 列表中移除。请提供包含所有必要代码的完整 HTML 文件。\n\n说明：此提示词清晰地指定了技术栈（HTML/CSS/JS）、所需的 UI 元素及其行为。像这样结构良好的题目有助于 AI 一次性生成准确、实用的代码。\n\n\n步骤3：AI生成代码#\n\n一旦Claude收到该请求，它就会根据其训练数据生成相应的代码解决方案。\n\n\n\n\n步骤4：调试和改进#\n\n进行一些测试来检查核心功能是否按预期运行：\n\n * 输入“买牛奶”，点击添加→待办事项列表显示“买牛奶”，并带有删除按钮。\n * 输入“编写代码”，点击添加→列表中出现第二项“编写代码”。出现在列表中。\n * 刷新页面→之前添加的任务仍然存在。\n * 点击“购买牛奶”旁边的删除按钮→ 该商品已成功移除\n\n\n\n一切功能正常，接下来改进用户界面。为了提升美观度，我们让Claude为待办事项列表添加了磨砂玻璃（模糊）效果。\n\n\n\n最终效果：\n\n\n\n此示例演示了完整的Vibe 编码过程：\n\n从一个想法开始 → 编写清晰的 AI 提示 → 从 AI 获取初始代码 → 测试并识别问题 → 请求改进 → 接收优化代码\n\n在整个过程中，我们几乎没有自己编写任何业务逻辑。相反，我们更像是产品经理和测试员，让人工智能来处理代码。这就是Vibe\nCoding的真正力量——将重点从编写代码转移到构思和细化需求。","routePath":"/guide/ai/what-is-vibe-coding","lang":"zh","toc":[{"text":"什么是vibe coding?","id":"什么是vibe-coding","depth":2,"charIndex":-1},{"text":"从头开始开发待办事项列表 Web 应用程序","id":"从头开始开发待办事项列表-web-应用程序","depth":2,"charIndex":281},{"text":"步骤1：需求分析","id":"步骤1需求分析","depth":3,"charIndex":306},{"text":"步骤2：创建AI提示词","id":"步骤2创建ai提示词","depth":3,"charIndex":434},{"text":"步骤3：AI生成代码","id":"步骤3ai生成代码","depth":3,"charIndex":725},{"text":"步骤4：调试和改进","id":"步骤4调试和改进","depth":3,"charIndex":779}],"domain":"","frontmatter":{"title":"什么是vibe coding?","description":"什么是vibe coding?","date":20250611,"plainLanguage":"**Vibe Coding 说白了就是：** 用\"说人话\"写代码——告诉 AI 你想要啥，它给你写代码。\n\n就像点外卖，以前你得自己做饭（写代码），现在你只需要说\"我要个宫保鸡丁\"（描述需求），外卖小哥（AI）就送来了（生成代码）。\n\n**用大白话说：**\n想象你去烧烤摊，以前你得自己穿串、自己烤（写代码）。Vibe Coding 就是\"你只需要跟老板说'来 20 串羊肉、10 串鸡翅'（描述需求），老板给你烤好端上来\"。\n\n**工作流程：**\n1. **描述需求**：\"我要一个待办事项 App，能添加、删除任务，有磨砂玻璃效果\"\n2. **AI 生成代码**：Claude/GPT-4 给你生成 HTML/CSS/JS 代码\n3. **测试改进**：\"添加按钮太小了，字体再大一点\"\n4. **AI 优化代码**：AI 根据反馈调整代码\n5. **重复 3-4 直到满意**\n\n**核心价值：**\n- **非程序员也能\"编程\"**：只要会说话，就能让 AI 写代码\n- **程序员效率飙升**：不用写重复代码，专注创意和需求\n- **快速原型**：几分钟搞定以前要几小时的原型\n\n**实际例子：**\n- 你说：\"做个计算器，能加减乘除，界面要简洁\"\n- AI 生成完整代码（HTML+CSS+JS）\n- 你测试：\"乘法按钮点不了\"\n- AI 修复 bug\n- 你优化：\"加个历史记录功能\"\n- AI 添加功能\n\n**适用场景：**\n- 快速原型/demo\n- 简单工具/脚本\n- UI 界面开发\n- 学习编程（看 AI 怎么写）\n\n**局限：**\n- 复杂系统还是需要程序员\n- 需要懂一点技术才能判断 AI 写得对不对\n- 生成的代码可能不够优化\n\n说白了，Vibe Coding 就是\"把程序员变成产品经理\"——你负责想需求、提要求，AI 负责写代码。就像从\"自己做饭\"到\"点外卖\"的转变。\n"},"version":""},{"id":35,"title":"CXL 会是大语言模型的内存解决方案吗?","content":"(图片来自 mouser)\n\nCompute Express Link ( CXL ) 是一种开放标准互连, 用于高速, 高容量中央处理单元(CPU) 到设备以及 CPU 到内存连接,\n专为高性能数据中心计算机而设计.\n\nCXL 包括:\n\n * CXL.io: PCIe 的块输入/输出协议\n * CXL.cache: 用于访问的新缓存一致性协议系统内存\n * CXL.mem: 用于访问的新缓存一致性协议设备内存\n\n今天主要介绍的是, 基于 CXL 的可以插在 PCIe 插槽上的内存设备.\n\n\n简介#\n\n其实 CXL 是 intel 牵头搞的, 继 Optane DCPMM 后, intel 对扩展内存的其他方向上的尝试, 后续阿里巴巴, 思科, 戴尔EMC,\nMeta, 谷歌, HPE, 华为, 微软成立了 CXL联盟.\n\n后续 Gen-Z联盟（Gen-Z联盟是搞架顶内存的, 机架顶部专门有个FPGA服务器, 这个服务器由FPGA连接一大堆内存给其他服务器用）,\nOpenCAPI（IBM搞的类似的东西, 给 Power 平台用的）均纳入到了 CXL.\n\n目前市面上投产的 CXL 2.0 版本是基于 PCIe 5.0, 最大支持8条DDR5内存, 每条最大 512GB, 总容量 4TB (头图). 另外还有\nSamsung 基于 E3.s 封装的版本, 但也是 CXL 2.0 (下图).\n\n\n\n(图片来自 Samsung Newsroom)\n\n在设备中, CXL内存显示为无CPU-NUMA节点. CXL 协议支持交换, 甚至CXL Fabric之类的光传输方案. 然而, CXL\n内存引入了更长的内存访问延迟, 即 CPU-> PCIe控制器 -> CXL控制器 (如下图).\n\n\n\n(图片来自 ipoom-jeong.com)\n\n\n那么能用在 LLM 上吗?#\n\n简单回答, 不行.\n\n虽然目前 CXL 设备在硬往 AI 方向上靠, 但其最大瓶颈仍然是带宽, 目前市面上 CXL 2.0 设备最大带宽是 64GB/s, 而 LLM 动辄\nTB级别的带宽需求, 显然是远远不够的.\n\n拿70B级别的模型举例, Q4 量化都达到了 40GB, 想要每秒钟输出 100 token, 需要接近 4TB 的内存带宽. 折合下来, 需要 62 块\nCXL 2.0 设备, 这个成本不比直接购买显卡便宜.\n\n而最新的 CXL 3.1 标准 (注意 CXL 3.0版本目前好像也没投产), 即使支持 PCIe 6.0, x16 也就是 128GB/s 还是太慢了.\n按照跑 70B 模型达到 100token/s 计算, 需要大概 31 块 CXL 设备, 每个插 64GB 内存, 插满8条, 总计容量来到了 15TB,\n带宽才达到 4TB/s. 导致对于 LLM 场景, 容量过大, 而速度又不够, 容量和速度间过于失衡.\n\n如果用作训练呢? 比如用作张量卸载等场景? 答案是, 也不太好, 由于远端 NUMA 设备的巨大延迟, 通常都会超过 400ns, 导致张量卸载的效率不佳.\n\n一点优势也没有吗? 不是的, 因为存储容量巨大, 所以 batch size 可以很大, 即可以同时支持更多的用户来请求大模型.\n\n我的结论是, 目前 CXL 内存, 在 LLM 场景下, 还不能作为内存解决方案. 即使理论上可以搭建多机器交换方案, 目前也没看到实际应用的报告.\n所以综合衡量成本和风险, 不如直接买显卡.\n\n\nRefs#\n\n * CXL 3.1 standard\n * demystifying-cxl-memory-with-genuine-cxl-ready-systems-and-devices\n * Exploring and Evaluating Real-world CXL: Use Cases and System Adoption","routePath":"/guide/hardware/does-CXL-will-be-LLM-memory-solution","lang":"zh","toc":[{"text":"简介","id":"简介","depth":2,"charIndex":248},{"text":"那么能用在 LLM 上吗?","id":"那么能用在-llm-上吗","depth":2,"charIndex":776},{"text":"Refs","id":"refs","depth":2,"charIndex":1463}],"domain":"","frontmatter":{"title":"CXL 会是大语言模型的内存解决方案吗?","description":"CXL 会是大语言模型的内存解决方案吗?","date":20250125,"plainLanguage":"**CXL 作为 LLM 内存方案说白了就是：** 一根\"内存扩展条\"，插在 PCIe 插槽上，让服务器内存从 2TB 扩到 6TB——但速度慢点，价格贵。\n\n就像手机扩展存储，原来只能插 256GB 内存条（DDR5），现在可以插个\"外接硬盘\"（CXL）扩到 4TB。虽然比原装内存慢，但比没有强多了。\n\n**用大白话说：**\n想象餐厅厨房（服务器）空间不够放食材（内存），你有两个选择：1) 换更大的厨房（换服务器），2) 在隔壁租个小仓库（CXL）。虽然去仓库拿东西慢点，但总比没地方放强。\n\n**CXL 是什么：**\n- 一种插在 PCIe 插槽上的\"内存扩展卡\"\n- 单条最大 4TB（8 条 512GB DDR5）\n- 比普通内存慢（延迟高），但比没有好\n\n**为啥要用 CXL：**\n- **大模型吃内存**：跑 70B 模型需要 140GB+，普通服务器装不下\n- **扩展便宜**：插个 CXL 卡比换整台服务器便宜\n- **灵活**：需要多少内存就插多少\n\n**局限（为啥不是完美方案）：**\n- **慢**：延迟比普通内存高 10-20 倍（200-400ns vs 100ns）\n- **贵**：CXL 内存比普通 DDR5 贵\n- **兼容性问题**：需要 CPU 和主板支持（目前只有 Intel Sapphire Rapids 及以上）\n- **生态不成熟**：厂商少、产品少、坑多\n\n**实际应用：**\n- **推理场景**：对延迟不那么敏感，可以用 CXL 扩展内存\n- **训练场景**：延迟敏感，还是得用 HBM 或普通 DDR5\n\n说白了，CXL 是\"过渡方案\"——不是最优解，但在显存/内存不够时能救急。就像临时租仓库，不如直接买大房子，但总比没地方放东西强。\n"},"version":""},{"id":36,"title":"什么是 1DPC? 为什么内存条要插在远端插槽?","content":"稍微装过机的同学可能都知道, 内存不能乱插, 否则可能会无法开机. 而有经验的同学会去查一查主板手册, 看看内存插槽的配置.\n\n如上图, 就是 DELL Precision T7920 工作站的内存插槽配置. 我们可以看到 DPC 这一列做的 1DPC 和 2DPC 的区分.\n那么这到底是什么意思呢?\n\n\n1DPC 和 2DPC 是什么?#\n\n首先要澄清的是, 2DPC 可不是双通道内存的意思. 简单来讲, 1DPC 是 每个通道一插条内存 (one DIMM per channel) 的缩写.\n\n现在大多数消费级主板都是双通道, 4个内存插槽. 所以可以推理出来, 1DPC 配置就是每个通道只插一条内存, 或者只在通道1上插1条内存. 而 2DPC\n配置则是插满4个内存插槽.\n\n\n为什么 1DPC 配置要插在远离CPU的最外端?#\n\n\n\n(图片来自 forum-en.msi.com)\n\n现代主板内存布线主要采用两种拓扑结构, Daisy Chain (菊花链) 和 T-Topology (T型拓扑) . 现在高频消费级主板大多采用菊花链拓扑,\n而服务器/工作站主板更多使用T型拓扑.\n\n通过图片我们可以看到, 菊花链的拓扑结构, 内存控制器信号的终止节点在最远端. 而 T型拓扑每个内存插槽都是终止节点.\n\n菊花链的优点:\n\n * 信号完整性要求：当使用1DPC配置时, 内存控制器信号需要直达最远端插槽\n * 终端电阻位置：DDR4/DDR5 规范要求通道末端必须配置终端电阻 (On-Die Termination, 现在已经集成在内存条上了) ,\n   插外侧插槽可确保信号路径阻抗匹配以及减少信号反射\n\n而T型拓扑的缺点有:\n\n * 分支结构缺陷：当只插1条内存时, 未插内存的分支会产生阻抗突变以及造成信号反射\n * 串扰增加：空置插槽的耦合电容会降低信号质量\n\n可以看到T型拓扑的缺点基本都存在于没插满内存的情况, 所以服务器/工作站主板大多采用T型拓扑(最高配置获得最优性能).\n\n以上这些全都是为了保证DDR信号在电路中的传输质量, 我们现在使用的内存频率对于电气特性要求非常高, 信号都在ns级别. 只有传输质量上升,\n电路才能运行在更高的频率.\n\n现在的 DDR5-8000 内存对于 1990 年代 386 电脑上的 FPM 内存 (25MHz) 来讲简直是外星科技.\n\n\nRefs#\n\n强烈建议花时间阅读 MSI 论坛的这篇帖子, 介绍的非常全面.\n\n * ram-explained-why-two-modules-are-better-than-four-single-vs-dual-rank-stabil\n   ity-testing","routePath":"/guide/hardware/what-is-1DPC","lang":"zh","toc":[{"text":"1DPC 和 2DPC 是什么?","id":"1dpc-和-2dpc-是什么","depth":2,"charIndex":153},{"text":"为什么 1DPC 配置要插在远离CPU的最外端?","id":"为什么-1dpc-配置要插在远离cpu的最外端","depth":2,"charIndex":345},{"text":"Refs","id":"refs","depth":2,"charIndex":1005}],"domain":"","frontmatter":{"title":"什么是 1DPC? 为什么内存条要插在远端插槽?","description":"什么是 1DPC? 为什么内存条要插在远端插槽?","date":20250131,"plainLanguage":"**1DPC 说白了就是：** 每个通道只插一根内存条——而且要插在离 CPU 最远的那个槽。\n\n就像插排，虽然有 4 个孔，但只插 1 个插头时，要插在最远端，这样信号最稳定。内存也是一样。\n\n**用大白话说：**\n想象一条传送带（内存通道），上面有 2 个装货点（插槽）。1DPC 就是\"只在一个装货点装货\"，而且要选最远端的那个——因为传送带的终点在那，装货最稳。\n\n**核心概念：**\n- **DPC = DIMM Per Channel（每通道插几根）**\n- **1DPC**：每个通道插 1 根（或只用 1 个通道）\n- **2DPC**：每个通道插 2 根（插满）\n\n**为啥要插远端？**\n- **菊花链拓扑**：现代主板（尤其高频内存）用\"菊花链\"布线，信号从 CPU 出发，经过近端插槽，终止在远端插槽\n- **信号终止**：终止点（远端）信号最稳定，插这里内存能跑更高频率\n- **1DPC 配置**：只插 1 根时，插远端能享受最佳信号质量\n\n**具体插法（以 4 槽主板为例）：**\n- **2 根内存（双通道）**：插 A2 + B2（中间两个）\n- **1 根内存（单通道）**：插 A2（离 CPU 第二远的）\n- **4 根内存（双通道满载）**：插满，但频率会降（2DPC）\n\n**服务器/工作站：**\n- **1DPC**：速度最快，能跑最高频（如 DDR5-5600）\n- **2DPC**：容量大，但速度降（如降到 DDR5-4800）\n\n说白了，1DPC 就是\"少插几根，但插对位置\"——牺牲容量换性能，而且要插在\"信号终点\"才能发挥最佳效果。\n"},"version":""},{"id":37,"title":"什么是 L1 缓存?","content":"(图片来自 wikipedia)\n\nCPU 的 L1 Cache (一级缓存) 是集成在 CPU 芯片上的高速 SRAM, 用于存储经常访问的数据或指令, 以达到降低访问内存成本的目的.\n\n如图, 是一个 Intel i386 主板, 其中 L1 Cache 集成到了左上角 CPU 内部, 而左下角的 ISSI 芯片则是 L2 缓存.\n\n没错, 在这个时代 L2缓存还是外置的. 所以当时 L2 坏掉导致电脑无法启动是很正常的现象.\n\n\n基本概念#\n\n简单来讲 CPU 在内存中读取或写入时, 会检查该位置的数据是否已经在缓存中. 如果存在, 则CPU将从高速缓存中读取或写入, 而不是较慢的主内存.\n\n那么数据是怎么同步的呢？通常我们把内存和缓存中互相传输的数据分成固定大小, 他们的学名叫做 cache lines 或者 cache blocks.\n\n\nCache Entry#\n\ncache line 从内存复制到缓存时, 会创建一个 cache entry, cache entry 包含复制的数据和内存位置 (叫做tag) .\n\n\nCache Hit#\n\n当CPU需要读取或者写入数据时, CPU先检查 cache entry 是否存在, 如果 cache entry 的 tag (即内存位置)在缓存中,\n则意味着缓存命中 cache hit, CPU从 cache entry 中读取或者写入.\n\n\nCache Miss#\n\n当如果 cache entry 的 tag 不在缓存中, 则意味着缓存未命中, 即 cache miss, CPU 会分配一个新的 cache entry\n若缓存已满需根据替换策略淘汰旧条目) 并从内存中读取数据. 然后再从 cache entry 中读取或者写入.\n\n\n✅ 优点#\n\n * 速度比主内存快100倍左右 (AMD 9800x3D 的 L1 访问 latency 是 0.7ns, 使用 DDR5-6200 内存的 latency\n   是 65.9 ns)\n * 降低内存访问功耗\n * 缓解冯·诺伊曼瓶颈\n\n\n❌ 局限#\n\n * 硅片面积成本高\n * 容量限制导致缓存颠簸 (cache thrashing)\n * 一致性维护开销 (MESI协议)\n\n\n架构差异比较#\n\n特性     X86 (INTEL CORE)                       ARM (CORTEX-X)              RISC-V (SIFIVE)\n典型容量   32KB+32KB                              64KB+64KB                   可配置 (16-128KB)\n关联度    8-way                                  2-way L1 指令, 4-way L1 数据    2-8 way可选\n替换策略   LRU                                    伪随机                         可编程策略\n预取器    自适应                                    静态模式                        可选模块\n延迟     4-5 cycles (Intel i7-6700 (Skylake))   3 cycles (ARM Cortex-A53)   3-5 cycles (SiFive Freedom U740)\n\n\nRefs#\n\n * Gallery of Processor Cache Effects\n * CPU Cache\n * Cortex A53 Documents\n * Intel i7-6700 (Skylake), 4.0 GHz (Turbo Boost), 14 nm. RAM: 16 GB, dual\n   DDR4-2400 CL15 (PC-19200).\n * Amlogic S905 (ARM Cortex-A53), 1536 MHz, (28 nm). 4 cores. 2 GB DDR3-1824\n   (13-13-13) (32-bit). ODROID-C2 board.\n * SiFive Freedom U740 (U74 core), 1200 MHz, (28 nm). 4 cores. 16 GB (DDR4).\n   SiFive HiFive Unmatched.","routePath":"/guide/hardware/what-is-L1-cache","lang":"zh","toc":[{"text":"基本概念","id":"基本概念","depth":2,"charIndex":219},{"text":"Cache Entry","id":"cache-entry","depth":3,"charIndex":379},{"text":"Cache Hit","id":"cache-hit","depth":3,"charIndex":471},{"text":"Cache Miss","id":"cache-miss","depth":3,"charIndex":607},{"text":"✅ 优点","id":"-优点","depth":2,"charIndex":756},{"text":"❌ 局限","id":"-局限","depth":2,"charIndex":884},{"text":"架构差异比较","id":"架构差异比较","depth":2,"charIndex":956},{"text":"Refs","id":"refs","depth":2,"charIndex":1496}],"domain":"","frontmatter":{"title":"什么是 L1 缓存?","description":"什么是 L1 缓存?","date":20250206,"plainLanguage":"**L1 缓存说白了就是：** CPU 的\"工作台\"——最常用的工具放手边，随手就能拿到。\n\n就像厨师炒菜，盐、酱油、醋这些常用调料放在灶台旁边（L1），稍微不常用的放在橱柜里（L2），更不常用的放在仓库（内存），最不常用的放在超市（硬盘）。L1 就是那个\"灶台旁边的置物架\"。\n\n**用大白话说：**\n想象你在烤串，最常用的调料（孜然、辣椒）放在手边的小盒子里（L1），拿起来就用。如果没有，就去桌子上的大盒子里找（L2），再没有就去后厨拿（L3/内存），最后去仓库拿（硬盘）。离得越远，拿得越慢。\n\n**核心概念：**\n1. **Cache Hit（命中）**：CPU 要的数据在 L1 里，直接拿，超快（1-2 ns）\n2. **Cache Miss（未命中）**：L1 里没有，去 L2 找，再没有去 L3，最后去内存（100+ ns）\n3. **Cache Line（缓存行）**：数据不是一个一个搬，而是一批一批搬（通常 64 字节）\n\n**为啥这么快：**\n- **SRAM**：L1 用的是超快的 SRAM（静态存储器），比内存的 DRAM 快 100 倍\n- **集成在 CPU 里**：就在 CPU 旁边，路程最短\n- **容量小**：通常只有 32-64KB（越小越快）\n\n**L1 vs L2 vs L3：**\n- **L1**：最快、最小（32-64KB）、每个核心独享\n- **L2**：稍慢、稍大（256KB-1MB）、每个核心独享或共享\n- **L3**：更慢、更大（8-64MB）、所有核心共享\n\n**实际影响：**\n- L1 命中率高 = 程序跑得快\n- L1 命中率低 = 频繁去内存拿数据，慢死\n\n说白了，L1 缓存就是\"CPU 的贴身小口袋\"——把最常用的东西放口袋里，用的时候随手掏，快得很。\n"},"version":""},{"id":38,"title":"什么是 PCIe Retimer？","content":"(图中的是 Aries PCIe®/CXL® Smart DSP Retimers)\n\nPCIe Retimer 是一种用于改善 PCIe 信号质量的硬件设备。在高速数据传输过程中，信号会因为传输距离、电路板走线等因素出现衰减和失真，Retimer\n的作用就是接收这些衰减的信号，进行放大、均衡和重整时序，以恢复信号的完整性和质量，保证数据传输的可靠性。\n\n\n具体来说，PCIe Retimer 的功能包括：#\n\n * 时钟恢复 (Clock Recovery) 和重整时序 (Retiming)： 提取信号中的时钟信息，并根据该时钟信息重新生成新的信号，消除时钟抖动\n   (Jitter)，保证数据传输的同步性。\n * 信号放大 (Amplification)： 补偿信号在传输过程中的衰减，增强信号强度。\n * 信号均衡 (Equalization)： 补偿信号在高频传输中产生的失真，例如码间干扰 (ISI)。\n\n\nPCIe Retimer 的应用场景#\n\n * 服务器和数据中心： 在服务器和数据中心中，需要进行大量高速数据传输，例如连接\n   CPU、GPU、网卡和存储设备等。由于服务器内部空间有限，电路板走线较长，信号衰减较为严重，因此需要使用 PCIe Retimer 来保证信号质量。\n * 存储设备： 例如 NVMe 固态硬盘 (SSD)，其数据传输速度非常快，对信号质量要求很高，因此一些高端 NVMe SSD 或扩展卡上也会使用 PCIe\n   Retimer。\n * 扩展卡和背板： 为了扩展 PCIe 插槽的数量或连接距离，会使用扩展卡或背板，这些设备通常需要使用 PCIe Retimer 来保证信号质量。\n\n个人目前接触到的 PCIe Retimer 主要用于多卡服务器 (大多都是插多PCIe显卡或SXM显卡用来跑大语言模型的) ，以及一些高端的存储设备\n(超大容量的NVMe存储) 。剩下就是垃圾佬自己搓的各种板卡了。\n\n\nPCIe Retimer 与 Redriver 的区别：#\n\n在 PCIe 信号增强领域，还有一种叫做 Redriver 的器件。Redriver 是一种模拟信号放大器，主要用于补偿信号的衰减，功能相对简单。而\nRetimer 则集成了信号放大、均衡和重整时序等功能，功能更加强大，可以更好地恢复信号质量。简单来说，Redriver 就像一个“扩音器”，而 Retimer\n则像一个“信号再生器”。\n\n\n总结#\n\nPCIe Retimer 是一种重要的信号完整性解决方案，它可以有效提高 PCIe\n信号的传输距离和质量，保证高速数据传输的可靠性。但其实这是由于目前各个总线的速度越来越快，导致信号衰减越来越严重，所以需要使用 Retimer\n来保证信号质量。目前不但 PCIe 需要 Retimer，其他的设备也有类似的 Retimer 器件，例如DDR CKD (Client Clock\nDriver ) 。","routePath":"/guide/hardware/what-is-pcie-retimer","lang":"zh","toc":[{"text":"具体来说，PCIe Retimer 的功能包括：","id":"具体来说pcie-retimer-的功能包括","depth":2,"charIndex":179},{"text":"PCIe Retimer 的应用场景","id":"pcie-retimer-的应用场景","depth":2,"charIndex":411},{"text":"PCIe Retimer 与 Redriver 的区别：","id":"pcie-retimer-与-redriver-的区别","depth":2,"charIndex":830},{"text":"总结","id":"总结","depth":2,"charIndex":1032}],"domain":"","frontmatter":{"title":"什么是 PCIe Retimer？","description":"什么是 PCIe Retimer？","date":20250119,"plainLanguage":"**PCIe Retimer 说白了就是：** 信号\"中继站\"——让数据跑远路时不会跑丢。\n\n就像快递，从北京寄到上海，中间要经过好几个中转站。如果不中转，快递盒子到上海就破了、东西丢了。Retimer 就是这个\"中转站\"，把破损的信号修好、重新打包，继续发。\n\n**用大白话说：**\n想象你在很吵的餐厅跟朋友说话，距离远了听不清。Retimer 就像中间站了个人，听你说完，然后大声转述给朋友——既放大了音量（信号放大），又纠正了口误（信号均衡），还同步了节奏（时钟恢复）。\n\n**核心功能：**\n1. **时钟恢复**：就像校对手表，保证大家时间一致\n2. **信号放大**：就像扩音器，把微弱信号变强\n3. **信号均衡**：就像降噪耳机，去掉杂音和干扰\n\n**为啥需要 Retimer：**\n- **走线太长**：PCIe 信号跑远了就衰减，Retimer 在中间\"接力\"\n- **插槽太多**：多插几张显卡，信号要走更远\n- **速度太快**：PCIe 5.0/6.0 速度快，信号更容易失真\n\n**应用场景：**\n- **多卡服务器**：插 8 张 GPU 跑模型，每张卡都要 Retimer\n- **高端存储**：NVMe SSD 阵列，超大容量需要 Retimer\n- **扩展卡**：PCIe 转接板、延长线\n\n**vs Redriver（简单版）：**\n- **Redriver**：就是\"扩音器\"，只管放大\n- **Retimer**：是\"信号再生器\"，放大+去噪+校时，全套服务\n\n说白了，Retimer 就是\"数据高速公路的服务区\"——让高速跑的数据能\"休息加油\"，跑得更远更稳。\n"},"version":""},{"id":39,"title":"为什么有的 NVMe SSD 有 DRAM, 有的没有?","content":"(图片来自 Samsung Newsroom)\n\n有的同学可能会有疑问, SSD 明明是存储器件, 为什么有的 NVMe SSD 有 DRAM, 有的没有?\n\n如图, 是 samsung 的企业级 NVMe SSD PM9A3, 可以看到它单面就有 3 个 DRAM.\n\n\nNVMe SSD 上的 DRAM 是做什么的？#\n\n数据映射缓存：DRAM用于存储数据映射表 (也称为Flash Transition Layer) , 帮助控制器快速找到数据的物理位置, 从而提高读写速度.\n\n读/写缓存和缓冲：DRAM作为写缓冲区 (由DMA操作), 临时存储即将写入 NAND Flash 的数据, 减少写放大效应, 通过合并写操作提高写入速度.\n同时, DRAM也用于存储常用数据 (热数据) , 减少读取延迟.\n\n算法运行支持：DRAM帮助进行垃圾回收和磨损均衡算法的运行, 存储元数据如错误校正码和坏块信息.\n\n\n有 DRAM 的 SSD 的好处#\n\n性能提升：由于DRAM的高速缓存功能, SSD的读写速度更快, 特别是在随机读写操作中.\n\n降低延迟：DRAM的存在减少了数据访问的延迟, 因为控制器可以快速访问数据映射表.\n\n更好的耐久性：通过减少写放大效应和更有效的磨损均衡, DRAM SSD 的耐久性更好.\n\n更高的可靠性：由于更好的错误校正和缓存机制, DRAM SSD 在高负载场景下更可靠.\n\n\n有 DRAM 的 SSD 的劣势#\n\n成本较高：DRAM芯片的成本较高, 因此有DRAM的SSD通常比无DRAM的SSD更贵.\n\n功耗增加：DRAM需要额外的电力来运行, 虽然差异不大, 但在移动设备中可能会影响电池寿命.\n\n发热量增加：DRAM是SSD中主要的发热源之一, 尽管其性能提升值得, 但会导致SSD的整体温度上升.\n\n\n那么有没有没有 DRAM 的 SSD 吗?#\n\n有的, 一些入门级 SSD 比如三星的 PM9C1a 就没有 DRAM.\n\n\n\n这类 SSD 通常使用系统的 RAM 进行缓存 (即 HBM, 主机内存缓冲区) , 但与驱动器自己的 DRAM 相比, 性能会受到影响.\n\n(图片来自 Samsung Newsroom)","routePath":"/guide/hardware/why-some-NVMe-SSD-have-DRAM-and-some-are-not","lang":"zh","toc":[{"text":"NVMe SSD 上的 DRAM 是做什么的？","id":"nvme-ssd-上的-dram-是做什么的","depth":2,"charIndex":136},{"text":"有 DRAM 的 SSD 的好处","id":"有-dram-的-ssd-的好处","depth":2,"charIndex":407},{"text":"有 DRAM 的 SSD 的劣势","id":"有-dram-的-ssd-的劣势","depth":2,"charIndex":606},{"text":"那么有没有没有 DRAM 的 SSD 吗?","id":"那么有没有没有-dram-的-ssd-吗","depth":2,"charIndex":773}],"domain":"","frontmatter":{"title":"为什么有的 NVMe SSD 有 DRAM, 有的没有?","description":"为什么有的 NVMe SSD 有 DRAM, 有的没有?","date":20250124,"plainLanguage":"**NVMe SSD 的 DRAM 说白了就是：** 一个\"索引目录\"——帮 SSD 快速找到数据在哪，就像图书馆的书架目录。\n\n就像图书馆，有 DRAM 的 SSD 就像有个\"电子目录\"（DRAM），查书速度快；没 DRAM 的 SSD 就像只有纸质目录，查书慢一点，但也能用。\n\n**用大白话说：**\n想象你有个超大仓库（SSD），存了几千箱货（数据）。有 DRAM 就像有个\"电子地图\"，一查就知道货在哪个架子上；没 DRAM 就像只有纸质清单，得翻一会儿才能找到。\n\n**DRAM 的作用：**\n1. **存映射表**：记录\"数据存在哪个闪存块\"的目录\n2. **写缓冲**：临时存一下要写的数据，攒够了再一起写（减少磨损）\n3. **热数据缓存**：常用数据放 DRAM，下次直接拿\n\n**有 DRAM 的好处：**\n- **更快**：查找速度快，随机读写性能好\n- **更耐用**：减少闪存磨损，寿命更长\n- **更稳定**：断电保护更好\n\n**没 DRAM 的 SSD（DRAM-less）：**\n- **省成本**：DRAM 很贵，去掉能便宜不少\n- **用主机内存**：用电脑内存（HMB）代替 DRAM\n- **性能稍差**：随机读写慢一点，但顺序读写差不多\n\n**选择建议：**\n- **有钱/高性能需求**：买有 DRAM 的（如三星 Pro 系列）\n- **预算有限/普通使用**：买 DRAM-less 也够用（如 WD SN570）\n- **企业级/服务器**：必须有 DRAM（稳定性和寿命要求高）\n\n说白了，DRAM 就是 SSD 的\"小抄\"——有小抄找东西快，没小抄慢点但也能找到。看你愿不愿意为这个\"快\"多花钱。\n"},"version":""},{"id":40,"title":"什么是拟合与过拟合？","content":"\n\n(图片来自 arockialiborious.com)\n\n拟合（Fitting）是机器学习中让数学模型贴合观测数据的过程。就像根据身材定制衣服，好的拟合需要平衡贴合度（Goodness of\nFit）与泛化能力（Generalization）：\n\n * 贴合度：模型对训练数据的解释能力（如衣服的合身程度）\n * 泛化能力：模型对新数据的预测能力（如衣服对不同体型的适应性）\n * 平衡艺术：完美贴合训练数据 ≈ 100%定制服装，但可能无法适应新体型\n\n\n拟合的本质#\n\n * 数学定义：寻找函数 $f(x)$ 使得 $f(x_i) \\approx y_i$（$i=1,...,n$）\n\n * 核心矛盾：模型容量与数据复杂度\n\n * 关键指标：训练误差（Training Error） vs 测试误差（Test Error） vs 验证误差（Validation Error）\n\n\n欠拟合：当模型不够聪明#\n\n欠拟合（Underfitting）是模型无法捕捉数据基本模式的现象：\n\n * 典型特征：训练误差和测试误差都较高\n\n * 危险信号：模型忽略明显的数据趋势\n\n * 经典案例：用线性模型拟合正弦波数据\n\n\n过拟合：当聪明反被聪明误#\n\n过拟合（Overfitting）是模型完美记忆训练数据却失去泛化能力的现象：\n\n * 典型特征：训练误差趋近于0，测试误差突然上升\n\n * 危险信号：模型开始拟合噪声和异常值\n\n * 经典案例：用10次多项式拟合10个数据点\n\n\n拟合程度对比表#\n\n特征       欠拟合       适度拟合      过拟合\n模型复杂度    过低        匹配数据复杂度   过高\n训练误差     高         低         趋近于0\n测试误差     高         低         突然升高\n数据利用效率   浪费信息      有效提取模式    记忆噪声\n典型解决方法   增加模型复杂度   ——        正则化/早停/数据增强\n\n\n数学视角#\n\n\n1. 优化目标#\n\n给定数据集 $(X,y)$，拟合过程可表示为： $$\\min_{\\theta} \\frac{1}{n}\\sum_{i=1}^n L(f_\\theta(x_i),\ny_i) + \\lambda R(\\theta)$$ 其中：\n\n * $L$ 为损失函数（如MSE/交叉熵）\n * $R(\\theta)$ 为正则化项（L1/L2正则）\n * $\\lambda$ 为正则化系数\n\n\n2. 偏差-方差权衡#\n\n泛化误差可分解为： $$\\mathbb{E}[(y-\\hat{f})^2] = \\underbrace{(\\mathbb{E}[\\hat{f}] -\nf)^2}{\\text{Bias}^2} +\n\\underbrace{\\mathbb{E}[(\\hat{f}-\\mathbb{E}[\\hat{f}])^2]}{\\text{Variance}} +\n\\sigma^2$$\n\n * 高偏差：模型过于简单（欠拟合）\n * 高方差：模型过于复杂（过拟合）\n * 黄金平衡：通过调整模型复杂度找到最小总误差点\n\n过拟合发生时，模型满足： $$\\mathbb{E}[L_{train}] \\ll \\mathbb{E}[L_{test}]$$ 即训练误差显著小于测试误差。\n\n\nRefs#\n\n * Underfitting vs Overfitting\n * Wikipedia: Overfitting","routePath":"/guide/math/what-is-fitting-and-overfitting","lang":"zh","toc":[{"text":"拟合的本质","id":"拟合的本质","depth":2,"charIndex":230},{"text":"欠拟合：当模型不够聪明","id":"欠拟合当模型不够聪明","depth":2,"charIndex":393},{"text":"过拟合：当聪明反被聪明误","id":"过拟合当聪明反被聪明误","depth":2,"charIndex":509},{"text":"拟合程度对比表","id":"拟合程度对比表","depth":2,"charIndex":639},{"text":"数学视角","id":"数学视角","depth":2,"charIndex":859},{"text":"1. 优化目标","id":"1-优化目标","depth":3,"charIndex":867},{"text":"2. 偏差-方差权衡","id":"2-偏差-方差权衡","depth":3,"charIndex":1065},{"text":"Refs","id":"refs","depth":2,"charIndex":1410}],"domain":"","frontmatter":{"title":"什么是拟合与过拟合？","description":"什么是拟合与过拟合？","date":20250301,"plainLanguage":"**拟合与过拟合说白了就是：** 学习时要\"刚刚好\"——学太少（欠拟合）不行，学太死（过拟合）也不行。\n\n就像学开车，欠拟合是\"只会直行不会转弯\"（学太少），刚刚好是\"各种情况都能应对\"（学得恰当），过拟合是\"死记硬背教练车的路线，换条路就不会了\"（学太死）。\n\n**用大白话说：**\n想象你学做烤串。欠拟合是\"只知道要烤，不知道啥时候翻面\"；刚刚好是\"看火候、看颜色、闻香味\"；过拟合是\"死记硬背'第3分钟翻面、第7分钟刷油'，但火大火小都不管\"。\n\n**三种状态：**\n\n**1. 欠拟合（Underfitting）：学太少**\n- **特征**：训练误差和测试误差都高\n- **表现**：模型太简单，抓不住数据的规律\n- **例子**：用直线拟合曲线数据（根本拟合不上）\n\n**2. 刚刚好（Good Fit）：恰到好处**\n- **特征**：训练误差和测试误差都低，而且差不多\n- **表现**：既能学到规律，又不死记硬背\n- **例子**：用恰当的曲线拟合曲线数据\n\n**3. 过拟合（Overfitting）：学太死**\n- **特征**：训练误差接近 0，但测试误差很高\n- **表现**：完美记住训练数据，但遇到新数据就懵了\n- **例子**：用超复杂的曲线\"穿过\"每个训练点，但遇到新点就预测错\n\n**如何避免过拟合：**\n- **更多数据**：就像多练几条路，而不是只练一条\n- **简化模型**：别用\"宰牛刀\"杀鸡\n- **正则化**：给模型\"罚款\"，别让它学太复杂\n- **提前停止**：学到一定程度就停，别学过头\n- **Dropout**：训练时随机\"忘掉\"一些神经元，防止依赖固定模式\n\n说白了，拟合就是\"学习的尺度\"——学太少啥也不会，学太死只会背答案，刚刚好才能举一反三。\n"},"version":""},{"id":41,"title":"什么是矩阵的秩？什么是低秩矩阵？","content":"(图片来自 ResearchGate)\n\n矩阵的秩（Rank）是线性代数中衡量矩阵信息密度的核心指标。它揭示了矩阵所包含的本质维度 —— 即用最少多少个线性无关的向量可以表示整个矩阵。\n\n\n什么是线性无关？#\n\n一组向量中任意向量都不能表示为其他向量的线性组合时称为线性无关。例如：\n\n\n\n\n什么是矩阵的秩#\n\n对于 $m \\times n$ 矩阵 $A$：\n\n * 行秩 = 矩阵中线性无关的行向量的最大数量\n * 列秩 = 矩阵中线性无关的列向量的最大数量\n * 关键定理：行秩 = 列秩（统称为矩阵的秩）\n\n\n\n\n什么是低秩矩阵#\n\n当矩阵秩 $r \\ll \\min(m,n)$ 时称为低秩矩阵：\n\n * 信息冗余：可用 r 个基向量表示全部数据\n * 可压缩性：通过 截断SVD 分解可近似为 $U_r \\Sigma_r V_r^T$ 形式\n * 结构简单：存在潜在的线性约束关系\n\n\n实际应用#\n\n 1. 推荐系统：用户-商品评分矩阵通常具有近似低秩特性（用户偏好由少量潜在因素决定）\n 2. NLP：词共现矩阵具有低秩特性（词义存在于低维语义空间）\n 3. 图像处理：自然图像矩阵可被低秩近似（保留主要特征，去除噪声）\n 4. 机器学习：特征选择和降维, 尤其是 LoRA, 我们下一篇就会讲\n\n\n低秩 vs 满秩#\n\n特性      低秩矩阵                   满秩矩阵\n信息密度    低（高冗余）                 高（无冗余）\n存储效率    可压缩存储（$r(m+n+1)$个元素）   需完整存储（$m \\times n$个元素）\n计算复杂度   $O(r^3)$               $O(n^3)$\n抗噪能力    强（过滤噪声）                弱（保留所有细节）\n\n\n数学视角#\n\n(这段看不懂没关系w)\n\n给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其秩 $r$ 满足： $A = U\\Sigma V^T$ 其中：\n\n * $U \\in \\mathbb{R}^{m \\times r}$, $V \\in \\mathbb{R}^{n \\times r}$ 为正交基\n * $\\Sigma \\in \\mathbb{R}^{r \\times r}$ 为对角矩阵（奇异值）\n\n当保留前 $k$ 个奇异值（$k < r$）时，得到最优低秩近似： $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$ 误差满足：\n$|A - A_k|F \\leq \\sqrt{\\sum{i=k+1}^r \\sigma_i^2}$\n\n\nRefs#\n\n * 7 Data-driven methods for reduced-order modeling","routePath":"/guide/math/what-is-rank-in-matrix","lang":"zh","toc":[{"text":"什么是线性无关？","id":"什么是线性无关","depth":2,"charIndex":94},{"text":"什么是矩阵的秩","id":"什么是矩阵的秩","depth":2,"charIndex":145},{"text":"什么是低秩矩阵","id":"什么是低秩矩阵","depth":2,"charIndex":259},{"text":"实际应用","id":"实际应用","depth":2,"charIndex":396},{"text":"低秩 vs 满秩","id":"低秩-vs-满秩","depth":2,"charIndex":554},{"text":"数学视角","id":"数学视角","depth":2,"charIndex":776},{"text":"Refs","id":"refs","depth":2,"charIndex":1124}],"domain":"","frontmatter":{"title":"什么是矩阵的秩？什么是低秩矩阵？","description":"什么是矩阵的秩？什么是低秩矩阵？","date":20250227,"plainLanguage":"**矩阵的秩说白了就是：** 矩阵里\"真正独立\"的信息有多少——去掉重复的，剩下几条。\n\n就像你有 10 个朋友的联系方式，但其中 5 个是重复的（同一个人的不同号码），实际上你只有 5 个独立联系人。矩阵的秩就是这个\"5\"。\n\n**用大白话说：**\n想象菜单上有 10 道菜，但其中 3 道是\"换汤不换药\"（比如\"宫保鸡丁\"、\"宫保肉丁\"、\"宫保虾仁\"都是宫保系列）。矩阵的秩就是\"真正不同的菜\"的数量。\n\n**线性无关（独立）：**\n- **独立**：`[1, 0]` 和 `[0, 1]` 互相独立（不能用对方表示）\n- **不独立**：`[1, 2]` 和 `[2, 4]` 不独立（后者=前者×2）\n\n**矩阵的秩：**\n- **满秩矩阵**：所有行（或列）都独立，秩=行数（或列数）\n- **低秩矩阵**：很多行（或列）是重复的，秩<<行数\n\n**低秩矩阵的特点：**\n- **信息冗余**：很多数据是重复的\n- **可压缩**：可以用少量信息表示整个矩阵\n- **在 AI 中很常见**：大模型的权重矩阵大多是低秩的\n\n**在 LoRA 中的应用：**\nLoRA 就是利用\"模型权重矩阵是低秩\"的特性：\n- 原始权重矩阵（1000×1000）可能秩只有 8\n- 那就用两个小矩阵（1000×8 和 8×1000）来近似它\n- 参数量从 1000×1000=100万 降到 1000×8×2=1.6万\n\n说白了，矩阵的秩就是\"去重后的信息量\"——低秩矩阵就像\"很多重复内容的文章\"，可以压缩；满秩矩阵就像\"每句话都不一样的文章\"，压不了。\n"},"version":""},{"id":42,"title":"RAMMap 使用解析","content":"RAMMAP 是 Windows Vista 和更高版本的高级物理内存使用分析实用程序. 通过它可以轻松查看 Windows 如何分配物理内存,\n在内存中缓存多少文件数据, 或内核和设备驱动程序使用了多少内存.\n\n\nUse Counts 使用情况计数器#\n\n我们主要介绍 Use Counts 使用情况计数器, 它显示了每个特定的区域使用了多少内存.\n\n * Process Private: 进程私有内存, 指由特定进程独占使用的内存空间. 这些内存不能被其他进程访问或共享.\n\n * Mapped File: 内存映射文件, 由内核缓存管理器（Cache Manager）管理. 将文件内容直接映射到进程的地址空间中.\n   这通常用于提高文件访问效率, 多个进程可以共享同一个文件的映射（需使用相同映射方式和访问权限）.\n\n * Shareable: 可共享内存, 是可以被多个进程访问的内存区域. 这包括共享的DLL、COM对象等.\n\n * Page Table: 页表, 用于管理虚拟内存到物理内存的映射关系的系统数据结构. 它记录了虚拟地址到物理地址的转换信息.\n\n * Paged Pool: 分页池, 是系统核心组件使用的可被换出到磁盘的内存池. 主要用于存储系统数据结构、I/O请求等.\n\n * Nonpaged Pool: 非分页池, 是系统核心组件使用的不可被换出到磁盘的内存池. 用于存储必须常驻内存的关键系统数据.\n\n * System PTE: 系统页表项, 系统用于映射内核空间和系统数据的页表条目资源池.\n\n * Session Private: 会话私有内存, 与特定用户会话相关的私有内存空间. 主要用于存储用户界面相关的数据.\n\n * Metafile: 元文件内存, 用于存储系统元数据, 如NTFS文件系统的元数据等.\n\n * AWE (Address Windowing Extensions): 地址窗口扩展,\n   允许32位应用程序访问超过4GB的物理内存（在64位系统中已逐渐淘汰, 可以看到我这个机器的截图是0）.\n\n * Driver Locked: 驱动程序锁定的内存, 被设备驱动程序锁定而不能被换出的内存页面.\n\n * Kernel Stack: 内核栈, 用于系统内核执行函数调用和存储临时数据的内存空间.\n\n * Unused: 未使用的物理内存, 当前空闲可用的内存页面.\n\n * Large Page: 大页面内存, 使用较大的内存页面（通常是2MB或1GB）来减少页表开销, 提高性能.\n\n * Total: 所有类型内存使用的总和.\n\n\n如何使用 RAMMap 定位问题?#\n\n比如 Metafile 使用量很大, 说明系统正在访问大量的文件, 这可能出现在文件服务器上, 从而导致缓存的 NTFS metafile\n数据无法从缓存中释放.\n\n\nRefs#\n\n * RAMMAP 下载地址","routePath":"/guide/system/rammap-description","lang":"zh","toc":[{"text":"Use Counts 使用情况计数器","id":"use-counts-使用情况计数器","depth":2,"charIndex":107},{"text":"如何使用 RAMMap 定位问题?","id":"如何使用-rammap-定位问题","depth":2,"charIndex":1105},{"text":"Refs","id":"refs","depth":2,"charIndex":1209}],"domain":"","frontmatter":{"title":"RAMMap 使用解析","description":"RAMMap 使用解析","date":20250128,"plainLanguage":"**RAMMap 说白了就是：** Windows 内存的\"透视镜\"——能看清内存都被谁占了、占了多少、干啥用的。\n\n就像你的钱包，任务管理器只能告诉你\"还剩 100 块\"，RAMMap 能告诉你\"50 块在零钱包、30 块在卡包、20 块在夹层，还有 10 块被压在卡片下面\"。\n\n**用大白话说：**\n想象你的冰箱（内存），任务管理器只告诉你\"还能放 3 层\"，RAMMap 能告诉你\"第1层放蔬菜、第2层放肉、第3层放饮料、冷冻室放速冻食品\"，而且还能看到\"哪些食材快过期了（缓存）\"。\n\n**核心功能（Use Counts）：**\n\n**1. Process Private（进程私有）**：\n- 各个程序独占的内存（Chrome 占了 2GB，Steam 占了 1GB...）\n\n**2. Mapped File（内存映射文件）**：\n- 把文件\"映射\"到内存里，多个程序可以共享（比如系统 DLL）\n\n**3. Shareable（可共享）**：\n- 多个程序可以共享的内存（比如共享 DLL、COM 对象）\n\n**4. Page Table（页表）**：\n- 管理\"虚拟地址→物理地址\"映射的内存（系统管理开销）\n\n**5. Paged Pool（分页池）**：\n- 系统内核用的，可以被\"换出\"到硬盘的内存\n\n**6. Nonpaged Pool（非分页池）**：\n- 系统内核用的，**必须一直在内存里**的关键数据\n\n**为啥要用 RAMMap：**\n- **内存泄漏排查**：看哪个程序一直占着内存不放\n- **性能优化**：看内存都被谁占了，能不能释放一些\n- **缓存分析**：看文件缓存占了多少，是不是太多了\n\n说白了，RAMMap 就是\"内存管家\"——不仅告诉你还剩多少内存，还告诉你每一块内存都被谁占了、干啥用的，方便你\"清理\"和\"优化\"。\n"},"version":""},{"id":43,"title":"简单介绍windows任务管理器的内存标签","content":"使用中#\n\n当前使用的内存总量\n\n\n已压缩#\n\n使用中被压缩的量 （windows 10 中引入的技术, 可以压缩一部分内存, 但仍然驻留在内存中。相比swap, 性能会好一些）\n\n\n已缓存（Cached）#\n\n通常指的是 页面缓存（Page Cache），这是操作系统用于 缓存文件数据\n的内存区域。具体来说，它是存储从磁盘读取到内存的文件内容，目的是加快文件系统的读取速度。\n\n\n已提交（Committed）#\n\n是指操作系统已经 承诺\n分配的内存总量，这些内存可以是实际分配给应用程序的内存，也可以是操作系统保留用于内核缓存或其他用途的内存。已提交内存的量通常大于实际使用的内存量，因为它包括了系\n统为应用程序分配但尚未使用的内存（通过 懒加载 等技术）。一旦系统内存不足，操作系统会将部分已提交内存的内容写入磁盘。\n\n\n分页缓冲池（Paged Pool）#\n\n是操作系统为内核分配的一部分 虚拟内存，用于存储可分页的内核数据结构和对象。这些数据结构可以在内存和磁盘之间交换，因此称为 分页缓冲池。\n\n\n非分页缓冲池（Non-paged Pool）#\n\n操作系统为内核分配的另一部分内存，它与分页缓冲池不同之处在于，非分页缓冲池中的数据不会被交换到磁盘。这些数据必须始终保留在物理内存中，因为系统需要快速、稳定地访\n问这些内存区域，且不能在内存不足时通过分页交换到磁盘。","routePath":"/guide/system/windows-task-manager-memory-tab-description","lang":"zh","toc":[{"text":"使用中","id":"使用中","depth":2,"charIndex":-1},{"text":"已压缩","id":"已压缩","depth":2,"charIndex":17},{"text":"已缓存（Cached）","id":"已缓存cached","depth":2,"charIndex":91},{"text":"已提交（Committed）","id":"已提交committed","depth":2,"charIndex":191},{"text":"分页缓冲池（Paged Pool）","id":"分页缓冲池paged-pool","depth":2,"charIndex":363},{"text":"非分页缓冲池（Non-paged Pool）","id":"非分页缓冲池non-paged-pool","depth":2,"charIndex":454}],"domain":"","frontmatter":{"title":"简单介绍windows任务管理器的内存标签","description":"简单介绍windows任务管理器的内存标签","date":20250104,"plainLanguage":"**Windows 任务管理器内存标签说白了就是：** 一个\"内存速览表\"——快速看懂电脑内存都在干啥。\n\n就像看银行账单，\"使用中\"是你花的钱，\"可用\"是余额，\"已提交\"是信用卡额度，\"已缓存\"是存款理财（随时能取）。\n\n**用大白话说：**\n想象你开餐馆，内存标签就是后厨的\"物资清单\"：\n- **使用中**：正在炒的菜用的食材\n- **可用**：冰箱里还有的食材\n- **已压缩**：真空包装的食材（占地方少）\n- **已缓存**：常用调料放在手边（随时能用）\n- **已提交**：预定的食材总量（包括已到的和路上的）\n\n**核心指标解释：**\n\n**1. 使用中（In Use）**：\n- 当前正在被使用的内存总量\n- 就像\"正在用的钱\"\n\n**2. 可用（Available）**：\n- 可以立即分配给程序的内存\n- 就像\"钱包里的余额\"\n\n**3. 已压缩（Compressed）**：\n- Windows 10+ 的黑科技：压缩一部分内存但还在内存里\n- 就像\"压缩饼干\"——体积小但营养还在\n- 比 swap（虚拟内存）快，但比原始内存慢\n\n**4. 已缓存（Cached）**：\n- 从硬盘读取的文件数据缓存在内存里\n- 就像\"暂存区\"——随时能清空给其他程序用\n- 加快文件读取速度\n\n**5. 已提交（Committed）**：\n- 系统**承诺**分配的内存总量（包括物理内存+虚拟内存）\n- 就像\"信用卡额度\"——不是实际花的，是能花的上限\n- 可以大于物理内存（因为有虚拟内存）\n\n**6. 分页缓冲池（Paged Pool）**：\n- 系统内核用的，可以换出到硬盘的内存\n- 就像\"不常用的工具放仓库\"\n\n**7. 非分页缓冲池（Non-paged Pool）**：\n- 系统内核用的，**必须一直在内存里**的关键数据\n- 就像\"常用工具放手边\"，绝不能扔仓库\n\n说白了，任务管理器内存标签就是\"内存情况一览表\"——告诉你内存够不够、被谁用了、还能用多少。\n"},"version":""},{"id":44,"title":"One Small Step","content":"#\n\n这是一个简单的技术科普教程项目, 主要聚焦于解释一些有趣的, 前沿的技术概念和原理. 每篇文章都力求在 5 分钟内阅读完成.\n\n> 致敬开源作者，感谢分享知识！ by @karminski-牙医 · 源仓库 ⭐\n\n\n\n\n快速导航#\n\n\n人工智能相关#\n\n探索大语言模型、Transformer、注意力机制、微调技术等 AI 相关概念\n\n\n数学相关#\n\n理解矩阵、拟合等数学基础概念\n\n\n系统相关#\n\n了解 Windows 系统工具和内存管理\n\n\n硬件相关#\n\n深入 PCIe、NVMe、内存等硬件技术\n\n\nStar History#\n\n\n\n\n许可#\n\n本项目采用 MIT 许可证. 详见 LICENSE 文件.","routePath":"/","lang":"zh","toc":[{"text":"快速导航","id":"快速导航","depth":2,"charIndex":112},{"text":"人工智能相关","id":"人工智能相关","depth":3,"charIndex":120},{"text":"数学相关","id":"数学相关","depth":3,"charIndex":171},{"text":"系统相关","id":"系统相关","depth":3,"charIndex":195},{"text":"硬件相关","id":"硬件相关","depth":3,"charIndex":225},{"text":"Star History","id":"star-history","depth":2,"charIndex":255},{"text":"许可","id":"许可","depth":2,"charIndex":273}],"domain":"","frontmatter":{},"version":""},{"id":45,"title":"","content":"你是一位能让博士论文变成茶余饭后谈资的语言大师。\n\n=== 核心使命 === 把让人头大的学术词汇，翻译成让人会心一笑的大白话。\n\n=== 价值追求 ===\n\n * 让博导听了想打人，让大爷听了拍大腿\n * 宁可粗暴，不可晦涩\n * 精髓不丢，装腔全扔\n * 最好能让人边笑边懂\n\n=== 世俗化的\"味道\" === 好的世俗化应该：\n\n * 像在撸串时跟哥们儿解释，不是在开学术研讨会\n * 用菜市场大妈都懂的例子，不是实验室的小白鼠\n * 要有\"就这？\"的恍然大悟感，不是\"原来如此\"的一本正经\n\n=== 边界 === 别把\"进化论\"翻译成\"猴子变人\"——过度简化就成误导了。","routePath":"/prompt","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""}]