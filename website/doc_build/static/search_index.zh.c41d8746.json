[{"id":0,"title":"","content":"","routePath":"/guide/ai/How-to-avoid-KVCache-invalidation","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":1,"title":"","content":"","routePath":"/guide/ai/LLM-fine-tuning-summary","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":2,"title":"","content":"","routePath":"/guide/ai/Parameter-Precision-Formats-for-LLMs","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":3,"title":"","content":"","routePath":"/guide/ai/What-is-Dual-Chunk-Attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":4,"title":"","content":"","routePath":"/guide/ai/What-is-Recall","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":5,"title":"","content":"","routePath":"/guide/ai/When-to-Use-Fine-Tuning-and-When-Not-To","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":6,"title":"","content":"","routePath":"/guide/ai/how-are-tokens-calculated-in-LLMs","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":7,"title":"","content":"","routePath":"/guide/ai/how-to-optimize-transformer","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":8,"title":"","content":"","routePath":"/guide/ai/how-to-run-gguf-LLM-model","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":9,"title":"","content":"","routePath":"/guide/ai/what-is-AI-Agent","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":10,"title":"","content":"","routePath":"/guide/ai/what-is-AI-Hallucination","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":11,"title":"","content":"","routePath":"/guide/ai/what-is-LLM-distill","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":12,"title":"","content":"","routePath":"/guide/ai/what-is-LLM-fine-tuning","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":13,"title":"","content":"","routePath":"/guide/ai/what-is-LoRA","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":14,"title":"","content":"","routePath":"/guide/ai/what-is-MoE","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":15,"title":"","content":"","routePath":"/guide/ai/what-is-RAG","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":16,"title":"","content":"","routePath":"/guide/ai/what-is-flash-attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":17,"title":"","content":"","routePath":"/guide/ai/what-is-gguf","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":18,"title":"","content":"","routePath":"/guide/ai/what-is-gropued-query-attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":19,"title":"","content":"","routePath":"/guide/ai/what-is-llm-perplexity","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":20,"title":"","content":"","routePath":"/guide/ai/what-is-modal-encoding","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":21,"title":"","content":"","routePath":"/guide/ai/what-is-multi-head-attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":22,"title":"","content":"","routePath":"/guide/ai/what-is-multi-model-llm","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":23,"title":"","content":"","routePath":"/guide/ai/what-is-multi-query-attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":24,"title":"","content":"","routePath":"/guide/ai/what-is-onnx","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":25,"title":"","content":"","routePath":"/guide/ai/what-is-pythonic-function-call","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":26,"title":"","content":"","routePath":"/guide/ai/what-is-quantization-in-LLM","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":27,"title":"","content":"","routePath":"/guide/ai/what-is-representation-space","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":28,"title":"","content":"","routePath":"/guide/ai/what-is-safetensors","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":29,"title":"","content":"","routePath":"/guide/ai/what-is-sliding-window-attention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":30,"title":"","content":"","routePath":"/guide/ai/what-is-speculative-decoding","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":31,"title":"","content":"","routePath":"/guide/ai/what-is-transformer","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":32,"title":"","content":"","routePath":"/guide/ai/what-is-vector-database","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":33,"title":"","content":"","routePath":"/guide/ai/what-is-vector-embedding","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":34,"title":"","content":"","routePath":"/guide/ai/what-is-vibe-coding","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":35,"title":"","content":"","routePath":"/guide/hardware/does-CXL-will-be-LLM-memory-solution","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":36,"title":"","content":"","routePath":"/guide/hardware/what-is-1DPC","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":37,"title":"","content":"","routePath":"/guide/hardware/what-is-L1-cache","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":38,"title":"","content":"","routePath":"/guide/hardware/what-is-pcie-retimer","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":39,"title":"","content":"","routePath":"/guide/hardware/why-some-NVMe-SSD-have-DRAM-and-some-are-not","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":40,"title":"","content":"","routePath":"/guide/math/what-is-fitting-and-overfitting","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":41,"title":"","content":"","routePath":"/guide/math/what-is-rank-in-matrix","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":42,"title":"","content":"","routePath":"/guide/system/rammap-description","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":43,"title":"","content":"","routePath":"/guide/system/windows-task-manager-memory-tab-description","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""},{"id":44,"title":"One Small Step","content":"#\n\n这是一个简单的技术科普教程项目, 主要聚焦于解释一些有趣的, 前沿的技术概念和原理. 每篇文章都力求在 5 分钟内阅读完成.\n\n> 致敬开源作者，感谢分享知识！ by @karminski-牙医 · 源仓库 ⭐\n\n\n\n\n快速导航#\n\n\n人工智能相关#\n\n探索大语言模型、Transformer、注意力机制、微调技术等 AI 相关概念\n\n\n数学相关#\n\n理解矩阵、拟合等数学基础概念\n\n\n系统相关#\n\n了解 Windows 系统工具和内存管理\n\n\n硬件相关#\n\n深入 PCIe、NVMe、内存等硬件技术\n\n\nStar History#\n\n\n\n\n许可#\n\n本项目采用 MIT 许可证. 详见 LICENSE 文件.","routePath":"/","lang":"zh","toc":[{"text":"快速导航","id":"快速导航","depth":2,"charIndex":112},{"text":"人工智能相关","id":"人工智能相关","depth":3,"charIndex":120},{"text":"数学相关","id":"数学相关","depth":3,"charIndex":171},{"text":"系统相关","id":"系统相关","depth":3,"charIndex":195},{"text":"硬件相关","id":"硬件相关","depth":3,"charIndex":225},{"text":"Star History","id":"star-history","depth":2,"charIndex":255},{"text":"许可","id":"许可","depth":2,"charIndex":273}],"domain":"","frontmatter":{},"version":""},{"id":45,"title":"","content":"你是一位能让博士论文变成茶余饭后谈资的语言大师。\n\n=== 核心使命 === 把让人头大的学术词汇，翻译成让人会心一笑的大白话。\n\n=== 价值追求 ===\n\n * 让博导听了想打人，让大爷听了拍大腿\n * 宁可粗暴，不可晦涩\n * 精髓不丢，装腔全扔\n * 最好能让人边笑边懂\n\n=== 世俗化的\"味道\" === 好的世俗化应该：\n\n * 像在撸串时跟哥们儿解释，不是在开学术研讨会\n * 用菜市场大妈都懂的例子，不是实验室的小白鼠\n * 要有\"就这？\"的恍然大悟感，不是\"原来如此\"的一本正经\n\n=== 边界 === 别把\"进化论\"翻译成\"猴子变人\"——过度简化就成误导了。","routePath":"/prompt","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":""}]