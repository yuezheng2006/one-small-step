---
title: ## 大模型精度格式一览
description: ## 大模型精度格式一览

date: 20251110
plainLanguage: |
  **大模型精度格式说白了就是：** 用多少位数字存一个参数——位数越少越省空间，但精度会下降。

  就像照片质量，4K（FP32）最清晰但文件大，1080P（FP16）清晰度还行文件小一半，720P（INT8）更小但有点糊，480P（INT4）很糊但超小。精度格式就是在"清晰度"和"文件大小"之间选择。

  **用大白话说：**
  想象称肉，电子秤精确到 0.01 克（FP32），弹簧秤精确到 10 克（INT8）。电子秤准但贵又慢，弹簧秤快又便宜但不够准。大模型精度就是选"用什么秤"。

  **常见精度（从高到低）：**

  **训练常用：**
  - **FP32（32位）**：最准，但占空间大、慢（基准）
  - **BF16（16位）**：🔥 **目前最流行**的训练精度，省一半空间，效果还行
  - **FP8（8位）**：DeepSeek-V3 用的，超省资源，H100 专属

  **推理常用：**
  - **Q8（8位整数）**：质量接近原版，文件小 4 倍
  - **Q5（5位）**：🔥 **推荐**，质量和大小平衡得最好
  - **Q4（4位）**：🔥 **最常用**，文件小 8 倍，质量还能接受
  - **Q2（2位）**：超大模型（200B+）测试用，质量会明显下降

  **极限压缩：**
  - **1-bit（二值化）**：只有 -1 和 +1 两个值，超省但质量很差

  **核心权衡：**
  - 精度高 = 效果好但占空间大、慢
  - 精度低 = 省空间、快但效果差

  **选择建议：**
  - **训练**：BF16（主流）或 FP8（有 H100 的话）
  - **推理**：Q4_K_M 或 Q5_K_M（普通人首选）
  - **测试**：Q2_K_M（超大模型，想快速体验）

  说白了，精度格式就是"精度vs空间"的权衡——就像买车，豪车（高精度）舒服但贵，经济型车（低精度）便宜但体验稍差，看你的需求和预算。
---


## 大模型精度格式一览

| 精度格式            | 全称/别称                             | 位数                | 格式构成 (符号/指数/尾数)                  | 代表性模型                                         | 主要特点与应用                                                                                                                                                                   |
| :------------------ | :------------------------------------ | :------------------ | :----------------------------------------- | :------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **FP128**           | 四倍精度浮点 (Quadruple Precision)    | 128位               | 1 / 15 / 112                               | 科学计算模拟、高精度金融模型                       | 提供约34位十进制有效数字的极高精度，主要用于科学计算、金融建模和需要极高数值精度的研究领域。在AI领域应用较少，但对精度要求极高的场景会使用。                                     |
| **FP64**            | 双精度浮点 (Double Precision)         | 64位                | 1 / 11 / 52                                | AlphaFold（部分计算）、科学计算应用                | 提供约15位十进制有效数字的高精度，是科学计算的标准。在AI训练中较少使用（由于计算成本高），但在需要高精度梯度累积或数值稳定性的场景中会使用。                                     |
| **FP32**            | 单精度浮点 (Single Precision)         | 32位                | 1 / 8 / 23                                 | GPT-2、ResNet、早期BERT等传统模型                  | 动态范围和精度都很高，是传统深度学习训练的基准，兼容性最广。                                                                                                                     |
| **TF32**            | TensorFloat-32                        | 32位 (内部19位计算) | 1 / 8 / 10                                 | A100上训练的Llama、GPT-3等大模型                   | NVIDIA Ampere架构及更新GPU中Tensor Core的内部格式。拥有与FP32相同的动态范围和FP16的精度，通过截断FP32的尾数位实现加速，无需修改代码即可提升FP32训练速度。                        |
| **FP16**            | 半精度浮点 (Half Precision)           | 16位                | 1 / 5 / 10                                 | GPT-3（混合精度训练）、Stable Diffusion            | 相比FP32内存和计算减半。动态范围较小（约6e-5 到 65504），训练时可能出现上溢或下溢，通常需要结合动态损失缩放（Dynamic Loss Scaling）使用。                                        |
| **BF16**            | BFloat16 / 脑浮点数                   | 16位                | 1 / 8 / 7                                  | Llama 2/3、Qwen、Gemini                            | 动态范围与FP32相同，解决了FP16的溢出问题，但尾数精度较低。**🔥 目前Instruction大模型训练最常用的精度格式**，非常适合大规模模型的训练，是云端AI芯片的主流格式。                    |
| **INT16**           | 16位整型 (16-bit Integer)             | 16位                | `value = (int_value - zero_point) * scale` | 音频处理模型、部分混合精度训练场景                 | 提供比INT8更高的精度和更大的动态范围。内存占用为FP32的1/2，适用于需要比INT8更高精度的量化场景。在某些混合精度训练中用于梯度和权重的中间表示。                                    |
| **FP8 (E4M3)**      | 8位浮点                               | 8位                 | 1 / 4 / 3                                  | H100上训练的大模型、Transformer Engine             | **NVIDIA Hopper架构及更新GPU支持**。动态范围较小，但精度相对E5M2更高。适用于模型权重和激活值的表示，在Transformer模型的前向传播中表现出色。                                      |
| **FP8 (E5M2)**      | 8位浮点                               | 8位                 | 1 / 5 / 2                                  | DeepSeek-R1 (671B)、H100上训练的大模型（梯度计算） | **NVIDIA Hopper架构及更新GPU支持**。动态范围比E4M3更广，但精度较低。适用于反向传播中的梯度计算，能更好地处理数值较大的梯度值。                                                   |
| **MXFP8 (E5M2)**    | Microscaling FP8                      | 8位                 | 1 / 5 / 2 + 每32值共享的E8M0缩放           | OCP微缩放规范研究、实验性推理                      | 微缩放8位浮点格式，每32个E5M2值共享一个E8M0缩放因子（8位指数+0位尾数，表示2的幂次）。支持硬件加速，提供比标准FP8更大的动态范围（2^-127到2^128）。                                |
| **INT8**            | 8位整型 (8-bit Integer)               | 8位                 | `value = (int_value - zero_point) * scale` | MobileNet、MobileBERT、各类Llama量化推理版本       | 内存占用为FP32的1/4，计算速度极快，尤其在有INT8加速单元的硬件上。广泛用于模型推理量化，通过缩放因子和零点将浮点数映射到[-128, 127]或的整数范围。                                 |
| **FP4 (E2M1)**      | 4位浮点标准格式                       | 4位                 | 1 / 2 / 1 + 软件缩放因子                   | 实验性量化研究                                     | 基础的4位浮点格式，需要软件层面的缩放因子。相比FP16内存最多减少4倍，但与FP8相比存在明显的准确性下降风险。无硬件加速缩放支持。                                                    |
| **MXFP4**           | Microscaling FP4                      | 4位                 | 1 / 2 / 1 + 每32值共享的E8M0缩放           | GPT-OSS (120B/20B)、OCP微缩放规范研究              | 微缩放浮点格式，每32个E2M1值共享一个E8M0缩放因子（8位指数+0位尾数，表示2的幂次）。支持硬件加速缩放，相比FP16内存最多减少4倍。与FP8相比存在明显的准确性下降风险，但硬件效率更高。 |
| **NVFP4**           | NVIDIA FP4                            | 4位                 | 1 / 2 / 1 + 每16值共享的FP8缩放            | Blackwell架构上的超大规模LLM推理                   | **NVIDIA Blackwell架构引入**。每16个值共享一个FP8缩放因子，采用二级微块缩放策略。支持硬件加速缩放，相比FP16内存最多减少4倍。特别适用于大型LLM推理，准确性下降风险相对较低。      |
| **INT4**            | 4位整型 (4-bit Integer)               | 4位                 | `value = (int_value - zero_point) * scale` | Kimi k2 thinking、Llama-GPTQ/AWQ、ChatGLM-INT4     | 极高的压缩率（FP32的1/8）。通常用于推理，对模型性能有一定挑战，需要配合先进的量化算法（如GPTQ, AWQ）来降低精度损失。将浮点数映射到[-8, 7]或的整数范围。                          |
| **INT2**            | 2位整型 (2-bit Integer)               | 2位                 | `value = (int_value - zero_point) * scale` | 极端量化实验、研究原型                             | 极端压缩率（FP32的1/16）。将浮点数映射到4个离散值（如[-2, -1, 1, 2]）。主要用于极端量化研究，需要特殊的量化技术来维持可用的模型性能。目前仍处于实验阶段。                        |
| **1-bit (Binary)**  | 1位二值化 (e.g., BinaryNet, XNOR-Net) | 1位                 | 参数被量化为 {-1, +1}                      | BinaryNet、XNOR-Net、边缘设备轻量级模型            | 纯二值化网络，权重只有两个值。内存占用极小（FP32的1/32），使用XNOR和popcount操作替代乘法和加法，能效极高。主要用于边缘设备推理。准确性损失较大，需要特殊训练技术。               |
| **1-bit (Ternary)** | 1位/三元量化 (e.g., BitNet b1.58)     | ~1.58位             | 参数被量化为 {-1, 0, 1}                    | BitNet b1.58、Microsoft 1-bit LLM研究              | 极致的压缩和能效。通过将权重约束到三个值，可以用`log2(3) ≈ 1.58`位来存储。这使得乘法运算可以被替换为更高效的加法/减法，极大地降低了计算成本。目前属于前沿研究领域。              |

## NVIDIA 专属精度格式

NVIDIA在其GPU架构中引入了多个专属的精度格式，以优化AI训练和推理性能：

1. **TF32 (TensorFloat-32)** - Ampere架构（2020年）引入，无需代码修改即可加速FP32训练
2. **FP8 (E4M3 & E5M2)** - Hopper架构（2022年，H100）引入，首个硬件支持的8位浮点格式
3. **NVFP4** - Blackwell架构（2024年，B100/B200）引入，支持极致压缩的大模型推理

这些格式均在NVIDIA的Tensor Core中获得硬件加速支持，代表了业界在低精度AI计算方面的领先技术。

## 微缩放格式与E8M0缩放因子

**E8M0**（8位指数+0位尾数）是一种特殊的8位格式，专门用作微缩放（Microscaling, MX）格式的缩放因子，而非独立的数据格式：

- **结构**：8位指数，无尾数，无符号位
- **表示范围**：只能表示2的整数次幂，范围从2^-127到2^128
- **用途**：在MXFP8、MXFP4等微缩放格式中作为共享缩放因子，大幅扩展动态范围
- **优势**：通过块级共享缩放因子，在极低位宽下仍能保持足够的数值表示范围

这种设计使得微缩放格式能够在保持硬件效率的同时，提供比传统低精度格式更好的数值稳定性。
