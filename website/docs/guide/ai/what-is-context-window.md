---
title: 什么是上下文窗口（Context Window）
description: 什么是上下文窗口（Context Window）
date: 20250112
author: AI收集
plainLanguage: |
  **Context Window 说白了就是：** AI 的"工作台大小"——能一次性看多少字。

  就像你写作业，桌子小的话只能摊开一页纸（小上下文），桌子大的话能摊开一整本书（大上下文）。AI 的上下文窗口就是它的"桌子大小"。

  **用大白话说：**

  想象你跟朋友聊天：
  - **小上下文（4K）**：你只能记住最近 10 句话，再往前的就忘了
  - **大上下文（128K）**：你能记住整整 1 小时的对话，前前后后都记得清清楚楚

  AI 的上下文窗口就是它的"短期记忆容量"——一次对话里能记住多少内容。

  **具体例子：**

  假设你要 AI 总结一篇 5 万字的小说：
  - **上下文窗口 = 4K（约 3000 字）**：❌ 装不下，AI 说"太长了，我看不完"
  - **上下文窗口 = 32K（约 2.4 万字）**：❌ 还是不够
  - **上下文窗口 = 128K（约 9.6 万字）**：✅ 能装下，AI 一次性读完整本小说

  **计量单位：**
  - 单位是 **token**（不是字数！）
  - 中文：1 个字 ≈ 1-2 token
  - 英文：1 个单词 ≈ 1-2 token
  - 常见窗口大小：
    - 4K = 约 3000 汉字
    - 8K = 约 6000 汉字
    - 32K = 约 2.4 万汉字
    - 128K = 约 9.6 万汉字（一本中篇小说）
    - 1M = 约 75 万汉字（几本长篇小说）

  **实际应用：**

  **小上下文够用：**
  - 简单问答："今天天气怎么样？"
  - 写代码片段
  - 翻译短句

  **需要大上下文：**
  - 总结长文档
  - 分析整个代码库
  - 长时间对话（记住前面说过的话）
  - 处理长篇文章

  **为什么不直接做成无限大？**
  1. **贵**：上下文越大，计算成本越高（二次方增长）
  2. **慢**：处理时间更长
  3. **技术难**：长上下文很容易"记不住"远处的内容

  **现实例子：**
  - GPT-3.5：4K（约 3000 字）
  - GPT-4：8K/32K（分版本）
  - GPT-4 Turbo：128K（约 10 万字）
  - Claude 3.5 Sonnet：200K（约 15 万字）
  - Gemini 1.5 Pro：1M（约 75 万字，能看完几本书！）

  说白了，上下文窗口就是 AI 的"短期记忆容量"——越大越好，但也越贵。就像你买手机，内存越大越好用，但也越贵。
---

![context-window-comparison](/assets/images/context-window-comparison.png)

上下文窗口（Context Window）是大型语言模型（LLM）在处理单次请求时能够"看到"和"记住"的最大文本长度。

它决定了模型能够同时处理多少信息，是衡量 LLM 能力的重要指标之一。

## 核心概念

### 什么是上下文？

在 LLM 中，"上下文"包括：
- 用户的输入（Prompt）
- 模型的历史回复
- 系统提示词（System Prompt）
- 所有先前的对话记录

**示例：**
```
系统提示：你是一个有帮助的助手。             [50 tokens]
用户：请帮我总结这篇文章：[文章内容]          [2000 tokens]
助手：这篇文章主要讲述了...                  [300 tokens]
用户：能详细说说第三段吗？                   [10 tokens]
----------------------------------------
总上下文：50 + 2000 + 300 + 10 = 2360 tokens
```

如果上下文窗口 = 4K，还剩余 1640 token 用于模型回复。

### Token 与字符的关系

- **英文**：1 token ≈ 4 个字符 ≈ 0.75 个单词
- **中文**：1 token ≈ 1-2 个汉字
- **代码**：1 token ≈ 1-2 个字符（取决于编程语言）

**快速换算：**
| Token 数 | 英文单词数 | 中文字数 |
|----------|-----------|---------|
| 1K | ~750 | ~1000 |
| 4K | ~3000 | ~4000 |
| 8K | ~6000 | ~8000 |
| 32K | ~24,000 | ~32,000 |
| 128K | ~96,000 | ~128,000 |
| 1M | ~750,000 | ~1,000,000 |

## 上下文窗口的限制

### 1. 硬性限制

超过上下文窗口的内容会被截断：

```
上下文窗口 = 4K
输入 = 5K tokens

结果：
- 方式 1：截断最前面的内容（保留最新对话）
- 方式 2：返回错误："超过最大长度"
```

### 2. 注意力衰减

即使在窗口内，距离越远的内容，模型"记住"的能力越弱。

这被称为"**Lost in the Middle**"现象：
- 开头的内容：记得很清楚
- 中间的内容：容易忽略
- 结尾的内容：记得很清楚

**实验结果：**
```
文档1 (重要)   ← 记得清楚
文档2          ← 容易忽略
文档3          ← 容易忽略
文档4          ← 容易忽略
文档5 (重要)   ← 记得清楚

问题：文档2里说了什么？
模型：(答不上来或答错)
```

### 3. 计算成本

注意力机制的计算复杂度是 **O(n²)**：
- 上下文长度翻倍 → 计算量增加 4 倍
- 128K 的计算量是 4K 的 1024 倍

这导致：
- 响应时间更长
- 成本更高（API 按 token 计费）

## 主流模型的上下文窗口

### 2023 年之前（小上下文时代）

| 模型 | 上下文窗口 | 发布时间 |
|------|-----------|---------|
| GPT-3 | 2K-4K | 2020 |
| GPT-3.5 | 4K | 2022 |
| ChatGPT | 4K | 2022.11 |

### 2023-2024（中等上下文）

| 模型 | 上下文窗口 | 发布时间 |
|------|-----------|---------|
| GPT-4 | 8K/32K | 2023.03 |
| GPT-4 Turbo | 128K | 2023.11 |
| Claude 2 | 100K | 2023.07 |
| Claude 3 | 200K | 2024.03 |

### 2024-2025（超长上下文）

| 模型 | 上下文窗口 | 发布时间 |
|------|-----------|---------|
| Gemini 1.5 Pro | 1M (100万) | 2024.02 |
| Gemini 1.5 Flash | 1M | 2024.05 |
| Claude 3.5 Sonnet | 200K | 2024.06 |
| GPT-4o | 128K | 2024.05 |
| Qwen3 | 1M | 2025.01 |

**1M token 能装多少内容？**
- 约 75 万英文单词
- 约 100 万汉字
- 相当于：
  - 4-5 本长篇小说
  - 整个代码库（中小型项目）
  - 数十份 PDF 文档

## 长上下文的技术挑战

### 挑战 1：计算效率

**解决方案：**
- **Flash Attention**：优化注意力计算，降低内存占用
- **Sparse Attention**：只关注部分重要 token
- **Grouped Query Attention (GQA)**：减少 KV Cache 大小
- **Sliding Window Attention**：只关注局部窗口

### 挑战 2：位置编码外推

模型训练时的序列长度有限，如何外推到更长序列？

**解决方案：**
- **RoPE（旋转位置编码）**：天然支持外推
- **ALiBi**：线性偏置，无需位置编码
- **YaRN**：改进 RoPE 的外推能力
- **Dual-Chunk Attention**：Qwen3 用于支持 1M 上下文

### 挑战 3："Lost in the Middle"

模型容易忽略中间部分的内容。

**解决方案：**
- 改进训练数据（确保模型关注全文）
- 使用 Retrieval（检索重要片段，而非全文输入）
- 提示词工程（将重要信息放在开头或结尾）

## 实际应用场景

### 短上下文（4K-8K）适用场景

- ✅ 日常对话
- ✅ 简单问答
- ✅ 短文本翻译
- ✅ 代码片段生成

### 中等上下文（32K-128K）适用场景

- ✅ 文档总结（10-50 页）
- ✅ 代码审查（单个文件或小模块）
- ✅ 长对话历史
- ✅ 论文分析

### 超长上下文（1M）适用场景

- ✅ 整本书总结
- ✅ 整个代码库分析
- ✅ 长时间对话（数小时）
- ✅ 批量文档处理

## 如何优化上下文使用

### 1. 提示词压缩

```
❌ 冗长版（500 tokens）：
"请帮我分析一下这个代码，看看有没有问题，如果有问题请告诉我，
并且给出修改建议，同时解释为什么要这样改..."

✅ 精简版（50 tokens）：
"分析代码问题并提供修改建议："
```

### 2. 使用总结功能

对于长文档，先总结再提问：
```
第一步：总结这篇 10 万字的文档
第二步：基于总结回答问题
```

### 3. 结合 RAG（检索增强生成）

不把整个文档塞进上下文，而是：
1. 检索相关片段
2. 只把相关部分送入模型

这样即使文档很长，也不占用太多上下文。

### 4. 滚动窗口（Sliding Window）

对于超长对话，只保留最近的 N 条消息：
```
对话历史：[消息1, 消息2, ..., 消息100]
上下文窗口 = 4K

策略：只保留最近 10 条消息
```

## 成本考虑

不同上下文窗口的价格差异巨大：

**示例（OpenAI GPT-4 定价）：**
| 模型 | 上下文 | 输入价格（$/1M tokens） | 输出价格（$/1M tokens） |
|------|--------|---------------------|---------------------|
| GPT-4 | 8K | $30 | $60 |
| GPT-4 Turbo | 128K | $10 | $30 |

**实际计算：**
```
处理一本 10 万字的书（约 15 万 token）

GPT-4 (8K)：
- 需要分成 ~20 次请求
- 成本：15万 × $30 / 100万 × 20 = $9

GPT-4 Turbo (128K)：
- 一次性处理
- 成本：15万 × $10 / 100万 = $1.5

节省 83%！
```

## 未来趋势

### 1. 无限上下文？

理论上可行，但实际挑战：
- 计算成本过高
- 信息检索效率低
- 并非所有任务都需要

### 2. 混合方案

- **短上下文模型**：处理简单任务（便宜、快速）
- **长上下文模型**：处理复杂任务（昂贵、慢速）
- **检索系统**：只提取相关信息（节省上下文）

### 3. 更高效的架构

- **Mamba**：线性复杂度的序列模型
- **RWKV**：结合 RNN 和 Transformer 优势
- **Hyena**：次二次复杂度的注意力机制

## 常见问题

**Q: 上下文窗口越大越好吗？**

A: 不一定。
- 成本更高
- 速度更慢
- 对于简单任务，小上下文足够

**Q: 如何知道自己的输入有多少 token？**

A: 使用 Tokenizer 工具：
- OpenAI：[tiktoken](https://github.com/openai/tiktoken)
- 在线工具：[OpenAI Tokenizer](https://platform.openai.com/tokenizer)

**Q: 上下文窗口和模型参数量有关系吗？**

A: 没有直接关系。
- 7B 参数的模型可以有 1M 上下文（如 Gemini 1.5 Flash）
- 175B 参数的模型可能只有 4K 上下文（如 GPT-3）

关键在于架构设计和训练方法。

## 参考资料

- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [Extending Context Window of Large Language Models via Position Interpolation](https://arxiv.org/abs/2306.15595)
- [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)
- [什么是 Position Encoding](/guide/ai/what-is-position-encoding) - 本站相关文章
- [Qwen3 扩展到 1M 上下文是如何做到的?](/guide/ai/What-is-Dual-Chunk-Attention) - 本站相关文章
- [什么是 Flash Attention](/guide/ai/what-is-flash-attention) - 本站相关文章
