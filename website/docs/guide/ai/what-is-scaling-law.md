---
title: 什么是 Scaling Law（缩放定律）
description: 什么是 Scaling Law（缩放定律）
date: 20250112
author: AI收集
plainLanguage: |
  **Scaling Law 说白了就是：** 堆料就能变强——数据越多、模型越大、算力越强，AI 就越聪明。

  就像做饭：面粉多、水多、火大，馒头就能蒸得又大又好。AI 也一样——只要舍得花钱，无脑堆资源就能变强。

  **用大白话说：**

  想象你在玩游戏练级：
  - **小号**（70 亿参数）：能打怪，但经常翻车
  - **中号**（130 亿参数）：稳定了，偶尔还是会失误
  - **大号**（700 亿参数）：已经很强了，大部分情况都能搞定
  - **满级号**（1750 亿参数，GPT-3）：基本无敌，啥都能干

  Scaling Law 就是发现：**只要氪金（投钱），就能变强，而且变强的程度是可预测的！**

  **三大资源：**

  1. **模型大小（参数量）**：
     - 7B（70 亿）= 小学生
     - 70B（700 亿）= 大学生
     - 175B（1750 亿，GPT-3）= 博士生
     - 1T（1 万亿）= 教授级

  2. **训练数据量**：
     - 1B token = 读了几本书
     - 1T token = 读了一个图书馆
     - 10T token = 读了整个互联网

  3. **算力（GPU 数量 × 训练时间）**：
     - 8 卡 A100，训练 1 个月 = 小模型
     - 1000 卡 H100，训练 3 个月 = 大模型
     - 10000 卡 H100，训练 6 个月 = 超大模型

  **神奇的发现：**

  OpenAI 在 2020 年发现：
  - **投入 2 倍资源 → 性能提升是可预测的！**
  - **不是线性增长，而是"幂律"增长**（越大越划算）

  具体来说：
  ```
  训练 GPT-2（15 亿参数） = 100 万美元
  训练 GPT-3（1750 亿参数） = 1000 万美元（10 倍投入）
  性能提升 = 5 倍以上（超值！）
  ```

  **为什么重要？**

  1. **可预测**：不用瞎试，知道花多少钱能得到多强的模型
  2. **能规划**：公司能提前规划预算和资源
  3. **有信心**：确信"堆料就能变强"，于是大家开始疯狂投钱

  **实际例子：**

  - **GPT-2**（2019）：15 亿参数，训练成本 ~100 万美元
  - **GPT-3**（2020）：1750 亿参数，训练成本 ~1000 万美元
  - **GPT-4**（2023）：推测 1.8 万亿参数（MoE），训练成本 ~1 亿美元
  - **Gemini Ultra**（2024）：推测成本 ~1-2 亿美元

  **但也有限制：**

  1. **收益递减**：从 7B 到 70B 提升明显，从 700B 到 7T 提升变小
  2. **成本爆炸**：训练成本指数级增长，不是所有公司都玩得起
  3. **不是万能**：有些问题（如推理、数学）光堆参数解决不了

  **最新趋势：**

  大家开始"优化投资组合"：
  - **数据质量 > 数据数量**：精选高质量数据，而非无脑爬全网
  - **推理时计算**：让模型"慢思考"（如 OpenAI o1），而非只堆训练参数
  - **小而精**：DeepSeek-R1（671B）吊打很多更大的模型

  说白了，Scaling Law 就是"**氪金就能变强**"的科学证明——只要舍得投资，AI 就能越来越聪明。但现在大家也在找"性价比"更高的路线，而不是无脑堆料。
---

![scaling-law](/assets/images/scaling-law.png)

Scaling Law（缩放定律）是指大型语言模型（LLM）的性能与模型规模、数据规模、计算资源之间存在的**可预测的数学关系**。

这一发现彻底改变了 AI 研究的方向，推动了从小模型到超大模型的范式转变。

## 核心发现

2020 年，OpenAI 在论文 [*Scaling Laws for Neural Language Models*](https://arxiv.org/abs/2001.08361) 中首次系统性地揭示了这一规律：

**关键结论：**
- 模型性能（通常用损失函数 Loss 衡量）与三个因素呈现**幂律关系**（Power Law）：
  1. **模型大小**（参数量 N）
  2. **数据集大小**（训练 token 数 D）
  3. **计算量**（FLOPs，即浮点运算次数 C）

**数学表达：**
```
Loss ∝ N^(-α)  （模型大小）
Loss ∝ D^(-β)  （数据大小）
Loss ∝ C^(-γ)  （计算量）

其中 α, β, γ 是常数
```

**通俗理解：**
- 参数量增加 10 倍 → 性能提升约 2-3 倍
- 数据量增加 10 倍 → 性能提升约 1.5-2 倍
- 计算量增加 10 倍 → 性能提升约 2 倍

## 三大缩放维度

### 1. 模型大小（参数量）

| 参数量 | 代表模型 | 能力描述 |
|--------|---------|---------|
| 125M (1.25亿) | GPT-2 Small | 基础语言生成 |
| 1.5B (15亿) | GPT-2 XL | 简单对话、基础推理 |
| 7B (70亿) | LLaMA-7B, Mistral-7B | 实用级对话、代码生成 |
| 13B (130亿) | LLaMA-13B, Vicuna-13B | 较强推理能力 |
| 70B (700亿) | LLaMA-70B | 接近 GPT-3.5 水平 |
| 175B (1750亿) | GPT-3 | 强大的通用能力 |
| 540B+ | PaLM, GPT-4 (推测) | 顶级性能 |

**观察：**
- 7B → 70B：10 倍参数，性能提升 ~2-3 倍
- 70B → 175B：2.5 倍参数，性能提升 ~1.5 倍
- 收益递减规律明显

### 2. 数据大小（训练 token 数）

| 数据量 | 相当于 | 代表模型 |
|--------|--------|---------|
| 300B (3000亿) | 几百本书 | GPT-3 (初版) |
| 1T (1万亿) | 一个大图书馆 | LLaMA |
| 2T (2万亿) | 大部分互联网文本 | LLaMA 2 |
| 15T (15万亿) | 几乎全网 | Gemini 1.0 |

**Chinchilla 定律（2022）：**

DeepMind 提出优化的数据-参数比例：
```
最优训练 token 数 ≈ 20 × 参数量

例如：
70B 参数模型 → 应该训练 1.4T token
175B 参数模型 → 应该训练 3.5T token
```

**结论：**
- GPT-3（175B 参数，300B token）**训练不足**
- LLaMA（7B-65B 参数，1-2T token）**训练充分**

这解释了为什么 LLaMA-70B 能接近 GPT-3 的性能，尽管参数量只有一半。

### 3. 计算量（FLOPs）

训练一个模型需要的总计算量：
```
C (FLOPs) ≈ 6 × N × D

其中：
N = 参数量
D = 训练 token 数
```

**示例：**
```
GPT-3（175B 参数，300B token）：
C = 6 × 175B × 300B = 3.15 × 10^23 FLOPs
≈ 3000 petaFLOP-days（在 V100 上需要约 355 年）

实际使用：
- 10,000 张 V100 GPU
- 训练约 1 个月
- 成本：~1000 万美元
```

## Scaling Law 的实际应用

### 1. 模型设计决策

**问题：** 给定 1000 万美元预算，应该训练多大的模型？

**传统做法：** 凭经验试错

**Scaling Law 做法：** 用公式预测

```
预算 → 可用 GPU 小时 → 总计算量 C
根据 C 确定最优的 (N, D) 组合
```

**结论（基于 Chinchilla）：**
- 不要只追求大参数
- 应该平衡模型大小和训练数据量
- 7B 模型训练 2T token > 70B 模型训练 200B token

### 2. 性能预测

**案例：GPT-4 的规划**

OpenAI 在训练 GPT-4 之前，通过小规模实验（1M-10M 参数）拟合 Scaling Law：
1. 训练一系列小模型
2. 绘制 Loss vs 模型大小/数据量 曲线
3. 外推到大模型（1T+ 参数）
4. 预测最终性能

**结果：** 预测误差 < 5%

这让 OpenAI 敢于投入上亿美元训练 GPT-4，因为他们**提前知道能得到什么**。

### 3. 成本优化

**问题：** 如何在预算内最大化性能？

**Chinchilla 的答案：**
```
✗ 旧思路：尽可能大的模型 + 尽量少的数据
  GPT-3: 175B 参数, 300B token

✓ 新思路：平衡模型和数据
  Chinchilla: 70B 参数, 1.4T token
  结果：性能更好，推理成本更低
```

**成本对比：**
| 模型 | 参数 | 训练 token | 训练成本 | 推理成本 | 性能 |
|------|------|-----------|---------|---------|------|
| GPT-3 | 175B | 300B | 高 | 极高 | 基准 |
| Chinchilla | 70B | 1.4T | 同等 | 低 40% | +5% |

### 4. 小模型的逆袭

**发现：** 小模型 + 充分训练 > 大模型 + 不足训练

**示例：**
- **LLaMA-7B**（训练 1T token）超过 GPT-3 (175B)
- **Mistral-7B**（精选数据 + 充分训练）接近 LLaMA-13B

**启示：**
- 开源社区可以用更少资源训练强大模型
- 数据质量 > 数据数量

## Scaling Law 的局限性

### 1. 收益递减

**观察：**
```
1B → 10B：性能大幅提升（10倍投入，3倍收益）
10B → 100B：提升明显（10倍投入，2倍收益）
100B → 1T：提升减缓（10倍投入，1.5倍收益）
```

**结论：** 不可能无限缩放

### 2. 涌现能力（Emergent Abilities）

某些能力在特定规模**突然出现**，难以预测：
- **数学推理**：在 ~60B 参数时突然变强
- **多步推理**：在 ~100B 参数时显著改善
- **指令遵循**：在 ~10B 参数时出现

这些"涌现"能力不遵循平滑的 Scaling Law。

### 3. 特定任务的饱和

**示例：**
- **简单分类任务**：7B 模型就够了，继续扩大无意义
- **常识问答**：70B 已接近饱和
- **数学竞赛题**：即使 1T 参数也不行（需要新架构）

### 4. 数据瓶颈

**问题：** 互联网的高质量文本是有限的

**估计：**
- 全网高质量文本：~10T token
- 已被用于训练：~5T token

**后果：**
- 继续扩大数据量 → 只能加入低质量数据
- 低质量数据 → 性能提升不明显甚至下降

**解决方案：**
- 数据去重（LLaMA 2 做了激进去重）
- 合成数据（用 AI 生成训练数据）
- 多模态数据（图片、视频、音频）

## 后 Scaling Law 时代

### 1. 测试时计算（Test-Time Compute）

**新思路：** 不只在训练时堆资源，也在推理时堆资源

**代表：**
- **OpenAI o1**：推理时"慢思考"，花更多时间解题
- **DeepSeek-R1**：通过强化学习让模型学会推理

**结果：**
- 7B 模型 + 10 分钟思考 > 70B 模型 + 1 秒回答（在数学题上）

### 2. 数据质量优先

**从数量到质量：**
- ✗ 爬取全网数据（15T token）
- ✓ 精选高质量数据（2T token）

**案例：**
- **Phi-3**：3.8B 参数，只用精选数据，性能接近 7B 主流模型
- **Mistral-7B**：精选数据 + 优化架构，超越 LLaMA-13B

### 3. 架构创新

**不只靠规模，也靠设计：**
- **MoE（混合专家）**：1T 参数，但每次只用 8B（如 DeepSeek-V3）
- **更好的注意力机制**：Flash Attention, GQA, MQA
- **更长的上下文**：1M token 上下文窗口（Gemini 1.5）

### 4. 后训练优化

**发现：** SFT + RLHF 的性价比极高

**投入：**
```
预训练 GPT-3：1000 万美元
SFT + RLHF：50 万美元（仅 5%）
性能提升：30-50%（用户满意度）
```

**结论：** 后训练比预训练性价比更高

## 对行业的影响

### 1. 大厂的军备竞赛

**观察：**
- OpenAI：GPT-3 → GPT-4 → GPT-5（传言）
- Google：PaLM → Gemini → Gemini 2
- Meta：LLaMA → LLaMA 2 → LLaMA 3

**趋势：** 训练成本从千万美元飙升到数亿美元

### 2. 开源社区的策略

**无法拼算力，改拼效率：**
- 优化数据质量（少而精）
- 改进训练方法（蒸馏、量化）
- 专注小模型（7B-70B）

**成果：**
- LLaMA 系列（Meta 开源）
- Mistral 系列（欧洲创业公司）
- Qwen 系列（阿里巴巴）

### 3. 商业模式的分化

**两种路线：**

**路线 1：超大模型 API**
- 代表：OpenAI, Anthropic, Google
- 成本：数亿美元
- 定价：按 token 收费，贵但强

**路线 2：本地小模型**
- 代表：Mistral, LLaMA, Phi
- 成本：数百万美元
- 定价：开源或低成本，够用但不顶尖

## 常见问题

**Q: Scaling Law 是否意味着"参数越大越好"？**

A: 不完全是。Chinchilla 定律表明，应该**平衡参数量和训练数据量**。

**Q: 为什么 OpenAI 还在追求更大的模型？**

A: 因为：
1. 有些能力只在大规模时涌现
2. 用户愿意为顶级性能付费
3. 技术护城河（小公司追不上）

**Q: Scaling Law 在未来还会有效吗？**

A: 可能会遇到瓶颈：
- 数据枯竭（高质量数据有限）
- 能源限制（训练一个模型耗电量 = 一个小镇一年用电）
- 芯片限制（H100 产能有限）

但短期内（5 年）仍然有效。

## 参考资料

- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - OpenAI 2020（原始论文）
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) - Chinchilla 论文（DeepMind 2022）
- [Scaling Laws for Autoregressive Generative Modeling](https://arxiv.org/abs/2010.14701) - 多模态 Scaling Law
- [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682) - 涌现能力研究
