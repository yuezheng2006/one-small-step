---
title: 什么是 Sliding Window Attention
description: 什么是 Sliding Window Attention

date: 20250522
plainLanguage: |
  **滑动窗口注意力说白了就是：** 让 AI 只看"附近的内容"，而不是"全文"，省内存又快。

  就像你读书，传统方法要求你每读一个字都回头看一遍全书（累死），滑动窗口就是"只看前后几页"（比如前后 100 页），既节省精力又不影响理解。

  **用大白话说：**
  想象你在听老师讲课，传统注意力就像每说一句话，你都要把之前说的所有话再回想一遍（脑子要炸）。滑动窗口就是"只记住最近 10 分钟的内容"，远一点的就让它飘过去，既不累又能跟上节奏。

  **核心机制：**
  1. **固定窗口**：每次只关注前后 N 个词（比如 4096 个词）
  2. **滑动处理**：窗口跟着当前位置移动，就像"移动的放大镜"
  3. **信息接力**：通过多层叠加，远距离信息一层层传递过来

  **好处：**
  - **省显存**：不用存整个文本的注意力矩阵，只存窗口内的
  - **速度快**：计算量从 O(n²) 降到 O(n)，处理长文本快 3-5 倍
  - **能处理超长文本**：可以处理几万字甚至整本书

  **权衡：**
  - 远距离依赖能力稍弱（但通过多层叠加可以缓解）
  - 需要精心设计窗口大小

  **应用：**
  - 处理长文档（法律文件、小说、研究报告）
  - 实时对话（只记最近的对话，不记全部历史）
  - 资源受限环境（手机、边缘设备）

  说白了，滑动窗口注意力就是"用局部换全局"——只看眼前的，远处的让它飘，省资源还不影响效果。
---




![swa-mechanism](/assets/images/swa.png)  
*图：Transformer中的滑动窗口注意力机制示意图（来源：arXiv 2502.18845v1）*

[Sliding Window Attention](https://arxiv.org/html/2502.18845v1)（滑动窗口注意力）是一种用于提升大型语言模型长文本处理效率的注意力机制. 它通过限制每个token的注意力范围, 将Transformer的计算复杂度从平方级（O(n²)）降低到线性级（O(n)）, 同时保持对长距离依赖的捕捉能力. 

简单来讲, 在推理引擎中应用 SWA 可以显著降低长上下文的显存消耗. 

## SWA 的核心原理与优势

* **局部注意力窗口**：  
  每个token只关注固定窗口大小（如4096 tokens）内的上下文, 而非整个序列. 如图1所示, 窗口会随着处理位置滑动, 形成连续的上下文覆盖. 

* **线性计算复杂度**：  
  传统Transformer的注意力计算量随序列长度呈平方增长, SWA通过固定窗口大小实现线性增长, 使处理万级token的文本成为可能. 

* **信息接力机制**：  
  采用sigmoid激活函数替代softmax（SWAT改进）, 结合平衡的ALiBi位置编码, 使模型能通过滑动窗口逐层传递上下文信息, 解决传统SWA的"注意力下沉"问题. 

* **训练-推理一致性**：  
  通过Sliding Window Attention Training（SWAT）框架, 在训练阶段就采用窗口化注意力, 消除传统方法中训练全注意力与推理局部注意力的差异. 

* **硬件友好性**：  
  固定窗口大小更适合GPU的并行计算特性, 配合内存映射（mmap）技术可实现快速加载, 相比传统Transformer推理速度提升3-5倍. 

## SWA 的实际应用

* **长文档处理**：  
  处理整本书籍（如PG-19数据集）、法律文档等超长文本时, 显存占用保持稳定. 实验显示在16k tokens长度下, 困惑度（perplexity）仅上升0.15

* **实时对话系统**：  
  在持续对话场景中, 采用动态窗口滑动策略, 只保留最近N轮对话的注意力上下文, 避免历史信息累积导致的性能下降

## 支持 SWA 的实践框架

- [llama.cpp](https://github.com/ggml-org/llama.cpp/pull/13194)
  刚刚合并 SWA 支持的 PR

- [flash-linear-attention](https://github.com/Fzkuji/flash-linear-attention)  
  提供CUDA优化的SWA实现, 支持多GPU并行训练

- [nanoGPT](https://github.com/karpathy/nanoGPT)  
  新增SWA训练模式, 可在消费级GPU上训练10B级长文本模型


## 参考文献
- 核心论文：https://arxiv.org/abs/2502.18845v1  
- 技术解析：https://klu.ai/glossary/sliding-window-attention
