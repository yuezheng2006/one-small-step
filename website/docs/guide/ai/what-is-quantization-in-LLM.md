---
title: 什么是大语言模型量化? 每个量化精度都代表什么?
description: 什么是大语言模型量化? 每个量化精度都代表什么?

date: 20250129
plainLanguage: |
  **量化说白了就是：** 把模型的"精度"降低，就像把高清照片压缩成标清，但 AI 还能用。

  就像你存照片，原图 10MB，压缩后 2MB，虽然画质差了点，但还能看。量化就是把模型参数从"高清"（32位）降到"标清"（4位、8位），模型变小了，但基本功能还在。

  **用菜市场的话说：**
  想象你买水果，以前用电子秤（精确到 0.01 克），现在用弹簧秤（精确到 10 克）。虽然精度低了，但够用，而且秤更便宜、更轻。量化就是给 AI 模型"换个秤"。

  **精度等级：**
  - **FP32（32位）**：原版，最精确，但占地方大
  - **FP16（16位）**：减半，效果差不多，训练常用
  - **Q4（4位）**：再减半，只有原来的 1/4 大小，普通人跑模型的首选
  - **Q2（2位）**：极限压缩，只有原来的 1/8，效果会差一些

  **Q4_K_M 是啥意思？**
  - **Q4**：4位量化
  - **K**：K 量化方法（一种优化算法）
  - **M**：中等质量（在文件大小和效果之间平衡）

  **效果：**
  - 模型文件从 100GB 降到 25GB（Q4）
  - 显存需求也降 4 倍
  - 速度可能还更快（因为数据量小了）
  - 但精度会稍微下降（通常不明显）

  说白了，量化就是"用空间换精度"——牺牲一点点精度，换来巨大的空间和速度提升。对普通人来说，Q4 或 Q5 量化是最佳选择。
---




![](/assets/images/number-representation.png)

量化 (Quantization) 是一种通过降低模型参数的数值精度来压缩模型大小的技术. 在深度学习中, 模型参数通常以32位浮点数 (FP32) 存储, 通过量化可以将其转换为更低精度的表示形式, 从而减少模型的内存占用和计算开销. 

如图, FP32 的大小是 4 字节 (每个字节8bit, 4字节 * 8bit = 32bit), 而 FP16 的大小是 2 字节 (每个字节8bit, 2字节 * 8bit = 16bit). 

这也是为什么大家喜欢用 Q4 量化模型的原因, 跟 FP16 (16bit) 的模型相比, Q4 (4bit) 的模型只有 1/4 的大小. 运行起来需要的内存也是1/4.

现在大多数模型训练都采用 FP16 的精度, 最近出圈的 DeepSeek-V3 采用了 FP8 精度训练, 能显著提升训练速度和降低硬件成本. 


## Q4_K_M 到底是什么意思?

所以我们大概了解了 Q4 实际上指的是 4bit 量化, 那么后缀都是什么意思呢？ 于是我整理了常见的量化精度和后缀供大家参考：

这种命名方式一般是 GGUF & GGML 格式的模型. 他们通常采用 K 量化模型, 格式类似 Q4_K_M, 这里的 Q 后面的数字代表量化精度, K 代表 K 量化方法, M 代表模型在尺寸和 PPL (Perplexity, 困惑度) 之间的平衡度, 有 0, 1, XS, S, M, L 等. 



PPL 是评估语言模型性能的重要指标, 它用来衡量模型对下一个词的预测准确程度

常见 K 量化版本的PPL对比 (这是一个7B模型): 

| type | ppl increase | ppl 13b to 7b % | file size | note |
| ---- | ------------ | --------------- | --------- | ---- |
| q2_k | 0.8698 | >100% | 2.67GB | 超大号的模型想要测试, 可以考虑 Q2 版本, 比如 unsloth 团队的 DeepSeek-V3-Q2_K_M 量化版本, 我测下来实际感觉是可用的 |
| q4_0 | 0.2499 | 38.3% | 3.5GB |  |
| q4_1 | 0.1846 | 28.3% | 3.9GB |  |
| q4_ks | 0.1149 | 17.6% | 3.56GB |  |
| q4_km | 0.0535 | 8.2% | 3.80GB | 如果没有提供 Q5 量化版本, 那么 Q4 量化版本也可以考虑, 建议至少 Q4_K_M 版本 |
| q5_0 | 0.0796 | 12.2% | 4.3GB |  |
| q5_1 | 0.0415 | 6.36% | 4.7GB |  |
| q5_ks | 0.0353 | 5.41% | 4.33GB |  |
| q5_km | 0.0142 | 2.18% | 4.45GB | 目前我最推荐的量化大小, 实际体验下来各种模型量化中这个版本是最理想的 |
| q6_k | 0.0044 | 0.67% | 5.15GB |  |
| k8_0 | 0.0004 | 0.061% | 6.7GB | 如果您显存十分富裕, 当然推荐这个 |

(数据来自: [llama.cpp PR 1684](https://github.com/ggerganov/llama.cpp/pull/1684))  
(具体全部量化选项可以看 llama.cpp 的源代码 [ggml-quants.h](https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/ggml/src/ggml-quants.h))


## 那 bf16, 4bit, int4, fp8 这种呢?

这种常见于其它量化格式的模型. 有了上面的经验我们很容易猜出来 bf16 是 16bit 的精度, 4bit 是 4bit 的精度. 同样也建议至少使用 4bit 量化的模型, 除非模型特别大 200B+.
