---
title: 什么是 Chain of Thought（思维链）
description: 什么是 Chain of Thought（CoT，思维链）推理
date: 20250112
author: AI收集
plainLanguage: |
  **Chain of Thought 说白了就是：** 让 AI "把思考过程说出来"——不直接给答案，而是一步步推理。

  就像小学数学老师要求"列出解题步骤"，而不是直接写答案。AI 通过"说出思考过程"，能解决更复杂的问题。

  **用大白话说：**

  想象你问小学生："鸡兔同笼，35 个头，94 只脚，多少只鸡？"

  **没有 CoT（直接答）：**
  ```
  学生：嗯...我猜是 20 只鸡？
  （瞎蒙，错了）
  ```

  **有 CoT（列步骤）：**
  ```
  学生：
  1. 假设全是鸡，35 只鸡应该有 35×2=70 只脚
  2. 但实际有 94 只脚，多了 94-70=24 只脚
  3. 每只兔子比鸡多 2 只脚，所以有 24÷2=12 只兔子
  4. 鸡有 35-12=23 只
  答案：23 只鸡
  ```

  **核心思想：** 把问题分解成小步骤，逐步推理，而不是"一步到位"。

podcastUrl: https://assets.listenhub.ai/listenhub-public-prod/podcast/69159cc023647d391e2ead42.mp3
podcastTitle: 思维链提示：教AI分步思考，解锁深层推理能力
---

  **实际例子：**

  **例子 1：数学题**

  问题："如果 5 个苹果卖 15 元，8 个苹果多少钱？"

  **普通 AI（可能错）：**
  ```
  AI：24 元
  （直接算，可能算错）
  ```

  **CoT AI（分步骤）：**
  ```
  AI：让我一步步分析：
  1. 首先，5 个苹果卖 15 元
  2. 那么 1 个苹果的价格是 15 ÷ 5 = 3 元
  3. 8 个苹果的价格是 8 × 3 = 24 元
  答案：24 元
  ```

  **例子 2：逻辑推理**

  问题："张三比李四大 3 岁，李四比王五小 2 岁，王五 20 岁，张三多大？"

  **CoT 推理：**
  ```
  AI：让我理一下：
  1. 王五 20 岁
  2. 李四比王五小 2 岁，所以李四 = 20 - 2 = 18 岁
  3. 张三比李四大 3 岁，所以张三 = 18 + 3 = 21 岁
  答案：张三 21 岁
  ```

  **例子 3：常识推理**

  问题："我把袜子放进烘干机，现在湿袜子在哪？"

  **CoT 推理：**
  ```
  AI：让我想想：
  1. 你把湿袜子放进烘干机
  2. 烘干机的作用是烘干衣物
  3. 所以袜子应该已经不湿了
  4. 答案：袜子应该还在烘干机里，但已经是干的了
  ```

  ---

  **两种使用方式：**

  **方式 1：Zero-shot CoT（零样本）**

  只需要在提示词最后加："**Let's think step by step**"（让我们一步步思考）

  ```
  问题：
  一个数加上它自己等于 12，这个数是多少？

  Let's think step by step.

  AI 回答：
  1. 设这个数为 x
  2. x + x = 12
  3. 2x = 12
  4. x = 6
  答案：6
  ```

  **方式 2：Few-shot CoT（少样本）**

  给 AI 几个"有步骤"的例子，让它学会这个模式。

  ```
  例子：
  问：3 个苹果 9 元，5 个多少钱？
  答：
  1. 1 个苹果 = 9 ÷ 3 = 3 元
  2. 5 个苹果 = 5 × 3 = 15 元
  答案：15 元

  现在回答：
  问：4 个橙子 12 元，7 个多少钱？
  答：
  ```

  AI 会模仿例子的格式，给出步骤化的答案。

  ---

  **为什么有效？**

  **传统 AI 的问题：**
  - 复杂问题"一步到位"容易出错
  - 就像让小学生直接心算 237 × 48，很难算对

  **CoT 的优势：**
  - 把复杂问题分解成简单步骤
  - 每步都简单，串起来就能解决复杂问题
  - 就像"分步计算"，降低出错率

  **数学原理：**
  ```
  一步到位：P(正确答案) = 30%
  分 3 步：P(每步正确) = 70%
  总正确率 = 0.7 × 0.7 × 0.7 = 34%（已经更高了！）

  实际上，AI 每步的正确率 > 70%，所以 CoT 提升明显
  ```

  ---

  **适用场景：**

  **✅ 特别有效：**
  - 数学题（尤其是多步骤）
  - 逻辑推理（"谁比谁大"这类）
  - 常识推理（需要多步思考）
  - 代码调试（一步步排查）

  **❌ 不太需要：**
  - 简单问答（"秦始皇哪年统一六国？"）
  - 翻译（不需要推理）
  - 总结文章（直接提取即可）

  ---

  **实际使用：**

  **在 ChatGPT 中：**
  ```
  ❌ 普通提示：
  "计算 23 × 47"

  ✅ CoT 提示：
  "计算 23 × 47，请列出详细步骤"
  或
  "Let's think step by step: 23 × 47 = ?"
  ```

  **在代码中：**
  ```python
  # OpenAI API
  response = openai.chat.completions.create(
      model="gpt-4",
      messages=[
          {"role": "system", "content": "解决问题时，请一步步列出推理过程。"},
          {"role": "user", "content": "鸡兔同笼，35 个头，94 只脚，多少只鸡？"}
      ]
  )
  ```

  ---

  **进阶技巧：**

  **1. Self-Consistency（自洽性）**

  让 AI 多次推理，选最常见的答案：
  ```
  第 1 次推理：答案 23
  第 2 次推理：答案 23
  第 3 次推理：答案 24（可能出错了）
  第 4 次推理：答案 23
  第 5 次推理：答案 23

  最终答案：23（出现 4 次，最可信）
  ```

  **2. Tree of Thoughts（思维树）**

  不只一条推理链，探索多条路径：
  ```
  问题：如何用 4 个 4 算出 24？

  路径 1：(4 + 4) × (4 - 4) = 0 ❌
  路径 2：4 × 4 + 4 + 4 = 24 ✅
  路径 3：(4 × 4 - 4) × 4 = 48 ❌

  找到答案：路径 2
  ```

  **3. Program of Thoughts（程序化思维）**

  把推理步骤转成代码执行：
  ```
  问题：1 到 100 有多少个质数？

  AI 生成代码：
  count = 0
  for num in range(2, 101):
      is_prime = True
      for i in range(2, int(num**0.5) + 1):
          if num % i == 0:
              is_prime = False
              break
      if is_prime:
          count += 1
  print(count)

  执行：168
  ```

  ---

  **常见误区：**

  **误区 1："CoT 是万能的"**
  - 不对！简单问题用 CoT 反而浪费 token
  - 只在复杂推理时用

  **误区 2："一定要用英文'Let's think step by step'"**
  - 不一定！中文"让我们一步步思考"也行
  - 关键是让 AI 明白"要分步骤"

  **误区 3："CoT 一定更准"**
  - 不一定！如果 AI 的每一步都错，最后还是错
  - CoT 只是"降低出错率"，不是"保证正确"

  ---

  **实际效果：**

  **研究数据（Google 论文）：**
  - 数学题准确率：17% → 58%（提升 3.4 倍！）
  - 逻辑推理：29% → 79%（提升 2.7 倍）
  - 常识推理：47% → 74%（提升 1.6 倍）

  说白了，Chain of Thought 就是让 AI "把解题步骤写出来"——通过分步推理，把复杂问题拆成简单小问题，大幅提升准确率。就像老师说的："不要直接写答案，要列出解题过程！"
---

![chain-of-thought](/assets/images/chain-of-thought.png)

Chain of Thought（CoT，思维链）是一种提示工程技术，通过引导大型语言模型（LLM）**逐步展示推理过程**，而不是直接给出答案，从而显著提升复杂推理任务的准确性。

## 核心概念

### 传统推理 vs CoT 推理

**传统推理（Direct Answer）：**
```
问题：罗杰有 5 个网球。他又买了 2 罐网球，每罐 3 个。他现在有多少个网球？

AI 直接回答：11 个
```

**CoT 推理（Step-by-step）：**
```
问题：罗杰有 5 个网球。他又买了 2 罐网球，每罐 3 个。他现在有多少个网球？

AI 推理过程：
1. 罗杰最初有 5 个网球
2. 他买了 2 罐，每罐 3 个，所以买了 2 × 3 = 6 个网球
3. 现在总共有 5 + 6 = 11 个网球

答案：11 个
```

**关键差异：**
- 传统：问题 → 答案（黑盒）
- CoT：问题 → 推理步骤 → 答案（透明）

### 为什么有效？

**核心原理：**
1. **分解复杂性**：将复杂问题拆解为简单子问题
2. **降低单步错误率**：每一步都相对简单，容易做对
3. **可解释性**：能看到推理过程，发现错误

**数学直觉：**
```
假设：
- 一步到位正确率：P₁ = 40%
- 每一步正确率：Pₛ = 80%
- 需要 3 步

CoT 总正确率：P_CoT = 0.8³ = 51.2% > 40%

实际上，Pₛ 通常 > 80%，所以 CoT 提升更明显
```

## CoT 的两种形式

### 1. Few-shot CoT（少样本思维链）

通过在提示词中提供**带推理步骤的示例**，让模型学习这种模式。

**示例：**

```
问题：食堂有 23 个苹果。如果他们用掉了 20 个苹果做午餐，又买了 6 个，他们现在有多少个苹果？
推理：
食堂最初有 23 个苹果。
他们用掉了 20 个苹果，所以还剩 23 - 20 = 3 个苹果。
然后他们买了 6 个苹果，所以现在有 3 + 6 = 9 个苹果。
答案：9 个

问题：停车场有 3 辆车，又来了 2 辆车。现在有多少辆车？
推理：
停车场最初有 3 辆车。
又来了 2 辆车，所以现在有 3 + 2 = 5 辆车。
答案：5 辆

现在解决：
问题：Leah 有 32 颗巧克力，她妹妹有 42 颗。如果她们吃掉了 35 颗，还剩多少颗？
推理：
```

**模型输出：**
```
Leah 有 32 颗巧克力，妹妹有 42 颗。
总共有 32 + 42 = 74 颗巧克力。
她们吃掉了 35 颗，所以还剩 74 - 35 = 39 颗。
答案：39 颗
```

### 2. Zero-shot CoT（零样本思维链）

**无需示例**，只需在提示词末尾添加魔法短语：

**"Let's think step by step."**（让我们一步步思考）

**示例：**

```
问题：如果一辆车以 60 公里/小时的速度行驶，3.5 小时能行驶多远？

Let's think step by step.
```

**模型输出：**
```
1. 速度是 60 公里/小时
2. 时间是 3.5 小时
3. 距离 = 速度 × 时间
4. 距离 = 60 × 3.5 = 210 公里

答案：210 公里
```

**为什么有效？**
- "Let's think step by step" 触发了模型的推理模式
- 模型在预训练时见过大量"分步骤解题"的文本
- 这个短语激活了相关的推理模式

**其他有效的魔法短语：**
- "Let's solve this problem step by step."
- "Let's break this down:"
- "让我们一步步分析："
- "让我逐步思考："

## 实际应用案例

### 案例 1：数学应用题

**问题：**
```
一个农场有鸡和兔子，共 35 个头，94 只脚。有多少只鸡？
```

**Zero-shot CoT：**
```
Let's think step by step.

1. 设鸡有 x 只，兔子有 y 只
2. 头的数量：x + y = 35
3. 脚的数量：2x + 4y = 94（鸡 2 只脚，兔子 4 只脚）
4. 从第一个方程：y = 35 - x
5. 代入第二个方程：2x + 4(35 - x) = 94
6. 2x + 140 - 4x = 94
7. -2x = -46
8. x = 23

答案：23 只鸡
```

### 案例 2：逻辑推理

**问题：**
```
所有的猫都是哺乳动物。
所有的哺乳动物都需要氧气。
Tom 是一只猫。
Tom 需要氧气吗？
```

**CoT 推理：**
```
让我一步步推理：

1. 前提 1：所有的猫都是哺乳动物
2. 前提 2：所有的哺乳动物都需要氧气
3. 前提 3：Tom 是一只猫
4. 从前提 1 和 3：Tom 是猫 → Tom 是哺乳动物
5. 从前提 2 和步骤 4：Tom 是哺乳动物 → Tom 需要氧气

结论：是的，Tom 需要氧气。
```

### 案例 3：常识推理

**问题：**
```
我在冰箱里放了一个玻璃杯，里面装满了水。两小时后，玻璃杯里的水会变成什么状态？
```

**CoT 推理：**
```
让我思考一下：

1. 玻璃杯里装的是液态水
2. 放进了冰箱（温度通常在 0-4°C）
3. 液态水在 0°C 以下会结冰
4. 但冰箱冷藏室通常在 0°C 以上
5. 如果是冷藏室（0-4°C），水会变冷但不会结冰
6. 如果是冷冻室（-18°C 左右），2 小时后水会部分或全部结冰

结论：
- 冷藏室：仍是液态，但很冷
- 冷冻室：部分或全部结冰

需要更多信息才能确定。
```

### 案例 4：代码调试

**问题：**
```python
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)

# 调用
result = calculate_average([])
# 错误：ZeroDivisionError
```

**CoT 调试：**
```
让我一步步分析这个错误：

1. 函数目的：计算平均值
2. 输入：空列表 []
3. 循环：total = 0（没有循环）
4. 返回：total / len(numbers) = 0 / 0
5. 问题：len([]) = 0，不能除以 0

解决方案：
def calculate_average(numbers):
    if len(numbers) == 0:
        return 0  # 或抛出异常
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)
```

## CoT 的变体和扩展

### 1. Self-Consistency CoT（自洽性思维链）

**思想：** 多次生成推理路径，选择最常见的答案。

**流程：**
```
问题：2x + 5 = 13，求 x

生成 5 次推理：

推理 1：
2x + 5 = 13
2x = 8
x = 4 ✓

推理 2：
2x = 13 - 5 = 8
x = 4 ✓

推理 3：
2x + 5 = 13
2x = 13 - 5 = 7  ✗（错误）
x = 3.5

推理 4：
2x = 8
x = 4 ✓

推理 5：
x = (13 - 5) / 2 = 4 ✓

投票：x = 4（4 票） vs x = 3.5（1 票）
最终答案：x = 4
```

**优点：**
- 提高准确性（通过"多数投票"）
- 减少单次推理错误的影响

**缺点：**
- 成本增加（需要多次 LLM 调用）
- 速度变慢

### 2. Tree of Thoughts（思维树，ToT）

**思想：** 探索多个推理路径，像搜索树一样。

**示例：** 用 4 个 4 算出 24

```
                    问题：4 4 4 4 → 24
                           |
        ┌─────────────┬────┴─────┬─────────────┐
        |             |          |             |
    (4+4)=8      (4×4)=16    (4-4)=0      (4÷4)=1
        |             |          |             |
   ┌────┴───┐    ┌────┴───┐
8+4=12   8×4=32  16+4=20 16×4=64
   |        |       |       |
12+4=16 32-4=28 20+4=24✓ ...

找到解：(4 × 4 + 4 + 4 = 24)
```

**代表模型：**
- GPT-4 with ToT
- Claude with ToT

### 3. Program of Thoughts（思维程序，PoT）

**思想：** 将推理步骤转换为可执行代码。

**示例：**

**问题：** "1 到 100 之间有多少个质数？"

**CoT（纯文本推理）：**
```
1. 质数的定义：只能被 1 和自己整除
2. 2 是质数
3. 3 是质数
4. 4 不是（4 = 2 × 2）
5. 5 是质数
... （太繁琐！）
```

**PoT（程序化推理）：**
```python
# 生成代码
def count_primes(n):
    count = 0
    for num in range(2, n + 1):
        is_prime = True
        for i in range(2, int(num**0.5) + 1):
            if num % i == 0:
                is_prime = False
                break
        if is_prime:
            count += 1
    return count

result = count_primes(100)
print(result)  # 25
```

**优点：**
- 复杂计算交给代码（更准确）
- 适合数学、数据分析任务

### 4. Least-to-Most Prompting（从简到难）

**思想：** 先解决简单子问题，再组合成复杂问题的答案。

**示例：**

**问题：** "最后一个字母是元音字母的单词连接起来：take the last letters of the words in 'Elon Musk' and concatenate them."

**Least-to-Most：**
```
步骤 1：分解子问题
- 子问题 1：'Elon' 的最后一个字母是什么？
- 子问题 2：'Musk' 的最后一个字母是什么？
- 子问题 3：连接这两个字母

步骤 2：依次解决
- 答案 1：'Elon' → 'n'
- 答案 2：'Musk' → 'k'
- 答案 3：'n' + 'k' = 'nk'

最终答案：'nk'
```

## 实践建议

### 1. 何时使用 CoT

**✅ 推荐场景：**
- 多步骤数学题
- 逻辑推理
- 需要分析的问题
- 复杂的代码调试

**❌ 不必要场景：**
- 简单事实查询（"秦始皇哪年统一六国？"）
- 翻译
- 总结
- 简单计算（"1+1=?"）

### 2. 提示词模板

**Zero-shot CoT 模板：**
```
{问题}

Let's think step by step.
```

**Few-shot CoT 模板：**
```
问题：{示例问题 1}
推理：{示例推理 1}
答案：{示例答案 1}

问题：{示例问题 2}
推理：{示例推理 2}
答案：{示例答案 2}

现在解决：
问题：{实际问题}
推理：
```

**中文 Zero-shot CoT：**
```
{问题}

让我们一步步分析：
```

### 3. API 调用示例

**OpenAI GPT-4：**
```python
import openai

response = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "system",
            "content": "解决问题时，请逐步展示你的推理过程。"
        },
        {
            "role": "user",
            "content": "如果 3 个苹果卖 9 元，8 个苹果多少钱？\n\nLet's think step by step."
        }
    ],
    temperature=0  # 确定性输出
)

print(response.choices[0].message.content)
```

**Anthropic Claude：**
```python
import anthropic

client = anthropic.Client(api_key="...")
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=[{
        "role": "user",
        "content": "问题：鸡兔同笼，35 个头，94 只脚，多少只鸡？\n\n请一步步推理。"
    }],
    max_tokens=1024
)

print(response.content[0].text)
```

## 性能提升数据

### 学术研究成果（Google Brain，2022）

| 任务类型 | 基准准确率 | CoT 准确率 | 提升倍数 |
|----------|-----------|-----------|---------|
| 数学推理（GSM8K） | 17.9% | 58.1% | 3.2× |
| 常识推理（StrategyQA） | 54.2% | 74.3% | 1.4× |
| 符号推理 | 6.9% | 78.4% | 11.4× |
| 逻辑推理（BIG-Bench） | 29.2% | 79.0% | 2.7× |

### 模型规模影响

| 模型 | 参数量 | CoT 提升效果 |
|------|--------|------------|
| GPT-3 Small | 125M | 微弱 |
| GPT-3 Medium | 1.3B | 轻微 |
| GPT-3 Large | 6.7B | 中等 |
| GPT-3 XL | 13B | 明显 |
| GPT-3 | 175B | 显著 |

**结论：** CoT 在大模型（> 100B 参数）上效果最佳。

## 常见问题

**Q: 为什么小模型用 CoT 效果不好？**

A:
- 小模型（< 10B）缺乏足够的推理能力
- 即使"列步骤"，每一步都可能出错
- 建议：至少 70B+ 参数模型

**Q: CoT 会增加多少成本？**

A:
- 输出 token 数通常增加 2-5 倍
- 例如：原本 20 token → CoT 后 50-100 token
- 成本相应增加

**Q: 中文和英文 CoT 效果一样吗？**

A:
- 英文效果稍好（训练数据更多）
- 但中文 CoT 也很有效
- 建议：中文任务用中文提示词

**Q: 如何验证 CoT 的推理是否正确？**

A:
- 阅读推理步骤（人工检查）
- 使用 Self-Consistency（多次生成，投票）
- 对于数学题，可以用代码验证答案

## 参考资料

- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) - CoT 原始论文（Google Brain）
- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) - Zero-shot CoT 论文
- [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171) - Self-Consistency CoT
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) - ToT 论文
- [Program of Thoughts Prompting](https://arxiv.org/abs/2211.12588) - PoT 论文
- [什么是 Few-shot 和 Zero-shot Learning](/guide/ai/what-is-few-shot-zero-shot) - 本站相关文章
